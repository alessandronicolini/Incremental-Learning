{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LearningWithoutForgetting2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alessandronicolini/IncrementalLearning/blob/main/LearningWithoutForgetting2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t9nP5BV_m16C",
        "outputId": "937746e4-aec1-4875-ef77-e2703b442971"
      },
      "source": [
        "# upload work files from your git hub repository\n",
        "import sys\n",
        "!rm -r IncrementalLearning\n",
        "!git clone https://github.com/alessandronicolini/IncrementalLearning.git # clone proj repository\n",
        "!rm -rf IncrementalLearning/README.md \n",
        "!rm -rf IncrementalLearning/baselines.ipynb\n",
        "\n",
        "path = 'IncrementalLearning/'\n",
        "if path not in sys.path:\n",
        "    sys.path.append('IncrementalLearning/')\n",
        "\n",
        "!pip3 install import_ipynb"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'IncrementalLearning'...\n",
            "remote: Enumerating objects: 274, done.\u001b[K\n",
            "remote: Total 274 (delta 0), reused 0 (delta 0), pack-reused 274\u001b[K\n",
            "Receiving objects: 100% (274/274), 144.75 KiB | 6.89 MiB/s, done.\n",
            "Resolving deltas: 100% (144/144), done.\n",
            "Requirement already satisfied: import_ipynb in /usr/local/lib/python3.6/dist-packages (0.1.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R41rvWGamroV"
      },
      "source": [
        "import torch\n",
        "import os\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Subset, DataLoader\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms\n",
        "import numpy as np\n",
        "import import_ipynb\n",
        "import copy\n",
        "import pickle\n",
        "import pandas as pd\n",
        "\n",
        "# project classes --------------------------------------------------------------\n",
        "from IncrementalLearning.cifar100 import ilCIFAR100\n",
        "from resnet_cifar import resnet32"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yD1W9sXOnEHg"
      },
      "source": [
        "class lwf():\n",
        "  def __init__(self, random_seed, batch_size):\n",
        "    \n",
        "    # hyper parameters related attributes\n",
        "    self.batch_size = batch_size\n",
        "    self.classes_per_task = 10\n",
        "    self.num_tasks = 10\n",
        "    self.LR = 2\n",
        "    self.MOMENTUM = 0.9\n",
        "    self.WEIGHT_DECAY = 1e-5\n",
        "    self.MILESTONES = [49,63]\n",
        "    self.GAMMA = 0.2\n",
        "    self.numepochs = 70\n",
        "    \n",
        "    # dataset related attributes\n",
        "    self.random_seed = random_seed\n",
        "    self.original_training_set = ilCIFAR100(self.classes_per_task, random_seed)\n",
        "    self.original_test_set = ilCIFAR100(self.classes_per_task, random_seed, train=False)\n",
        "    \n",
        "    # models related attributes\n",
        "    self.model = resnet32(num_classes=100).to('cuda')\n",
        "    self.criterion = nn.BCEWithLogitsLoss()\n",
        "    self.mapper = self.original_training_set.get_dict()\n",
        "    self.current_classes = 0\n",
        "\n",
        "  def update_parameters(self, train_dataloader):\n",
        "    \n",
        "    # update current_classes\n",
        "    self.current_classes += self.classes_per_task\n",
        "\n",
        "    # initialize optimizer and scheduler\n",
        "    optimizer = optim.SGD(self.model.parameters(), lr=self.LR, momentum=self.MOMENTUM, weight_decay=self.WEIGHT_DECAY)\n",
        "    scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=self.MILESTONES, gamma=self.GAMMA)\n",
        "\n",
        "    # create old _model and set in evaluation mode\n",
        "    old_model = copy.deepcopy(self.model)\n",
        "    old_model = old_model.to('cuda')\n",
        "    old_model.eval()\n",
        "\n",
        "    # set current model in training mode\n",
        "    self.model.train()\n",
        "  \n",
        "    for epoch in range(self.numepochs):\n",
        "      for inputs, labels in train_dataloader:\n",
        "        \n",
        "        # read data batch inputs and map original labels to new labels\n",
        "        labels = torch.tensor([torch.tensor(self.mapper[c.item()]) for c in labels])\n",
        "        inputs = inputs.to('cuda')\n",
        "        labels = labels.to('cuda')\n",
        "        \n",
        "        # zeroing gradients\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # compute net outputs \n",
        "        outputs = self.model(inputs)\n",
        "        \n",
        "        # set up output targets\n",
        "        target = torch.eye(100)[labels] \n",
        "        target = target.to('cuda')\n",
        "\n",
        "        # update target tensor if you have more than ten classes activated (since second task)\n",
        "        if self.current_classes != self.classes_per_task:\n",
        "          known_classes = self.current_classes - self.classes_per_task\n",
        "          old_target = old_model(inputs).to('cuda')\n",
        "          old_target = torch.sigmoid(old_target).to('cuda')\n",
        "          target = torch.cat((old_target[:,:known_classes], target[:,known_classes:]), dim=1)\n",
        "        \n",
        "        # compute loss\n",
        "        loss=self.criterion(outputs, target)\n",
        "        \n",
        "        # update gradients\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "      # update scheduler\n",
        "      scheduler.step()\n",
        "\n",
        "  \n",
        "  def compute_accuracy(self, dataloader):\n",
        "    total = 0.0\n",
        "    running_corrects = 0.0\n",
        "    for images, labels in dataloader:\n",
        "      total += len(labels)\n",
        "      labels = torch.tensor([torch.tensor(self.mapper[c.item()]) for c in labels])\n",
        "      images = images.to('cuda')\n",
        "      labels = labels.to('cuda')\n",
        "      outputs = self.model(images)\n",
        "      _, preds = torch.max(outputs.data, 1)\n",
        "      running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "    accuracy = running_corrects / total\n",
        "    return accuracy\n",
        "\n",
        "  def training(self):\n",
        "    \n",
        "    # get train and test indixes\n",
        "    train_indexes= self.original_training_set.get_batch_indexes()\n",
        "    test_indexes = self.original_test_set.get_batch_indexes()\n",
        "    current_test_indexes=[]\n",
        "\n",
        "    # initialize accuracy related variables\n",
        "    accuracy = 0 \n",
        "    tasks_test_acc = []\n",
        "\n",
        "    for i in range(self.num_tasks):\n",
        "      \n",
        "      # make current train and test datasets \n",
        "      train_dataset = Subset(self.original_training_set, train_indexes[i])\n",
        "      current_test_indexes += test_indexes[i].tolist()\n",
        "      test_dataset = Subset(self.original_test_set, current_test_indexes)\n",
        "      \n",
        "      # make current train and test dataloaders \n",
        "      train_dataloader = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=False)\n",
        "      test_dataloader = DataLoader(test_dataset, batch_size=self.batch_size, shuffle=False)        \n",
        "      \n",
        "      # train model for current task\n",
        "      self.update_parameters(train_dataloader)\n",
        "      \n",
        "      # set model in evaluation mode\n",
        "      self.model.eval()\n",
        "\n",
        "      # LAST EPOCH MODEL TRAIN ACCURACY\n",
        "      train_acc = self.compute_accuracy(train_dataloader)\n",
        "      \n",
        "      # TEST ACCURACY\n",
        "      test_acc = self.compute_accuracy(test_dataloader)\n",
        "      print('TASK %d    TRAIN_ACC: %.2f    TEST_ACC: %.2f' % (i, 100*train_acc, 100*test_acc))\n",
        "      \n",
        "      # save current test accuracy\n",
        "      tasks_test_acc.append(accuracy)\n",
        "\n",
        "    with open(\"Lwf_\"+str(self.random_seed), \"wb\") as file:\n",
        "      pickle.dump(tasks_test_acc, file, pickle.HIGHEST_PROTOCOL)\n",
        "    "
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        },
        "id": "1b6XwdKK6Kqd",
        "outputId": "a9fcd6cc-4173-4b13-bbc7-007fb5d383bf"
      },
      "source": [
        "seed = 981\n",
        "print(\"NEW RUN, seed %d\"%(seed))\n",
        "lwf = lwf(batch_size=128, random_seed=seed)\n",
        "lwf.training()\n",
        "\n",
        "\"\"\"SEED = [981, 404, 182]\n",
        "for i in range(3):\n",
        "  print(\"NEW RUN, seed %d\"%(SEED[i]))\n",
        "  model = lwf(batch_size = 128, random_seed = SEED[i])\n",
        "  model.train_model()\"\"\""
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NEW RUN, seed 981\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "TASK 0    TRAIN_ACC: 99.12    TEST_ACC: 84.80\n",
            "TASK 1    TRAIN_ACC: 94.06    TEST_ACC: 67.90\n",
            "TASK 2    TRAIN_ACC: 93.98    TEST_ACC: 56.47\n",
            "TASK 3    TRAIN_ACC: 91.14    TEST_ACC: 47.90\n",
            "TASK 4    TRAIN_ACC: 90.60    TEST_ACC: 42.14\n",
            "TASK 5    TRAIN_ACC: 87.56    TEST_ACC: 38.17\n",
            "TASK 6    TRAIN_ACC: 87.10    TEST_ACC: 32.73\n",
            "TASK 7    TRAIN_ACC: 88.32    TEST_ACC: 29.40\n",
            "TASK 8    TRAIN_ACC: 85.66    TEST_ACC: 26.86\n",
            "TASK 9    TRAIN_ACC: 84.06    TEST_ACC: 22.74\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'SEED = [981, 404, 182]\\nfor i in range(3):\\n  print(\"NEW RUN, seed %d\"%(SEED[i]))\\n  model = lwf(batch_size = 128, random_seed = SEED[i])\\n  model.train_model()'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c4QTSW-K1is5"
      },
      "source": [
        "\"\"\"import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "test_acc = []\n",
        "for seed in SEED:\n",
        "  with open('Lwf_'+str(seed), 'rb') as f:\n",
        "    current = pickle.load(f)\n",
        "    test_acc += current\n",
        "\n",
        "acc_dict = {'model': ['lwf' for i in range(30)], \n",
        "            'classes': [i for i in range(10,110,10)]*3,\n",
        "            'test_acc': test_acc}\n",
        "            \n",
        "acc_df = pd.DataFrame(data=acc_dict)\n",
        "\n",
        "sns.set_theme(style=\"darkgrid\")\n",
        "ax = sns.pointplot(x=\"classes\", y=\"test_acc\", hue=\"model\", data=acc_df)\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}