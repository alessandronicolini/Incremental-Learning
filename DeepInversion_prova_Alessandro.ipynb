{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DeepInversion_prova_Alessandro.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPPylAiIJagEUlBFmNH3kE3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "3b8162c55332427a8ae76e869991ed45": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_579a97a5084e423b928b1d896750f435",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_a78e748535174fb8aa7c3f8db5e2b9ad",
              "IPY_MODEL_120454f355da46618d9888b0f870ea94"
            ]
          }
        },
        "579a97a5084e423b928b1d896750f435": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a78e748535174fb8aa7c3f8db5e2b9ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_8b07fc16b92b4970bacf9247ece8fcb0",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 70,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 70,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b1d36cb729c64d43b6ace41026bfdb6d"
          }
        },
        "120454f355da46618d9888b0f870ea94": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_00443eca221241d7b1c9ead7ec99a3e1",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 70/70 [10:36&lt;00:00,  9.09s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e9f93eb3434c4329b2a74c53ab57df87"
          }
        },
        "8b07fc16b92b4970bacf9247ece8fcb0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b1d36cb729c64d43b6ace41026bfdb6d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "00443eca221241d7b1c9ead7ec99a3e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e9f93eb3434c4329b2a74c53ab57df87": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "509acb6efdac486da16de095ec1e6ed8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_8ecaaf6b1a2843f1bf8a389a28a0f038",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_149f1ec304fb4f70b15f232617d04426",
              "IPY_MODEL_6f4ef1e423824dd38e2e920fe1a5d87a"
            ]
          }
        },
        "8ecaaf6b1a2843f1bf8a389a28a0f038": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "149f1ec304fb4f70b15f232617d04426": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_79cd8a2b51944d44a51232cc9b08222e",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 70,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 70,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1980561397c24b91a9ef8ea6de3fa144"
          }
        },
        "6f4ef1e423824dd38e2e920fe1a5d87a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_796ee09f6dc9472d80726fba6bd38b2e",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 70/70 [06:15&lt;00:00,  5.37s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_27642529b77c4d21812bd63897d8fb9a"
          }
        },
        "79cd8a2b51944d44a51232cc9b08222e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "1980561397c24b91a9ef8ea6de3fa144": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "796ee09f6dc9472d80726fba6bd38b2e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "27642529b77c4d21812bd63897d8fb9a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2881a7f479d64056ac89148fa01a00c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_6fc01a0e492e43258cbc58b29f959d3d",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_62edc7175f22499e9389f89b2dcc27d4",
              "IPY_MODEL_9c70873decd848f88350491efd36cd28"
            ]
          }
        },
        "6fc01a0e492e43258cbc58b29f959d3d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "62edc7175f22499e9389f89b2dcc27d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_90bb02580b6a46869e5a1276b7795173",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1000,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1000,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1ec07080e2114c179b92cad4263e6639"
          }
        },
        "9c70873decd848f88350491efd36cd28": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_2d46e5970029462c87c99723add72af5",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1000/1000 [02:42&lt;00:00,  6.16it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_567531b1f5a04b1ab49c8f876d98fc38"
          }
        },
        "90bb02580b6a46869e5a1276b7795173": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "1ec07080e2114c179b92cad4263e6639": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2d46e5970029462c87c99723add72af5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "567531b1f5a04b1ab49c8f876d98fc38": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alessandronicolini/IncrementalLearning/blob/main/DeepInversion_prova_Alessandro.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RfqenFowb1xL",
        "outputId": "b6591af5-a998-4e61-8928-deb3d28ce75d"
      },
      "source": [
        "!pip3 install 'import_ipynb'\n",
        "!pip3 install 'tqdm'\n",
        " \n",
        "!rm -r IncrementalLearning\n",
        "# upload work files from your git hub repository\n",
        "import sys\n",
        " \n",
        "!git clone https://github.com/alessandronicolini/IncrementalLearning.git # clone proj repository\n",
        "!rm -rf IncrementalLearning/README.md \n",
        "!rm -rf IncrementalLearning/baselines.ipynb\n",
        " \n",
        "path = 'IncrementalLearning/'\n",
        "if path not in sys.path:\n",
        "    sys.path.append('IncrementalLearning/')\n",
        " \n",
        "!pip3 install import_ipynb"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting import_ipynb\n",
            "  Downloading https://files.pythonhosted.org/packages/63/35/495e0021bfdcc924c7cdec4e9fbb87c88dd03b9b9b22419444dc370c8a45/import-ipynb-0.1.3.tar.gz\n",
            "Building wheels for collected packages: import-ipynb\n",
            "  Building wheel for import-ipynb (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for import-ipynb: filename=import_ipynb-0.1.3-cp36-none-any.whl size=2976 sha256=8049ef31275e3049a33c6db70b108fd32b10b3ea48d0d1fe6efdbf1c74597763\n",
            "  Stored in directory: /root/.cache/pip/wheels/b4/7b/e9/a3a6e496115dffdb4e3085d0ae39ffe8a814eacc44bbf494b5\n",
            "Successfully built import-ipynb\n",
            "Installing collected packages: import-ipynb\n",
            "Successfully installed import-ipynb-0.1.3\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.41.1)\n",
            "rm: cannot remove 'IncrementalLearning': No such file or directory\n",
            "Cloning into 'IncrementalLearning'...\n",
            "remote: Enumerating objects: 163, done.\u001b[K\n",
            "remote: Counting objects: 100% (163/163), done.\u001b[K\n",
            "remote: Compressing objects: 100% (162/162), done.\u001b[K\n",
            "remote: Total 637 (delta 100), reused 0 (delta 0), pack-reused 474\u001b[K\n",
            "Receiving objects: 100% (637/637), 831.40 KiB | 2.19 MiB/s, done.\n",
            "Resolving deltas: 100% (375/375), done.\n",
            "Requirement already satisfied: import_ipynb in /usr/local/lib/python3.6/dist-packages (0.1.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Us1alRIbb5pW"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import VisionDataset\n",
        "from PIL import Image\n",
        "import random\n",
        "import torchvision\n",
        "ROOT = './data'\n",
        "class ilCIFAR100(VisionDataset):\n",
        "    \"\"\"\n",
        "    Extends CIFAR100 class. Split the dataset into 10 batches, each one containing 10 classes.\n",
        "    You can retrieve the batches from the attribute \"batches\", it has different structure according to\n",
        "    test and train CIFAR100 splits:\n",
        "        - train -> batches is a dictionary {0:{'train':indexes, 'val':indexes}...} \n",
        "        - test -> batches is a dictionary {0:indexes...}\n",
        "    where the keys are the batch number.\n",
        "\n",
        "    Args:\n",
        "        root (string): Root directory of dataset where directory\n",
        "            `cifar-10-batches-py` exists or will be saved to if download is set to True.\n",
        "        seed(int): used to ensure reproducibility in shuffling operations.\n",
        "        val_size(float, optional): between 0 and 1, fraction of data used for validation.\n",
        "        train (bool, optional): If True, creates dataset from training set, otherwise\n",
        "            creates from test set.\n",
        "        transform (callable, optional): A function/transform that takes in an PIL image\n",
        "            and returns a transformed version. E.g, `transforms.RandomCrop`\n",
        "        target_transform (callable, optional): A function/transform that takes in the\n",
        "            target and transforms it.\n",
        "        download (bool, optional): If true, downloads the dataset from the internet and\n",
        "            puts it in root directory. If dataset is already downloaded, it is not\n",
        "            downloaded again.\n",
        "    \"\"\"\n",
        "    def __init__(self,classes_per_batch, seed, val_size=0.1, train=True, transform=None, target_transform=False, \n",
        "    download=True):\n",
        "        \n",
        "        super(ilCIFAR100, self).__init__(root=0)\n",
        "        self.classes_per_batch=classes_per_batch\n",
        "\n",
        "        \n",
        "        self.__rs = seed # set random seed \n",
        "        self.train=train\n",
        "        self.__transform_train = transforms.Compose([\n",
        "            transforms.RandomCrop(32, padding=4),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))\n",
        "        ])\n",
        "\n",
        "        self.__transform_test = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))\n",
        "        ])\n",
        "        # if train:\n",
        "        #     self.batches = self.__make_train_batches(val_size)\n",
        "        # else:\n",
        "        #     self.batches = self.__make_test_batches()\n",
        "        if self.train == 'train':\n",
        "          self.dataset = torchvision.datasets.CIFAR100(root=ROOT, train=True,\n",
        "                                            download=True, transform=self.__transform_train)\n",
        "        elif self.train == 'exemplars':\n",
        "          self.dataset = torchvision.datasets.CIFAR100(root=ROOT, train=True,\n",
        "                                            download=True, transform=self.__transform_test)\n",
        "        else:\n",
        "          self.dataset = torchvision.datasets.CIFAR100(root=ROOT, train=False,\n",
        "                                        download=True, transform=self.__transform_test)\n",
        "          \n",
        "        self.targets = np.array(self.dataset.targets) # make targets an array to exploit masking\n",
        "        random.seed(seed)\n",
        "        self.classes = random.sample(range(0, 100), 100)\n",
        "        #self.classes = self.classes.reshape((10, -1)) # each row contains the classes for the corrisponding batch\n",
        "        #print(self.classes)\n",
        "        self.__dictionary = {}\n",
        "        for i, c in enumerate(self.classes):\n",
        "          self.__dictionary[c] = i\n",
        "\n",
        "\n",
        "    def get_dict(self):\n",
        "      return self.__dictionary\n",
        "    def __getitem__(self, index):\n",
        "        return index,self.dataset.__getitem__(index)[0],self.dataset.__getitem__(index)[1]\n",
        "    def __len__(self):\n",
        "        return self.dataset.__len__()\n",
        "    def getbatches(self):\n",
        "      classlist=self.classes\n",
        "      batches=[]\n",
        "      for i in range(0,int(100/self.classes_per_batch)):\n",
        "        #print(i)\n",
        "        batch=classlist[int(i*self.classes_per_batch):int(i*10+self.classes_per_batch)]\n",
        "        batches.append(batch)\n",
        "      return batches\n",
        "    def get_batch_indexes(self):\n",
        "      classlist=self.classes\n",
        "      numclass=self.classes_per_batch\n",
        "      batch_indexes=[]\n",
        "      for i in range(0,int(100/self.classes_per_batch)):\n",
        "        batch=classlist[int(i*numclass):int(i*numclass+numclass)]\n",
        "        mask=np.isin(self.targets,batch)\n",
        "        indexes=np.array(np.arange(len(self.dataset.targets)))\n",
        "        indexes=indexes[mask]\n",
        "        batch_indexes.append(indexes)\n",
        "      return batch_indexes\n",
        "    def get_class_indexes(self,label):\n",
        "      indexes = np.array(np.arange(len(self.dataset.targets)))\n",
        "      labels = self.dataset.targets\n",
        "      mask = np.isin(labels, label)\n",
        "      indexes = indexes[mask]\n",
        "\n",
        "      return indexes\n",
        "    def get_train_val(self,valid):\n",
        "      batches=self.get_batch_indexes()\n",
        "      train=[]\n",
        "      val=[]\n",
        "      for batch in batches:\n",
        "        #print(type(batch))\n",
        "        random.shuffle(batch)\n",
        "        valbatch=batch[0:int(valid*len(batch))]\n",
        "        trainbatch=batch[int(valid*len(batch)):]\n",
        "        train.append(trainbatch)\n",
        "        val.append(valbatch)\n",
        "      return train,val\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import time\n",
        "import os\n",
        "import argparse\n",
        "import numpy as np\n",
        "\n",
        "def tensor2im(input_image, imtype=np.uint8):\n",
        "    mean = [0.5071, 0.4867, 0.4408]\n",
        "    std = [0.2675, 0.2565, 0.2761]\n",
        "    if not isinstance(input_image, np.ndarray):\n",
        "        if isinstance(input_image, torch.Tensor):\n",
        "            image_tensor = input_image.data\n",
        "        else:\n",
        "            return input_image\n",
        "        image_numpy = image_tensor.cpu().detach().float().numpy()\n",
        "        if image_numpy.shape[0] == 1:\n",
        "            image_numpy = np.tile(image_numpy, (3, 1, 1))\n",
        "        for i in range(len(mean)): \n",
        "            image_numpy[i] = image_numpy[i] * std[i] + mean[i]\n",
        "        image_numpy = image_numpy * 255\n",
        "        image_numpy = np.transpose(image_numpy, (1, 2, 0))\n",
        "    else:\n",
        "        image_numpy = input_image\n",
        "    return image_numpy.astype(imtype)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TTxZa5_vb7x-",
        "outputId": "83f1ec5f-5c67-45f7-bc93-2217475bc137"
      },
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import math\n",
        "from sklearn.preprocessing import normalize\n",
        "import copy\n",
        "import torchvision.datasets as dsets\n",
        "import torchvision.models as models\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.gridspec as gridspec\n",
        "from torch.utils.data import Subset, DataLoader, Dataset\n",
        "import random\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pickle\n",
        "import pandas as pd\n",
        "\n",
        "import import_ipynb\n",
        "#from IncrementalLearning.cifar100 import ilCIFAR100\n",
        "\n",
        "from IncrementalLearning.resnet_cifar import resnet32\n",
        "from tqdm.notebook import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "importing Jupyter notebook from /content/IncrementalLearning/resnet_cifar.ipynb\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        },
        "id": "E_tW2Uqnb_wT",
        "outputId": "79417ec5-fb60-47db-a604-b88ac3601c1b"
      },
      "source": [
        "class mnemonics():\n",
        "  def __init__(self, randomseed):\n",
        "    self.device = 'cuda'\n",
        "    self.model = resnet32(num_classes=100).to(self.device)\n",
        "    self.feature_extractor = self.model.features\n",
        "    self.temp_model = None\n",
        "    self.lr = 2\n",
        "    self.gamma = 0.2\n",
        "    self.weight_decay = 1e-5 \n",
        "    self.milestones = [49,63]\n",
        "    self.batch_size = 128\n",
        "    self.numepochs = 70\n",
        "    self.n_classes = 0\n",
        "    self.n_known = 0\n",
        "    self.feature_size=64\n",
        "    self.momentum=0.9\n",
        "    self.criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    self.NUM_BATCHES=10\n",
        "    self.randomseed=randomseed\n",
        "    self.trainloader=None\n",
        "    self.testloader=None\n",
        "    self.CLASSES_PER_BATCH=10\n",
        "\n",
        "    self.original_training_set = ilCIFAR100(self.CLASSES_PER_BATCH, self.randomseed, train = 'train')\n",
        "    self.original_exemplar_set = ilCIFAR100(self.CLASSES_PER_BATCH, self.randomseed, train = 'exemplars')\n",
        "    self.original_test_set = ilCIFAR100(self.CLASSES_PER_BATCH,self.randomseed, train= 'test')\n",
        "\n",
        "    self.last_test = None\n",
        "    self.y_pred = []\n",
        "    self.y_test = []\n",
        "\n",
        "    self.cumulative_class_mean = []\n",
        "\n",
        "    self.classes_seen=0\n",
        "    self.diz = self.original_training_set.get_dict()\n",
        "\n",
        "    self.exemplar_features_mean = None\n",
        "    # lista di liste, ogni lista contiene gli exemplars di una classe\n",
        "    self.exemplar_sets_idxs = [] # mn_exemplat_sets\n",
        "    # lista unica, tutti gli indici degli exemplar\n",
        "    self.exemplar_idxs = []\n",
        "\n",
        "  '''\n",
        "  def update_params(self, \n",
        "                    m,\n",
        "                    finetuning_idxs, \n",
        "                    training_idxs, \n",
        "                    mnemonics_to_optimize, \n",
        "                    batch_size,\n",
        "                    new=True,\n",
        "                    lr=10, \n",
        "                    momentum=0.9, \n",
        "                    weight_decay=1e-5, \n",
        "                    milestones=[10, 20, 30, 40],\n",
        "                    gamma=0.5, \n",
        "                    tuning_epochs=4,\n",
        "                    updating_epochs=50):\n",
        "    \n",
        "    \"\"\"\n",
        "    finetuning_idxs = indexes of current task elements\n",
        "    mnemonics_idxs = indexes of exemplar elements\n",
        "    mnemonics_to_optimize = the optimized parameters in the update phase\n",
        "    \"\"\"\n",
        "\n",
        "    # make a copy of the model\n",
        "    model_copy = copy.deepcopy(self.model)\n",
        "    model_copy.train()\n",
        "    model_copy.to(self.device)\n",
        "\n",
        "    # define the loss\n",
        "    # criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    # FINE TUNING FOR 1 EPOCH eq. 8 --------------------------------------------\n",
        "    \n",
        "    # define optimizer and scheduler for fine tuning phase\n",
        "    optimizer = optim.SGD(model_copy.parameters(), lr=2, momentum=momentum, weight_decay=weight_decay)\n",
        "    \n",
        "    # create the subset dataset to load the data you want, and the loader\n",
        "    finetuning_labels = np.array([self.original_training_set.__getitem__(idx)[2] for idx in finetuning_idxs], dtype=int)\n",
        "    meta_idxs = [i for i in range(len(finetuning_idxs))]\n",
        "    random.shuffle(meta_idxs)\n",
        "\n",
        "    # split the meta idxs in batches\n",
        "    n_batches = int(np.floor(len(finetuning_idxs)/batch_size))\n",
        "    meta_idxs_batches = []\n",
        "    for i in range(n_batches):\n",
        "      meta_idxs_batches.append(np.array(meta_idxs[batch_size*i:batch_size*(i+1)]))\n",
        "    meta_idxs_batches.append(np.array(meta_idxs[batch_size*n_batches:]))\n",
        "\n",
        "    # now fine tune the copied model\n",
        "    for epoch in range(tuning_epochs):\n",
        "      for meta_idxs_batch in meta_idxs_batches:\n",
        "        inputs = mnemonics_to_optimize[0][meta_idxs_batch] # are already in cuda\n",
        "        labels = finetuning_labels[meta_idxs_batch]\n",
        "        labels = torch.tensor([self.diz[c] for c in labels])\n",
        "        labels = labels.to(self.device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model_copy(inputs)\n",
        "        labels_encoded = F.one_hot(labels,100).float().to(self.device)\n",
        "        loss = self.criterion(outputs, labels_encoded)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    \n",
        "\n",
        "    # UPDATE THE MNEMONICS eq.9/10 ---------------------------------------------\n",
        "    \n",
        "    model_copy.eval()\n",
        "    \n",
        "    optimizer = optim.SGD(mnemonics_to_optimize, lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
        "    scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=milestones, gamma=gamma)\n",
        "    \n",
        "\n",
        "    if new:\n",
        "      exlvl_training = Subset(self.original_training_set, training_idxs)\n",
        "      exlvl_loader = DataLoader(exlvl_training, batch_size=batch_size, shuffle=True, num_workers=4)\n",
        "\n",
        "      current_task_labels = set([self.original_training_set.__getitem__(idx)[2] for idx in training_idxs])\n",
        "      new_dict = {label:new_label for label, new_label in zip(current_task_labels, range(10))}\n",
        "\n",
        "      new_class_mean = {new_dict[key] : value for key, value in self.cumulative_class_mean.items()}\n",
        "      means_ready = torch.Tensor(list(new_class_mean.values())).to(self.device)\n",
        "\n",
        "    \n",
        "    else:\n",
        "      exlvl_training = Subset(self.original_exemplar_set, training_idxs)\n",
        "      exlvl_loader = DataLoader(exlvl_training, batch_size=batch_size, shuffle=True, num_workers=4)\n",
        "\n",
        "    print('lunghezza del exemplar update:', len(training_idxs))\n",
        "    for epoch in tqdm(range(updating_epochs)):\n",
        "\n",
        "      for _, inputs, labels in exlvl_loader:\n",
        "\n",
        "        if new:\n",
        "          labels = torch.tensor([new_dict[c.item()] for c in labels])\n",
        "        else:\n",
        "          labels = torch.tensor([self.diz[c.item()] for c in labels])\n",
        "\n",
        "        labels = labels.to(self.device)\n",
        "        inputs = inputs.to(self.device)\n",
        "        optimizer.zero_grad()\n",
        "        out_features = model_copy.features(inputs)\n",
        "        # compute features mean of mnemonics for each class\n",
        "        if new:\n",
        "          the_logits = F.linear(F.normalize(out_features, p=2, dim=1), F.normalize(means_ready, p=2, dim=1))\n",
        "        else:\n",
        "          ##Da capire questa cosa!!!!!!! se provassi a mettere self?\n",
        "          #OPPURE uso un altro tensore copiando all_class_means_calcolato_fuori?  \n",
        "          #dov'è il gradiente?\n",
        "          n_classes = int(len(finetuning_labels)/m)\n",
        "          all_class_means = torch.zeros((0, 64))\n",
        "          all_class_means = all_class_means.to(self.device)\n",
        "          for i in range(n_classes): # how many classes\n",
        "            mnemonics_features = model_copy.features(mnemonics_to_optimize[0][i*m:(i+1)*m])\n",
        "            this_class_means = torch.mean(mnemonics_features, dim=0) # size 64\n",
        "            this_class_means = torch.unsqueeze(this_class_means, dim=0) # add the second dimension\n",
        "            all_class_means = torch.cat((all_class_means, this_class_means), dim=0)\n",
        "          the_logits = F.linear(F.normalize(out_features, p=2, dim=1), F.normalize(all_class_means, p=2, dim=1))\n",
        "\n",
        "        #labels_encoded = F.one_hot(labels,100).float().cuda()\n",
        "\n",
        "        loss = F.cross_entropy(the_logits, labels) # al secondo batch di classi per i new mnemonics le uscite sono sempre 10 ma le label vanno da 10 a 19\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "  def exemplar_level_optimization(self, m, task_num, current_task_indices):  \n",
        "    \n",
        "    # UPDATING NEW EXEMPLAR-----------------------------------------------------\n",
        "\n",
        "    # isola gli indici dei nuovi exemplars\n",
        "    new_exemplar_idxs = []\n",
        "    for idxs in self.exemplar_sets_idxs[-10:]:\n",
        "      new_exemplar_idxs += idxs\n",
        "\n",
        "    # ora ottieni gli mnemonics che poi sono da ottimizzare\n",
        "    new_mnemonics_data = torch.zeros((10*m, 3, 32, 32))\n",
        "\n",
        "    for i, idx in enumerate(new_exemplar_idxs):\n",
        "      new_mnemonics_data[i, :, :, :] = self.original_training_set.__getitem__(int(idx))[1]\n",
        "\n",
        "    new_mnemonics = nn.ParameterList()\n",
        "    new_mnemonics.append(nn.Parameter(new_mnemonics_data))\n",
        "    new_mnemonics.to(self.device)\n",
        "    \n",
        "    #print(new_mnemonics[0][0])\n",
        "\n",
        "    options_new ={'finetuning_idxs': new_exemplar_idxs, \n",
        "                  'training_idxs': current_task_indices, \n",
        "                  'mnemonics_to_optimize':  new_mnemonics,  \n",
        "                  'batch_size':128,\n",
        "                  'm':m}\n",
        "\n",
        "    print('---start mnemonics updating---')\n",
        "\n",
        "    self.update_params(**options_new)    \n",
        "\n",
        "    for i, idx in enumerate(new_exemplar_idxs):\n",
        "      self.original_exemplar_set.dataset.data[idx] = tensor2im(new_mnemonics[0][i])\n",
        "\n",
        "    \n",
        "    # UPDATING OLD EXEMPLARS ---------------------------------------------------\n",
        "\n",
        "    if task_num:\n",
        "      # decidi quanti elementi ha ogni exemlar set in a e in b a seconda se m è \n",
        "      # pari o dispari\n",
        "      if m%2:\n",
        "        l_a = int((m+1)/2)\n",
        "      else:\n",
        "        l_a = int(m/2)\n",
        "      l_b = int(m-l_a)\n",
        "\n",
        "      # isola gli indici dei vecchi exemplars, dividendoli in due parti\n",
        "      # ogni classe deve avere circa la metà degli exemplar originali\n",
        "      old_exemplar_idxs_a = []\n",
        "      old_exemplar_idxs_b = []\n",
        "      \n",
        "      for idxs in self.exemplar_sets_idxs[:-10]:\n",
        "        old_exemplar_idxs_a += idxs[:l_a]\n",
        "        old_exemplar_idxs_b += idxs[l_a:]\n",
        "\n",
        "      old_mnemonics_data_a = torch.zeros((task_num*10*l_a, 3, 32, 32))\n",
        "      old_mnemonics_data_b = torch.zeros((task_num*10*l_b, 3, 32, 32))\n",
        "\n",
        "      for i, idx in enumerate(old_exemplar_idxs_a):\n",
        "        old_mnemonics_data_a[i, :, :, :] = self.original_training_set.__getitem__(int(idx))[1]\n",
        "      \n",
        "      for i, idx in enumerate(old_exemplar_idxs_b):\n",
        "          old_mnemonics_data_b[i, :, :, :] = self.original_training_set.__getitem__(int(idx))[1]\n",
        "      \n",
        "      old_mnemonics_a = nn.ParameterList()\n",
        "      old_mnemonics_a.append(nn.Parameter(old_mnemonics_data_a))\n",
        "      old_mnemonics_a.to(self.device)\n",
        "      old_mnemonics_b = nn.ParameterList()\n",
        "      old_mnemonics_b.append(nn.Parameter(old_mnemonics_data_b))\n",
        "      old_mnemonics_b.to(self.device)\n",
        "\n",
        "      options_old_a = {'finetuning_idxs':old_exemplar_idxs_a, \n",
        "                       'training_idxs':old_exemplar_idxs_b, \n",
        "                       'mnemonics_to_optimize':old_mnemonics_a, \n",
        "                       'batch_size':128,\n",
        "                       'm': l_a,\n",
        "                       'new':False}\n",
        "\n",
        "      options_old_b = {'finetuning_idxs':old_exemplar_idxs_b, \n",
        "                       'training_idxs':old_exemplar_idxs_a, \n",
        "                       'mnemonics_to_optimize':old_mnemonics_b, \n",
        "                       'batch_size':128,\n",
        "                       'm':l_b,\n",
        "                       'new':False}\n",
        "\n",
        "      self.update_params(**options_old_a) \n",
        "      self.update_params(**options_old_b)\n",
        "\n",
        "      # CONVERT AND STORE UPDATED EXEMPLAR as numpy array\n",
        "\n",
        "      for i, idx in enumerate(old_exemplar_idxs_a):\n",
        "        self.original_exemplar_set.dataset.data[idx] = tensor2im(old_mnemonics_a[0][i])\n",
        "\n",
        "      for i, idx in enumerate(old_exemplar_idxs_b):\n",
        "        self.original_exemplar_set.dataset.data[idx] = tensor2im(old_mnemonics_b[0][i])\n",
        "    \n",
        "\n",
        "\n",
        "    # FINE TUNE THE CURRENT NET ON ALL THE EXEMPLARS COLLECTED 'TILL NOW\n",
        "   \n",
        "  '''\n",
        "\n",
        "  def model_level_optimization(self):\n",
        "    \n",
        "    old_model = copy.deepcopy(self.model)\n",
        "    old_model.eval()\n",
        "    old_model.to(self.device)\n",
        "    n_classes = self.classes_seen+self.CLASSES_PER_BATCH\n",
        "    print(n_classes)\n",
        "    optimizer = optim.SGD(self.model.parameters(), lr=self.lr, momentum=self.momentum, weight_decay=self.weight_decay)\n",
        "    scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=self.milestones, gamma=self.gamma)\n",
        "    for epoch in tqdm(range(self.numepochs)):\n",
        "        \n",
        "      for _, inputs, labels in self.trainloader:\n",
        "        inputs = inputs.float().to(self.device)\n",
        "        labels = torch.tensor([self.diz[c.item()] for c in labels])\n",
        "\n",
        "        labels=labels.to(self.device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs=self.model(inputs)\n",
        "\n",
        "        labels_encoded = F.one_hot(labels,100).float().to(self.device) #CAMBIARE ONE_HOT\n",
        "        \n",
        "        if self.classes_seen:\n",
        "          old_target = old_model(inputs).to(self.device)\n",
        "          old_target = torch.sigmoid(old_target).to(self.device)\n",
        "          \n",
        "          target = torch.cat((old_target[:,:self.classes_seen], labels_encoded[:, self.classes_seen:]), dim=1)\n",
        "          loss = self.criterion(outputs, target)\n",
        "        else:\n",
        "          loss = self.criterion(outputs, labels_encoded) \n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "      \n",
        "      scheduler.step()\n",
        "\n",
        "\n",
        "  def classify_nme(self, input_batch):\n",
        "    min_distances = float('inf')*torch.ones(len(input_batch)).to(self.device) # shape: batch_size --> 128\n",
        "    y_pred = torch.zeros(len(input_batch), dtype=torch.int8).to(self.device) # shape: batch_size --> 128\n",
        "    input_features = self.model.features(input_batch) # shape: (batch_size, feature_size) --> (128, 64)\n",
        "\n",
        "    for i in range(len(self.exemplar_sets_idxs)):\n",
        "      ex_mean = self.exemplar_means[i,:]\n",
        "\n",
        "      # compute distances between inputs features and exemplar set means\n",
        "      pdist = nn.PairwiseDistance(p=2)\n",
        "      distances = pdist(input_features, ex_mean) # shape: batch_size --> 128\n",
        "\n",
        "      # update min distancies and predicted labels\n",
        "      mask = distances < min_distances\n",
        "      min_distances[mask] = distances[mask]\n",
        "      y_pred[mask] = self.exemplar_labels[i]\n",
        "\n",
        "    return y_pred\n",
        "    \n",
        "\n",
        "\n",
        "  def get_new_exemplars(self, batch, m):\n",
        "    loader = torch.utils.data.DataLoader(batch, batch_size=self.batch_size,shuffle=False, num_workers=4)\n",
        "    features = np.zeros((0,self.feature_size))\n",
        "    indices = np.zeros((0), dtype=int)\n",
        "    with torch.no_grad():\n",
        "      for indexes, images, labels in loader:\n",
        "        images = images.cuda()\n",
        "        feature = self.feature_extractor(images).data.cpu().numpy()\n",
        "        feature = normalize(feature, axis=1, norm='l2')\n",
        "        features = np.concatenate((features,feature), axis=0)\n",
        "        indices = np.concatenate((indices,indexes), axis=0)\n",
        "\n",
        "    class_mean = np.mean(features, axis=0)\n",
        "    class_mean = class_mean / np.linalg.norm(class_mean)  # Normalize\n",
        "\n",
        "    #self.cumulative_class_mean.append(class_mean)\n",
        "\n",
        "    exemplar_set = []\n",
        "    exemplar_features = np.zeros((0,self.feature_size))\n",
        "\n",
        "    for k in range(1, int(m)+1):\n",
        "        S = np.sum(exemplar_features, axis=0)\n",
        "        phi = features\n",
        "        mu = class_mean\n",
        "        mu_p = 1.0 / k * (phi + S)\n",
        "        mu_p = normalize(mu_p, axis=1, norm='l2')\n",
        "        i = np.argmin(np.sqrt(np.sum((mu - mu_p) ** 2, axis=1)))\n",
        "        exemplar_set.append(int(indices[i]))\n",
        "        addfeature =  np.expand_dims(features[i], axis=0)\n",
        "        exemplar_features = np.concatenate((exemplar_features,addfeature), axis=0)\n",
        "\n",
        "        #remove duplicates\n",
        "        features = np.delete(features, i, 0)\n",
        "        indices = np.delete(indices, i, 0)\n",
        "        \n",
        "    self.exemplar_sets_idxs.append(exemplar_set)\n",
        "    #self.exemplar_sets_idxs.append(random.sample(list(batch), m))\n",
        "\n",
        "\n",
        "  def reduce_old_exemplars(self, m):\n",
        "    for i, set_i in enumerate(self.exemplar_sets_idxs):\n",
        "      self.exemplar_sets_idxs[i] = random.sample(set_i, m)\n",
        "\n",
        "  def __accuracy_fc(self, dl, mapper):\n",
        "    total = 0.0\n",
        "    correct = 0.0\n",
        "    for  _, images, labels in dl:\n",
        "      labels = torch.tensor([torch.tensor(mapper[c.item()]) for c in labels])\n",
        "      labels = labels.to(self.device)\n",
        "      images = images.to(self.device)\n",
        "      outputs = self.model(images)\n",
        "      _, preds = torch.max(outputs, dim=1)\n",
        "      total += len(labels)\n",
        "      correct += torch.sum(preds == labels).item()\n",
        "\n",
        "    acc = correct / total\n",
        "    return acc\n",
        "\n",
        "\n",
        "  def __accuracy_nme(self, dl):\n",
        "    \n",
        "    total = 0.0\n",
        "    correct = 0.0\n",
        "    \n",
        "    for  _, images, labels in dl:\n",
        "      labels = labels.to(self.device)\n",
        "      images = images.to(self.device)\n",
        "      preds = self.classify_nme(images)\n",
        "      total += len(labels)\n",
        "      correct += torch.sum(preds == labels).item()\n",
        "\n",
        "      if self.last_test:\n",
        "        self.y_pred += preds.tolist()\n",
        "        self.y_test += labels.tolist()\n",
        "\n",
        "    acc = correct / total\n",
        "    return acc\n",
        "\n",
        "\n",
        "  def plot_confusion_matrix(self):\n",
        " \n",
        "    cm = confusion_matrix(self.y_test, self.y_pred)\n",
        "    cm = np.log(cm+1)\n",
        "    fig, ax = plt.subplots(figsize=(7,7))\n",
        "    sns.heatmap(cm, square=True, cbar=False, ax=ax, cmap=plt.get_cmap('seismic'))\n",
        "    ax.set_xticks(np.linspace(19,99,5))\n",
        "    ax.set_yticks(np.linspace(19,99,5))\n",
        "    ax.set_xticklabels([20,40,60,80,100], rotation=0)\n",
        "    ax.set_yticklabels([20,40,60,80,100], rotation=0)\n",
        "    ax.set_title(\"iCaRL\")\n",
        "    ax.set_xlabel(\"Predicted class\")\n",
        "    ax.set_ylabel(\"True class\")\n",
        "    plt.savefig(\"iCaRL_\"+str(self.randomseed)+\"_cm.png\")\n",
        "    plt.show()\n",
        "    return cm\n",
        "\n",
        "  def plot_data(self, train_dl):\n",
        "\n",
        "    from sklearn.manifold import TSNE\n",
        "    print('------plot data------')\n",
        "\n",
        "    #Data points\n",
        "    train_labels_array = torch.zeros(0).to('cuda')\n",
        "    train_dataset_to_reduce = np.zeros((0,64), dtype = float)\n",
        "\n",
        "    for  _, images, labels in train_dl:\n",
        "      labels = labels.to(self.device)\n",
        "      images = images.to(self.device)\n",
        "      train_dataset_to_reduce = np.concatenate((train_dataset_to_reduce, self.feature_extractor(images).cpu().detach().numpy()))\n",
        "      train_labels_array = torch.cat((train_labels_array, labels))\n",
        "\n",
        "    \n",
        "    #EX e MN loaders \n",
        "    current_exemplar_indices = np.array([], dtype=int)\n",
        "\n",
        "    for exemplar_set in self.exemplar_sets_idxs:\n",
        "      current_exemplar_indices = np.concatenate([current_exemplar_indices, np.array(exemplar_set)])\n",
        "\n",
        "    exemplar_dataset = Subset(self.original_training_set, current_exemplar_indices)\n",
        "    ex_dl = DataLoader(exemplar_dataset, batch_size=self.batch_size, shuffle=False, num_workers=4, drop_last=True) #usato solo per plottare\n",
        "\n",
        "    mn_dataset = Subset(self.original_exemplar_set, current_exemplar_indices)\n",
        "    mn_dl = DataLoader(mn_dataset, batch_size=self.batch_size, shuffle=False, num_workers=4, drop_last=True) #usato solo per plottare\n",
        "\n",
        "\n",
        "    #Exemplars\n",
        "\n",
        "    ex_labels_array = torch.zeros(0).to('cuda')\n",
        "    ex_dataset_to_reduce = np.zeros((0,64), dtype = float)\n",
        "\n",
        "    for  _, images, labels in ex_dl:\n",
        "      labels = labels.to(self.device)\n",
        "      images = images.to(self.device)\n",
        "      ex_dataset_to_reduce = np.concatenate((ex_dataset_to_reduce, self.feature_extractor(images).cpu().detach().numpy()))\n",
        "      ex_labels_array = torch.cat((ex_labels_array, labels), dim = 0)\n",
        "\n",
        "\n",
        "    #Mnemonics\n",
        "\n",
        "\n",
        "    mn_labels_array = torch.zeros(0).to('cuda')\n",
        "    mn_dataset_to_reduce = np.zeros((0,64), dtype = float)\n",
        "\n",
        "    for  _, images, labels in mn_dl:\n",
        "      labels = labels.to(self.device)\n",
        "      images = images.to(self.device)\n",
        "      mn_dataset_to_reduce = np.concatenate((mn_dataset_to_reduce, self.feature_extractor(images).cpu().detach().numpy()))\n",
        "      mn_labels_array = torch.cat((mn_labels_array, labels), dim = 0)\n",
        "\n",
        "    #PLOT'''\n",
        "    total_data_w_exemplars = np.concatenate((train_dataset_to_reduce, ex_dataset_to_reduce))\n",
        "    total_data_w_mn =  np.concatenate((train_dataset_to_reduce, mn_dataset_to_reduce))\n",
        "\n",
        "    total_transformed_ex = TSNE(n_components=2).fit_transform(total_data_w_exemplars)\n",
        "    X_transformed_w_ex = total_transformed_ex[:train_dataset_to_reduce.shape[0]]\n",
        "    ex_transformed = total_transformed_ex[train_dataset_to_reduce.shape[0]:]\n",
        "\n",
        "    total_transformed_mn = TSNE(n_components=2).fit_transform(total_data_w_mn)\n",
        "    X_transformed_w_mn = total_transformed_mn[:train_dataset_to_reduce.shape[0]]\n",
        "    mn_transformed = total_transformed_mn[train_dataset_to_reduce.shape[0]:]\n",
        "\n",
        "    fig, (ax1, ax2) = plt.subplots(1,2, figsize=(24,12))\n",
        "    ax1.scatter(X_transformed_w_ex[:,0], X_transformed_w_ex[:,1], c = train_labels_array.cpu(), alpha = 0.1)\n",
        "    ax1.scatter(ex_transformed[:,0], ex_transformed[:,1], c = ex_labels_array.cpu(), alpha = 1)\n",
        "    #ax1.title('EXEMPLARS')\n",
        "\n",
        "    ax2.scatter(X_transformed_w_mn[:,0], X_transformed_w_mn[:,1], c = train_labels_array.cpu(), alpha = 0.1)\n",
        "    ax2.scatter(mn_transformed[:,0], mn_transformed[:,1], c = mn_labels_array.cpu(), alpha = 1)\n",
        "    #ax2.title('MNEMONICS')\n",
        "    plt.show()\n",
        "\n",
        "  def trainer(self):\n",
        "    \n",
        "    train_indices = self.original_training_set.get_batch_indexes()\n",
        "    test_indices = self.original_test_set.get_batch_indexes()\n",
        "    batches=self.original_training_set.getbatches()\n",
        "    current_test_indexes=[]\n",
        "    test_acc = []\n",
        "    self.last_test = False\n",
        "\n",
        "    for i in range(self.NUM_BATCHES):\n",
        "      print('current batches', batches[i])\n",
        "      if i == self.NUM_BATCHES-1:\n",
        "        self.last_test = True\n",
        "\n",
        "      current_exemplar_indices = np.array([], dtype=int)\n",
        "    \n",
        "      for exemplar_set in self.exemplar_sets_idxs:\n",
        "        current_exemplar_indices = np.concatenate([current_exemplar_indices, np.array(exemplar_set)])\n",
        "\n",
        "      exemplar_dataset = Subset(self.original_training_set, current_exemplar_indices) \n",
        "      #DA CAMBIARE CON SELF.ORIGINAL EXEMPLAR SET\n",
        "      if i > 1: #FINETUNING\n",
        "        print('----inizio finetuning----')\n",
        "        print('numbero of classes in the exemplar sets', len(self.exemplar_sets_idxs))\n",
        "        self.numepochs = 10\n",
        "        self.lr = 0.2\n",
        "        temporary_classes_seen = self.classes_seen\n",
        "        self.classes_seen = 0\n",
        "        self.trainloader = DataLoader(exemplar_dataset, batch_size=self.batch_size, shuffle=False, num_workers=4, drop_last=True) \n",
        "        print('accuracy on exemplar set before finetuining:', 100*self.__accuracy_fc(self.trainloader,self.diz))\n",
        "        self.model.train()\n",
        "        self.model_level_optimization()\n",
        "        #BACK TO THE NORMAL PARAMETERS\n",
        "        self.model.eval()\n",
        "        self.numepochs = 70\n",
        "        self.lr = 2\n",
        "        self.classes_seen = temporary_classes_seen\n",
        "        print('accuracy on exemplar set after finetuining:', 100*self.__accuracy_fc(self.trainloader,self.diz))\n",
        "        current_test_acc = self.__accuracy_nme(self.testloader)\n",
        "        print('accuracy on test set after finetuning:', 100*current_test_acc)\n",
        "        print('-----fine finetuning------')\n",
        "        print('-'*80)\n",
        "\n",
        "      train_dataset = Subset(self.original_training_set, train_indices[i])\n",
        "      current_test_indexes += test_indices[i].tolist()\n",
        "      test_dataset = Subset(self.original_test_set,current_test_indexes)\n",
        "      self.train_loader = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True, num_workers=4, drop_last=True)\n",
        "      self.testloader = DataLoader(test_dataset, batch_size=self.batch_size, shuffle=False, num_workers=4, drop_last=True)        \n",
        "      \n",
        "\n",
        "      if i == 0:\n",
        "        self.trainloader = self.train_loader\n",
        "      else:\n",
        "        self.trainloader = DataLoader(torch.utils.data.ConcatDataset([train_dataset, exemplar_dataset]), batch_size=self.batch_size, shuffle=True,\n",
        "          num_workers=4, pin_memory=True)\n",
        "\n",
        "        \n",
        "      self.model.train()\n",
        "      self.model_level_optimization()    \n",
        "      self.classes_seen += 10\n",
        "      self.model.eval() # Set Network to evaluation mode\n",
        "      m=int(2000/(int(i*10+10)))\n",
        "      return self.model, batches[i]\n",
        "      \n",
        "      break\n",
        "'''\n",
        "      #NUOVO PAPER DEL PORCODDIO\n",
        "      labels_of_modified = torch.zeros(0, dtype = int).to('cuda')\n",
        "      for label in batches[i]:\n",
        "        labels = torch.LongTensor([self.diz[label]]*m).to('cuda')\n",
        "        labels_of_modified = torch.cat((labels_of_modified, labels), dim=0)\n",
        "      print('labels to be created', labels_of_modified)\n",
        "      print('len to be created', len(labels_of_modified))\n",
        "      number_of_images_created = m*10\n",
        "      net_student = resnet32(num_classes=100).to(self.device)\n",
        "      data_type = torch.float\n",
        "      inputs = torch.randn((number_of_images_created, 3, 32, 32), requires_grad=True, device='cuda', dtype=data_type)\n",
        "\n",
        "      net_student = copy.deepcopy(self.model)\n",
        "      net_student.eval() #important, otherwise generated images will be non natural\n",
        "      \n",
        "      train_writer = None  # tensorboard writter\n",
        "      global_iteration = 0\n",
        "      di_lr = 0.05\n",
        "      optimizer_di = optim.Adam([inputs], lr=di_lr)\n",
        "\n",
        "      print(\"Starting model inversion\")\n",
        "      batch_idx = 0\n",
        "      inputs = get_images(net=self.model, bs=200, epochs=2000, idx=batch_idx,\n",
        "                        net_student=net_student,\n",
        "                        train_writer=train_writer, use_amp=False,\n",
        "                        optimizer=optimizer_di, inputs=inputs, \n",
        "                        var_scale=0.00005, labels=labels)\n",
        "\n",
        "\n",
        "      plt.imshow(tensor2im(inputs[0]))\n",
        "      plt.show()\n",
        "      plt.imshow(tensor2im(inputs[55]))\n",
        "      plt.show()\n",
        "      print('deepinversion finshed')\n",
        "      # update exemplars number\n",
        "      \n",
        "\n",
        "      # reduce the number of each exemplars set\n",
        "      self.reduce_old_exemplars(m) \n",
        "\n",
        "      self.cumulative_class_mean = {}\n",
        "\n",
        "      for classlabel in batches[i]:\n",
        "        indexes_class = self.original_training_set.get_class_indexes(classlabel)\n",
        "        current_class = Subset(self.original_training_set, indexes_class)\n",
        "        #self.get_new_exemplars(indexes_class, m)\n",
        "        self.get_new_exemplars(current_class, m)\n",
        "        indexes_class = self.original_training_set.get_class_indexes(classlabel)\n",
        "        current_class = Subset(self.original_training_set, indexes_class)\n",
        "\n",
        "        loader = torch.utils.data.DataLoader(current_class, batch_size=self.batch_size,shuffle=False, num_workers=4)\n",
        "        features = np.zeros((0, 64))\n",
        "        with torch.no_grad():\n",
        "          for indexes, images, labels in loader:\n",
        "            images = images.cuda()\n",
        "            feature = self.feature_extractor(images).data.cpu().numpy()\n",
        "            feature = normalize(feature, axis=1, norm='l2')\n",
        "            features = np.concatenate((features,feature), axis=0)\n",
        "\n",
        "        class_mean = np.mean(features, axis=0)\n",
        "        class_mean = class_mean / np.linalg.norm(class_mean)  # Normalize\n",
        "\n",
        "        self.cumulative_class_mean[classlabel] = class_mean\n",
        "        \n",
        "      #plt.imshow(self.original_exemplar_set.dataset.data[self.exemplar_sets_idxs[0][0]])\n",
        "      #plt.show()\n",
        "      \n",
        "  \n",
        "      #self.exemplar_level_optimization(m, i, self.original_training_set.get_batch_indexes()[i])\n",
        "\n",
        "      # compute means of exemplar set\n",
        "      # cycle for each exemplar set\n",
        "      self.exemplar_means = torch.zeros((0, self.feature_size), dtype=torch.float).to(self.device)\n",
        "      self.exemplar_labels = []\n",
        "      for j in range(len(self.exemplar_sets_idxs)):\n",
        "        exemplars_dataset = Subset(self.original_training_set, self.exemplar_sets_idxs[j])\n",
        "        exemplars_loader = torch.utils.data.DataLoader(exemplars_dataset, batch_size=self.batch_size, shuffle=False, num_workers=4)\n",
        "        ex_features = torch.zeros((0, self.feature_size), dtype=torch.float).to(self.device) # alla fine shape: (len(exemplar_set), feature_size) --> (m, 64)\n",
        "      \n",
        "        with torch.no_grad():\n",
        "          _, _, exemplar_label = self.original_training_set.__getitem__(self.exemplar_sets_idxs[j][0]) \n",
        "          self.exemplar_labels.append(exemplar_label)\n",
        "          # cycle for each batch in the current exemplar set\n",
        "          for _,  exemplars, _ in exemplars_loader:\n",
        "          \n",
        "            # get exemplars features\n",
        "            exemplars = exemplars.to(self.device)\n",
        "            features = self.model.features(exemplars) # shape: (len(exemplars), feature_size)\n",
        "          \n",
        "            # normalize \n",
        "            feature_norms = torch.norm(features, p=2, dim=1) # shape: len(exemplars)\n",
        "            feature_norms.unsqueeze_(1) # shape: (len(exemplars), 1)\n",
        "            features = features/feature_norms\n",
        "          \n",
        "            # concatenate over columns\n",
        "            ex_features = torch.cat((ex_features, features), dim=0)\n",
        "          \n",
        "        # compute current exemplar set mean and normalize it\n",
        "        ex_mean = torch.mean(ex_features, dim=0) # shape: feature_size --> 64\n",
        "        ex_mean = ex_mean/torch.norm(ex_mean)\n",
        "        ex_mean.unsqueeze_(0) # shape: (1, feature_size) --> (1, 64)\n",
        "        self.exemplar_means = torch.cat((self.exemplar_means, ex_mean), dim=0) # shape: (n_examplar sets, feature size)\n",
        "      \n",
        "      #if i == 0:\n",
        "       # self.plot_data(self.trainloader)\n",
        "      print('accuracy on training set:', 100*self.__accuracy_fc(self.trainloader,self.diz))\n",
        "      # print('accuracy on test set:', self.__accuracy_on(self.testloader,self,self.diz))\n",
        "      current_test_acc = self.__accuracy_nme(self.testloader)\n",
        "      print('accuracy on test set:', 100*current_test_acc)\n",
        "      print('-' * 80)\n",
        "      test_acc.append(current_test_acc)\n",
        "\n",
        "    # compute comfusion matrix and save results\n",
        "    cm = self.plot_confusion_matrix()\n",
        "    with open('iCaRL_'+str(self.randomseed)+\"_cm\", 'wb') as file:\n",
        "      pickle.dump(cm, file, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "    with open('iCaRL_'+str(self.randomseed)+\"_testacc\", 'wb') as file:\n",
        "      pickle.dump(test_acc, file, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "      print('PRINT IMAGES')\n",
        "      print('with data augmentation')\n",
        "      plt.imshow(self.original_training_set.dataset.data[self.exemplar_sets_idxs[0][0]])\n",
        "      plt.show()\n",
        "      print('without data augmentation')\n",
        "      plt.imshow(self.original_exemplar_set.dataset.data[self.exemplar_sets_idxs[0][0]])\n",
        "      plt.show()\n",
        "'''"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n      #NUOVO PAPER DEL PORCODDIO\\n      labels_of_modified = torch.zeros(0, dtype = int).to(\\'cuda\\')\\n      for label in batches[i]:\\n        labels = torch.LongTensor([self.diz[label]]*m).to(\\'cuda\\')\\n        labels_of_modified = torch.cat((labels_of_modified, labels), dim=0)\\n      print(\\'labels to be created\\', labels_of_modified)\\n      print(\\'len to be created\\', len(labels_of_modified))\\n      number_of_images_created = m*10\\n      net_student = resnet32(num_classes=100).to(self.device)\\n      data_type = torch.float\\n      inputs = torch.randn((number_of_images_created, 3, 32, 32), requires_grad=True, device=\\'cuda\\', dtype=data_type)\\n\\n      net_student = copy.deepcopy(self.model)\\n      net_student.eval() #important, otherwise generated images will be non natural\\n      \\n      train_writer = None  # tensorboard writter\\n      global_iteration = 0\\n      di_lr = 0.05\\n      optimizer_di = optim.Adam([inputs], lr=di_lr)\\n\\n      print(\"Starting model inversion\")\\n      batch_idx = 0\\n      inputs = get_images(net=self.model, bs=200, epochs=2000, idx=batch_idx,\\n                        net_student=net_student,\\n                        train_writer=train_writer, use_amp=False,\\n                        optimizer=optimizer_di, inputs=inputs, \\n                        var_scale=0.00005, labels=labels)\\n\\n\\n      plt.imshow(tensor2im(inputs[0]))\\n      plt.show()\\n      plt.imshow(tensor2im(inputs[55]))\\n      plt.show()\\n      print(\\'deepinversion finshed\\')\\n      # update exemplars number\\n      \\n\\n      # reduce the number of each exemplars set\\n      self.reduce_old_exemplars(m) \\n\\n      self.cumulative_class_mean = {}\\n\\n      for classlabel in batches[i]:\\n        indexes_class = self.original_training_set.get_class_indexes(classlabel)\\n        current_class = Subset(self.original_training_set, indexes_class)\\n        #self.get_new_exemplars(indexes_class, m)\\n        self.get_new_exemplars(current_class, m)\\n        indexes_class = self.original_training_set.get_class_indexes(classlabel)\\n        current_class = Subset(self.original_training_set, indexes_class)\\n\\n        loader = torch.utils.data.DataLoader(current_class, batch_size=self.batch_size,shuffle=False, num_workers=4)\\n        features = np.zeros((0, 64))\\n        with torch.no_grad():\\n          for indexes, images, labels in loader:\\n            images = images.cuda()\\n            feature = self.feature_extractor(images).data.cpu().numpy()\\n            feature = normalize(feature, axis=1, norm=\\'l2\\')\\n            features = np.concatenate((features,feature), axis=0)\\n\\n        class_mean = np.mean(features, axis=0)\\n        class_mean = class_mean / np.linalg.norm(class_mean)  # Normalize\\n\\n        self.cumulative_class_mean[classlabel] = class_mean\\n        \\n      #plt.imshow(self.original_exemplar_set.dataset.data[self.exemplar_sets_idxs[0][0]])\\n      #plt.show()\\n      \\n  \\n      #self.exemplar_level_optimization(m, i, self.original_training_set.get_batch_indexes()[i])\\n\\n      # compute means of exemplar set\\n      # cycle for each exemplar set\\n      self.exemplar_means = torch.zeros((0, self.feature_size), dtype=torch.float).to(self.device)\\n      self.exemplar_labels = []\\n      for j in range(len(self.exemplar_sets_idxs)):\\n        exemplars_dataset = Subset(self.original_training_set, self.exemplar_sets_idxs[j])\\n        exemplars_loader = torch.utils.data.DataLoader(exemplars_dataset, batch_size=self.batch_size, shuffle=False, num_workers=4)\\n        ex_features = torch.zeros((0, self.feature_size), dtype=torch.float).to(self.device) # alla fine shape: (len(exemplar_set), feature_size) --> (m, 64)\\n      \\n        with torch.no_grad():\\n          _, _, exemplar_label = self.original_training_set.__getitem__(self.exemplar_sets_idxs[j][0]) \\n          self.exemplar_labels.append(exemplar_label)\\n          # cycle for each batch in the current exemplar set\\n          for _,  exemplars, _ in exemplars_loader:\\n          \\n            # get exemplars features\\n            exemplars = exemplars.to(self.device)\\n            features = self.model.features(exemplars) # shape: (len(exemplars), feature_size)\\n          \\n            # normalize \\n            feature_norms = torch.norm(features, p=2, dim=1) # shape: len(exemplars)\\n            feature_norms.unsqueeze_(1) # shape: (len(exemplars), 1)\\n            features = features/feature_norms\\n          \\n            # concatenate over columns\\n            ex_features = torch.cat((ex_features, features), dim=0)\\n          \\n        # compute current exemplar set mean and normalize it\\n        ex_mean = torch.mean(ex_features, dim=0) # shape: feature_size --> 64\\n        ex_mean = ex_mean/torch.norm(ex_mean)\\n        ex_mean.unsqueeze_(0) # shape: (1, feature_size) --> (1, 64)\\n        self.exemplar_means = torch.cat((self.exemplar_means, ex_mean), dim=0) # shape: (n_examplar sets, feature size)\\n      \\n      #if i == 0:\\n       # self.plot_data(self.trainloader)\\n      print(\\'accuracy on training set:\\', 100*self.__accuracy_fc(self.trainloader,self.diz))\\n      # print(\\'accuracy on test set:\\', self.__accuracy_on(self.testloader,self,self.diz))\\n      current_test_acc = self.__accuracy_nme(self.testloader)\\n      print(\\'accuracy on test set:\\', 100*current_test_acc)\\n      print(\\'-\\' * 80)\\n      test_acc.append(current_test_acc)\\n\\n    # compute comfusion matrix and save results\\n    cm = self.plot_confusion_matrix()\\n    with open(\\'iCaRL_\\'+str(self.randomseed)+\"_cm\", \\'wb\\') as file:\\n      pickle.dump(cm, file, protocol=pickle.HIGHEST_PROTOCOL)\\n    with open(\\'iCaRL_\\'+str(self.randomseed)+\"_testacc\", \\'wb\\') as file:\\n      pickle.dump(test_acc, file, protocol=pickle.HIGHEST_PROTOCOL)\\n\\n      print(\\'PRINT IMAGES\\')\\n      print(\\'with data augmentation\\')\\n      plt.imshow(self.original_training_set.dataset.data[self.exemplar_sets_idxs[0][0]])\\n      plt.show()\\n      print(\\'without data augmentation\\')\\n      plt.imshow(self.original_exemplar_set.dataset.data[self.exemplar_sets_idxs[0][0]])\\n      plt.show()\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RtarWXiNb9nG",
        "outputId": "8879836d-2619-4d64-d687-c55c3a8c3f93"
      },
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "from __future__ import unicode_literals\n",
        "\n",
        "import argparse\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "# import torch.nn.parallel\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.optim as optim\n",
        "# import torch.utils.data\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.utils as vutils\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import glob\n",
        "import collections\n",
        "\n",
        "#from resnet_cifar import ResNet34, ResNet18\n",
        "\n",
        "try:\n",
        "    from apex.parallel import DistributedDataParallel as DDP\n",
        "    from apex import amp, optimizers\n",
        "    USE_APEX = True\n",
        "except ImportError:\n",
        "    print(\"Please install apex from https://www.github.com/nvidia/apex to run this example.\")\n",
        "    print(\"will attempt to run without it\")\n",
        "    USE_APEX = False\n",
        "\n",
        "#provide intermeiate information\n",
        "debug_output = False\n",
        "debug_output = True\n",
        "\n",
        "\n",
        "class DeepInversionFeatureHook():\n",
        "    '''\n",
        "    Implementation of the forward hook to track feature statistics and compute a loss on them.\n",
        "    Will compute mean and variance, and will use l2 as a loss\n",
        "    '''\n",
        "\n",
        "    def __init__(self, module):\n",
        "        self.hook = module.register_forward_hook(self.hook_fn)\n",
        "\n",
        "    def hook_fn(self, module, input, output):\n",
        "        # hook co compute deepinversion's feature distribution regularization\n",
        "        nch = input[0].shape[1]\n",
        "\n",
        "        mean = input[0].mean([0, 2, 3])\n",
        "        var = input[0].permute(1, 0, 2, 3).contiguous().view([nch, -1]).var(1, unbiased=False)\n",
        "\n",
        "        # forcing mean and variance to match between two distributions\n",
        "        # other ways might work better, e.g. KL divergence\n",
        "        r_feature = torch.norm(module.running_var.data.type(var.type()) - var, 2) + torch.norm(\n",
        "            module.running_mean.data.type(var.type()) - mean, 2)\n",
        "\n",
        "        self.r_feature = r_feature\n",
        "        # must have no output\n",
        "\n",
        "    def close(self):\n",
        "        self.hook.remove()\n",
        "\n",
        "def get_images(net, bs=256, epochs=1000, idx=-1, var_scale=0.00005, competitive_scale=0.01,\n",
        "               net_student=None, prefix=None, train_writer = None, global_iteration=None,\n",
        "               use_amp=False, bn_reg_scale = 0.0,\n",
        "               optimizer = None, inputs = None, labels = False, l2_coeff=0.0):\n",
        "    '''\n",
        "    Function returns inverted images from the pretrained model, parameters are tight to CIFAR dataset\n",
        "    args in:\n",
        "        net: network to be inverted\n",
        "        bs: batch size\n",
        "        epochs: total number of iterations to generate inverted images, training longer helps a lot!\n",
        "        idx: an external flag for printing purposes: only print in the first round, set as -1 to disable\n",
        "        var_scale: the scaling factor for variance loss regularization. this may vary depending on bs\n",
        "            larger - more blurred but less noise\n",
        "        net_student: model to be used for Adaptive DeepInversion\n",
        "        prefix: defines the path to store images\n",
        "        competitive_scale: coefficient for Adaptive DeepInversion\n",
        "        train_writer: tensorboardX object to store intermediate losses\n",
        "        global_iteration: indexer to be used for tensorboard\n",
        "        use_amp: boolean to indicate usage of APEX AMP for FP16 calculations - twice faster and less memory on TensorCores\n",
        "        optimizer: potimizer to be used for model inversion\n",
        "        inputs: data place holder for optimization, will be reinitialized to noise\n",
        "        bn_reg_scale: weight for r_feature_regularization\n",
        "        random_labels: sample labels from random distribution or use columns of the same class\n",
        "        l2_coeff: coefficient for L2 loss on input\n",
        "    return:\n",
        "        A tensor on GPU with shape (bs, 3, 32, 32) for CIFAR\n",
        "    '''\n",
        "\n",
        "    kl_loss = nn.KLDivLoss(reduction='batchmean').cuda()\n",
        "\n",
        "    # preventing backpropagation through student for Adaptive DeepInversion\n",
        "    net_student.eval()\n",
        "\n",
        "    best_cost = 1e6\n",
        "\n",
        "    # initialize gaussian inputs\n",
        "    inputs.data = torch.randn((bs, 3, 32, 32), requires_grad=True, device='cuda')\n",
        "    # if use_amp:\n",
        "    #     inputs.data = inputs.data.half()\n",
        "\n",
        "    # set up criteria for optimization\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    optimizer.state = collections.defaultdict(dict)  # Reset state of optimizer\n",
        "\n",
        "    # target outputs to generate\n",
        "    #if labels:\n",
        "    targets = labels\n",
        "    #else:\n",
        "     #   targets = torch.LongTensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9] * 25 + [0, 1, 2, 3, 4, 5]).to('cuda')\n",
        "\n",
        "    outputs=net(inputs.data)\n",
        "    _, predicted_teach = outputs.max(1)\n",
        "\n",
        "    outputs_student=net_student(inputs.data)\n",
        "    _, predicted_std = outputs_student.max(1)\n",
        "\n",
        "    if idx == 0:\n",
        "        print('Teacher correct out of {}: {}, loss at {}'.format(bs, predicted_teach.eq(targets).sum().item(), criterion(outputs, targets).item()))\n",
        "        print('Student correct out of {}: {}, loss at {}'.format(bs, predicted_std.eq(targets).sum().item(), criterion(outputs_student, targets).item()))\n",
        "\n",
        "    ## Create hooks for feature statistics catching\n",
        "    loss_r_feature_layers = []\n",
        "    for module in net.modules():\n",
        "        if isinstance(module, nn.BatchNorm2d):\n",
        "            loss_r_feature_layers.append(DeepInversionFeatureHook(module))\n",
        "\n",
        "    # setting up the range for jitter\n",
        "    lim_0, lim_1 = 2, 2\n",
        "\n",
        "    for epoch in tqdm(range(epochs)):\n",
        "        # apply random jitter offsets\n",
        "        off1 = random.randint(-lim_0, lim_0)\n",
        "        off2 = random.randint(-lim_1, lim_1)\n",
        "        inputs_jit = torch.roll(inputs, shifts=(off1,off2), dims=(2,3))\n",
        "\n",
        "        # foward with jit images\n",
        "        optimizer.zero_grad()\n",
        "        net.zero_grad()\n",
        "        outputs = net(inputs_jit)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss_target = loss.item()\n",
        "\n",
        "        # competition loss, Adaptive DeepInvesrion\n",
        "        if competitive_scale != 0.0:\n",
        "            net_student.zero_grad()\n",
        "            outputs_student = net_student(inputs_jit)\n",
        "            T = 3.0\n",
        "\n",
        "            if 1:\n",
        "                # jensen shanon divergence:\n",
        "                # another way to force KL between negative probabilities\n",
        "                P = F.softmax(outputs_student / T, dim=1)\n",
        "                Q = F.softmax(outputs / T, dim=1)\n",
        "                M = 0.5 * (P + Q)\n",
        "\n",
        "                P = torch.clamp(P, 0.01, 0.99)\n",
        "                Q = torch.clamp(Q, 0.01, 0.99)\n",
        "                M = torch.clamp(M, 0.01, 0.99)\n",
        "                eps = 0.0\n",
        "                # loss_verifier_cig = 0.5 * kl_loss(F.log_softmax(outputs_verifier / T, dim=1), M) +  0.5 * kl_loss(F.log_softmax(outputs/T, dim=1), M)\n",
        "                loss_verifier_cig = 0.5 * kl_loss(torch.log(P + eps), M) + 0.5 * kl_loss(torch.log(Q + eps), M)\n",
        "                # JS criteria - 0 means full correlation, 1 - means completely different\n",
        "                loss_verifier_cig = 1.0 - torch.clamp(loss_verifier_cig, 0.0, 1.0)\n",
        "\n",
        "                loss = loss + competitive_scale * loss_verifier_cig\n",
        "\n",
        "        # apply total variation regularization\n",
        "        diff1 = inputs_jit[:,:,:,:-1] - inputs_jit[:,:,:,1:]\n",
        "        diff2 = inputs_jit[:,:,:-1,:] - inputs_jit[:,:,1:,:]\n",
        "        diff3 = inputs_jit[:,:,1:,:-1] - inputs_jit[:,:,:-1,1:]\n",
        "        diff4 = inputs_jit[:,:,:-1,:-1] - inputs_jit[:,:,1:,1:]\n",
        "        loss_var = torch.norm(diff1) + torch.norm(diff2) + torch.norm(diff3) + torch.norm(diff4)\n",
        "        loss = loss + var_scale*loss_var\n",
        "\n",
        "        # R_feature loss\n",
        "        loss_distr = sum([mod.r_feature for mod in loss_r_feature_layers])\n",
        "        loss = loss + bn_reg_scale*loss_distr # best for noise before BN\n",
        "\n",
        "        # l2 loss\n",
        "        if 1:\n",
        "            loss = loss + l2_coeff * torch.norm(inputs_jit, 2)\n",
        "\n",
        "        if debug_output and epoch % 200==0:\n",
        "            print(f\"It {epoch}\\t Losses: total: {loss.item():3.3f},\\ttarget: {loss_target:3.3f} \\tR_feature_loss unscaled:\\t {loss_distr.item():3.3f}\")\n",
        "            #vutils.save_image(inputs.data.clone(),\n",
        "             #                 './{}/output_{}.png'.format(prefix, epoch//200),\n",
        "              #                normalize=True, scale_each=True, nrow=10)\n",
        "\n",
        "        if best_cost > loss.item():\n",
        "            best_cost = loss.item()\n",
        "            best_inputs = inputs.data\n",
        "\n",
        "        # backward pass\n",
        "        if use_amp:\n",
        "            with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
        "                scaled_loss.backward()\n",
        "        else:\n",
        "            loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "    outputs=net(best_inputs)\n",
        "    _, predicted_teach = outputs.max(1)\n",
        "\n",
        "    outputs_student=net_student(best_inputs)\n",
        "    _, predicted_std = outputs_student.max(1)\n",
        "\n",
        "    if idx == 0:\n",
        "        print('Teacher correct out of {}: {}, loss at {}'.format(bs, predicted_teach.eq(targets).sum().item(), criterion(outputs, targets).item()))\n",
        "        print('Student correct out of {}: {}, loss at {}'.format(bs, predicted_std.eq(targets).sum().item(), criterion(outputs_student, targets).item()))\n",
        "\n",
        "    name_use = \"best_images\"\n",
        "    if prefix is not None:\n",
        "        name_use = prefix + name_use\n",
        "    next_batch = len(glob.glob(\"./%s/*.png\" % name_use)) // 1\n",
        "\n",
        "    #vutils.save_image(best_inputs[:20].clone(),\n",
        "     #                 './{}/output_{}.png'.format(name_use, next_batch),\n",
        "      #                normalize=True, scale_each = True, nrow=10)\n",
        "\n",
        "    #if train_writer is not None:\n",
        "     #   train_writer.add_scalar('gener_teacher_criteria', criterion(outputs, targets), global_iteration)\n",
        "      #  train_writer.add_scalar('gener_student_criteria', criterion(outputs_student, targets), global_iteration)\n",
        "\n",
        "       # train_writer.add_scalar('gener_teacher_acc', predicted_teach.eq(targets).sum().item() / bs, global_iteration)\n",
        "       # train_writer.add_scalar('gener_student_acc', predicted_std.eq(targets).sum().item() / bs, global_iteration)\n",
        "\n",
        "        #train_writer.add_scalar('gener_loss_total', loss.item(), global_iteration)\n",
        "        #train_writer.add_scalar('gener_loss_var', (var_scale*loss_var).item(), global_iteration)\n",
        "\n",
        "    net_student.train()\n",
        "\n",
        "    return best_inputs"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Please install apex from https://www.github.com/nvidia/apex to run this example.\n",
            "will attempt to run without it\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151,
          "referenced_widgets": [
            "3b8162c55332427a8ae76e869991ed45",
            "579a97a5084e423b928b1d896750f435",
            "a78e748535174fb8aa7c3f8db5e2b9ad",
            "120454f355da46618d9888b0f870ea94",
            "8b07fc16b92b4970bacf9247ece8fcb0",
            "b1d36cb729c64d43b6ace41026bfdb6d",
            "00443eca221241d7b1c9ead7ec99a3e1",
            "e9f93eb3434c4329b2a74c53ab57df87"
          ]
        },
        "id": "Ap6fvXN9cB98",
        "outputId": "395eaaa0-82a7-40b6-e517-84af98ae2f00"
      },
      "source": [
        "method = mnemonics(randomseed=203)\n",
        "model, batch = method.trainer()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "current batches [11, 5, 62, 76, 27, 3, 96, 33, 78, 30]\n",
            "10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3b8162c55332427a8ae76e869991ed45",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=70.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168,
          "referenced_widgets": [
            "509acb6efdac486da16de095ec1e6ed8",
            "8ecaaf6b1a2843f1bf8a389a28a0f038",
            "149f1ec304fb4f70b15f232617d04426",
            "6f4ef1e423824dd38e2e920fe1a5d87a",
            "79cd8a2b51944d44a51232cc9b08222e",
            "1980561397c24b91a9ef8ea6de3fa144",
            "796ee09f6dc9472d80726fba6bd38b2e",
            "27642529b77c4d21812bd63897d8fb9a"
          ]
        },
        "id": "vdgNQmOTcD_V",
        "outputId": "8f6aceb4-f8a1-415b-fd00-a99ae6cdd108"
      },
      "source": [
        "# Train only FC layers -> Freeze convolutional Layers\n",
        "for param in model.parameters():\n",
        "  param.requires_grad = False\n",
        "\n",
        "model.fc = nn.Linear(64, 10).to('cuda')\n",
        "\n",
        "train_dataset = Subset(ilCIFAR100(10, 203, train = 'train'), ilCIFAR100(10, 203, train = 'train').get_batch_indexes()[0])\n",
        "test_dataset = Subset(ilCIFAR100(10, 203, train = 'test'), ilCIFAR100(10, 203, train = 'test').get_batch_indexes()[0])\n",
        "train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True, num_workers=4, drop_last=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=256, shuffle=True, num_workers=4, drop_last=True)\n",
        "diz = ilCIFAR100(10, 203, train = 'train').get_dict()\n",
        "\n",
        "\n",
        "# Prepare Training\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.02, momentum=0.9, weight_decay= 1e-5)\n",
        "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[49,63], gamma=0.2)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "for epoch in tqdm(range(70)):\n",
        "    \n",
        "  for _, inputs, labels in train_loader:\n",
        "    inputs = inputs.float().to('cuda')\n",
        "    labels = torch.tensor([diz[c.item()] for c in labels])\n",
        "\n",
        "    labels=labels.to('cuda')\n",
        "    optimizer.zero_grad()\n",
        "    outputs=model(inputs)\n",
        "\n",
        "    labels_encoded = F.one_hot(labels,10).float().to('cuda') #CAMBIARE ONE_HOT\n",
        "    loss = criterion(outputs, labels_encoded) \n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "  \n",
        "  scheduler.step()\n",
        "\n",
        "total = 0.0\n",
        "correct = 0.0\n",
        "for  _, images, labels in test_loader:\n",
        "  labels = torch.tensor([torch.tensor(diz[c.item()]) for c in labels])\n",
        "  labels = labels.to('cuda')\n",
        "  images = images.to('cuda')\n",
        "  outputs = model(images)\n",
        "  _, preds = torch.max(outputs, dim=1)\n",
        "  total += len(labels)\n",
        "  correct += torch.sum(preds == labels).item()\n",
        "\n",
        "acc = correct / total\n",
        "print('test accuracy', acc)\n",
        "\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "509acb6efdac486da16de095ec1e6ed8",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=70.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "test accuracy 0.82421875\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sIsGltQmiHm4"
      },
      "source": [
        "for param in model.parameters():\n",
        "  param.requires_grad = True"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 768,
          "referenced_widgets": [
            "2881a7f479d64056ac89148fa01a00c3",
            "6fc01a0e492e43258cbc58b29f959d3d",
            "62edc7175f22499e9389f89b2dcc27d4",
            "9c70873decd848f88350491efd36cd28",
            "90bb02580b6a46869e5a1276b7795173",
            "1ec07080e2114c179b92cad4263e6639",
            "2d46e5970029462c87c99723add72af5",
            "567531b1f5a04b1ab49c8f876d98fc38"
          ]
        },
        "id": "GgHHC5lGcOwH",
        "outputId": "8e35873f-9437-40bc-c37d-4351fb59b65d"
      },
      "source": [
        "'''\n",
        "!git clone https://github.com/NVlabs/DeepInversion.git\n",
        "\n",
        "!pip install tensorboardX\n",
        "! cp -r /content/DeepInversion/cifar10/deepinversion_cifar10.py /content\n",
        "! cp -r /content/DeepInversion/cifar10/resnet_cifar.py /content\n",
        "'''\n",
        "#from resnet_cifar import ResNet18 # HO IMPORTATO LA CLASSE DEL PAPER E PROVATO CON LA LORO RESNET 18 (non cambia nulla)\n",
        "labels_of_modified = torch.zeros(0, dtype = int).to('cuda')\n",
        "for label in batch:\n",
        "  labels = torch.LongTensor([diz[label]]*20).to('cuda')\n",
        "  labels_of_modified = torch.cat((labels_of_modified, labels), dim=0)\n",
        "print('len to be created', len(labels_of_modified))\n",
        "number_of_images_created = 200\n",
        "\n",
        "teacher = copy.deepcopy(model)\n",
        "net_teacher = resnet32(num_classes=10).to('cuda')\n",
        "net_teacher.load_state_dict(teacher.state_dict())\n",
        "#net_student = resnet32(num_classes=10).to('cuda')\n",
        "net_student = ResNet18().to('cuda')\n",
        "net_teacher.eval()\n",
        "\n",
        "inputs = torch.randn((number_of_images_created, 3, 32, 32), requires_grad=True, device='cuda', dtype=torch.float)\n",
        "train_writer = None  # tensorboard writter\n",
        "global_iteration = 0\n",
        "di_lr = 0.05\n",
        "optimizer_di = optim.Adam([inputs], lr=di_lr)\n",
        "plt.imshow(tensor2im(inputs[0]))\n",
        "plt.show()\n",
        "print(\"Starting model inversion\")\n",
        "batch_idx = 0\n",
        "inputs = get_images(net=net_teacher, bs=len(labels_of_modified), epochs=1000, idx=batch_idx, \n",
        "                  net_student=net_student, competitive_scale=10.0, l2_coeff = 3e-8, bn_reg_scale = 1.0,\n",
        "                  train_writer=train_writer, use_amp=False,\n",
        "                  optimizer=optimizer_di, inputs=inputs, \n",
        "                  var_scale=2.5e-5, labels=labels_of_modified) #2.5e-5\n",
        "\n",
        "plt.imshow(tensor2im(inputs[0]))\n",
        "plt.show()\n",
        "print('deepinversion finshed')"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "len to be created 200\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Starting model inversion\n",
            "Teacher correct out of 200: 20, loss at 11.052681922912598\n",
            "Student correct out of 200: 20, loss at 2.3460044860839844\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2881a7f479d64056ac89148fa01a00c3",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=1000.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "It 0\t Losses: total: 235.118,\ttarget: 11.403 \tR_feature_loss unscaled:\t 217.853\n",
            "It 200\t Losses: total: 47.281,\ttarget: 0.111 \tR_feature_loss unscaled:\t 40.134\n",
            "It 400\t Losses: total: 30.351,\ttarget: 0.024 \tR_feature_loss unscaled:\t 24.441\n",
            "It 600\t Losses: total: 24.446,\ttarget: 0.020 \tR_feature_loss unscaled:\t 18.558\n",
            "It 800\t Losses: total: 26.505,\ttarget: 0.064 \tR_feature_loss unscaled:\t 19.785\n",
            "\n",
            "Teacher correct out of 200: 200, loss at 0.008894884958863258\n",
            "Student correct out of 200: 15, loss at 2.4105594158172607\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "deepinversion finshed\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}