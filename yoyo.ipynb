{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "yoyo.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOnHnzBx/5HuyJS4tXJb+mF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alessandronicolini/IncrementalLearning/blob/main/yoyo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4zH038AaaB6n",
        "outputId": "5146a3ab-cc12-48f2-bc24-bed50c26cf7d"
      },
      "source": [
        "#!git clone git@github.com:yaoyao-liu/class-incremental-learning/mnemonics-training.git\n",
        "!git clone https://github.com/yaoyao-liu/class-incremental-learning.git\n",
        "\n",
        "!pip install tensorboardX\n",
        "! cp -r /content/class-incremental-learning/mnemonics-training/1_train/models /content\n",
        "! cp -r /content/class-incremental-learning/mnemonics-training/1_train/trainer /content\n",
        "! cp -r /content/class-incremental-learning/mnemonics-training/1_train/utils /content\n",
        "! cp -r /content/class-incremental-learning/mnemonics-training/1_train/main.py /content"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'class-incremental-learning'...\n",
            "remote: Enumerating objects: 48, done.\u001b[K\n",
            "remote: Counting objects: 100% (48/48), done.\u001b[K\n",
            "remote: Compressing objects: 100% (35/35), done.\u001b[K\n",
            "remote: Total 90 (delta 17), reused 39 (delta 13), pack-reused 42\u001b[K\n",
            "Unpacking objects: 100% (90/90), done.\n",
            "Requirement already satisfied: tensorboardX in /usr/local/lib/python3.6/dist-packages (2.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.19.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.15.0)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (3.12.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorboardX) (51.1.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CXX0Tf035Shs"
      },
      "source": [
        "import copy\n",
        "import os\n",
        "import torch\n",
        "import torchvision\n",
        "import warnings\n",
        "import math\n",
        "import utils.misc\n",
        "import numpy as np\n",
        "import os.path as osp\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import models.modified_resnet_cifar as modified_resnet_cifar\n",
        "import models.modified_resnetmtl_cifar as modified_resnetmtl_cifar\n",
        "import models.modified_linear as modified_linear\n",
        "from PIL import Image\n",
        "from torch.optim import lr_scheduler\n",
        "from torchvision import datasets, transforms\n",
        "from tensorboardX import SummaryWriter\n",
        "from utils.compute_features import compute_features\n",
        "from utils.process_mnemonics import process_mnemonics\n",
        "from utils.compute_accuracy import compute_accuracy\n",
        "from trainer.incremental import incremental_train_and_eval\n",
        "from utils.misc import *\n",
        "from utils.process_fp import process_inputs_fp\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "class Trainer(object):\n",
        "    def __init__(self, the_args):\n",
        "        self.args = the_args\n",
        "        self.log_dir = './logs/'\n",
        "        if not osp.exists(self.log_dir):\n",
        "            os.mkdir(self.log_dir)\n",
        "        self.save_path = self.log_dir + self.args.dataset + '_nfg' + str(self.args.nb_cl_fg) + '_ncls' + str(self.args.nb_cl) + '_nproto' + str(self.args.nb_protos) \n",
        "        self.save_path += '_' + self.args.method        \n",
        "        if not osp.exists(self.save_path):\n",
        "            os.mkdir(self.save_path)\n",
        "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.transform_train = transforms.Compose([transforms.RandomCrop(32, padding=4), transforms.RandomHorizontalFlip(), transforms.ToTensor(), transforms.Normalize((0.5071,  0.4866,  0.4409), (0.2009,  0.1984,  0.2023))])\n",
        "        self.transform_test = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5071,  0.4866,  0.4409), (0.2009,  0.1984,  0.2023))])\n",
        "        self.trainset = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=self.transform_train)\n",
        "        self.testset = torchvision.datasets.CIFAR100(root='./data', train=False, download=True, transform=self.transform_test)\n",
        "        self.evalset = torchvision.datasets.CIFAR100(root='./data', train=False, download=False, transform=self.transform_test)\n",
        "        self.network = modified_resnet_cifar.resnet32\n",
        "        self.network_mtl = modified_resnetmtl_cifar.resnetmtl32\n",
        "        self.lr_strat_first_phase = [int(160*0.5), int(160*0.75)]\n",
        "        self.lr_strat = [int(self.args.epochs*0.5), int(self.args.epochs*0.75)]\n",
        "        self.dictionary_size = self.args.dictionary_size\n",
        "\n",
        "    def map_labels(self, order_list, Y_set):\n",
        "        map_Y = []\n",
        "        for idx in Y_set:\n",
        "            map_Y.append(order_list.index(idx))\n",
        "        map_Y = np.array(map_Y)\n",
        "        return map_Y\n",
        "\n",
        "    def train(self):\n",
        "        self.train_writer = SummaryWriter(logdir=self.save_path)\n",
        "        dictionary_size = self.dictionary_size\n",
        "        top1_acc_list_cumul = np.zeros((int(self.args.num_classes/self.args.nb_cl), 4, self.args.nb_runs))\n",
        "        top1_acc_list_ori = np.zeros((int(self.args.num_classes/self.args.nb_cl), 4, self.args.nb_runs))\n",
        "        X_train_total = np.array(self.trainset.data)\n",
        "        Y_train_total = np.array(self.trainset.targets)\n",
        "        X_valid_total = np.array(self.testset.data)\n",
        "        Y_valid_total = np.array(self.testset.targets)\n",
        "        np.random.seed(1993)\n",
        "        for iteration_total in range(self.args.nb_runs):\n",
        "            order_name = osp.join(self.save_path, \"seed_{}_{}_order_run_{}.pkl\".format(1993, self.args.dataset, iteration_total))\n",
        "            print(\"Order name:{}\".format(order_name))\n",
        "            if osp.exists(order_name):\n",
        "                print(\"Loading orders\")\n",
        "                order = utils.misc.unpickle(order_name)\n",
        "            else:\n",
        "                print(\"Generating orders\")\n",
        "                order = np.arange(self.args.num_classes)\n",
        "                np.random.shuffle(order)\n",
        "                utils.misc.savepickle(order, order_name)\n",
        "            order_list = list(order)\n",
        "            print(order_list)\n",
        "        np.random.seed(self.args.random_seed)\n",
        "        X_valid_cumuls    = []\n",
        "        X_protoset_cumuls = []\n",
        "        X_train_cumuls    = []\n",
        "        Y_valid_cumuls    = []\n",
        "        Y_protoset_cumuls = []\n",
        "        Y_train_cumuls    = []\n",
        "        alpha_dr_herding  = np.zeros((int(self.args.num_classes/self.args.nb_cl),dictionary_size,self.args.nb_cl),np.float32)\n",
        "        prototypes = np.zeros((self.args.num_classes,dictionary_size,X_train_total.shape[1],X_train_total.shape[2],X_train_total.shape[3]))\n",
        "        for orde in range(self.args.num_classes):\n",
        "            prototypes[orde,:,:,:,:] = X_train_total[np.where(Y_train_total==order[orde])]\n",
        "        start_iter = int(self.args.nb_cl_fg/self.args.nb_cl)-1\n",
        "\n",
        "        print('start iter: ', start_iter)\n",
        "        for iteration in range(start_iter, int(self.args.num_classes/self.args.nb_cl)):\n",
        "            if iteration == start_iter:\n",
        "                last_iter = 0\n",
        "                tg_model = self.network(num_classes=self.args.nb_cl_fg)\n",
        "                in_features = tg_model.fc.in_features\n",
        "                out_features = tg_model.fc.out_features\n",
        "                print(\"Out_features:\", out_features)\n",
        "                ref_model = None\n",
        "                free_model = None\n",
        "                ref_free_model = None\n",
        "            elif iteration == start_iter+1:\n",
        "                last_iter = iteration\n",
        "                ref_model = copy.deepcopy(tg_model)\n",
        "                print(\"Fusion Mode: \"+self.args.fusion_mode)\n",
        "                tg_model = self.network_mtl(num_classes=self.args.nb_cl_fg)\n",
        "                ref_dict = ref_model.state_dict()\n",
        "                tg_dict = tg_model.state_dict()\n",
        "                tg_dict.update(ref_dict)\n",
        "                tg_model.load_state_dict(tg_dict)\n",
        "                tg_model.to(self.device)\n",
        "                in_features = tg_model.fc.in_features\n",
        "                out_features = tg_model.fc.out_features\n",
        "                print(\"Out_features:\", out_features)\n",
        "                new_fc = modified_linear.SplitCosineLinear(in_features, out_features, self.args.nb_cl)\n",
        "                new_fc.fc1.weight.data = tg_model.fc.weight.data\n",
        "                new_fc.sigma.data = tg_model.fc.sigma.data\n",
        "                tg_model.fc = new_fc\n",
        "                lamda_mult = out_features*1.0 / self.args.nb_cl\n",
        "            else:\n",
        "                last_iter = iteration\n",
        "                ref_model = copy.deepcopy(tg_model)\n",
        "                in_features = tg_model.fc.in_features\n",
        "                out_features1 = tg_model.fc.fc1.out_features\n",
        "                out_features2 = tg_model.fc.fc2.out_features\n",
        "                print(\"Out_features:\", out_features1+out_features2)\n",
        "                new_fc = modified_linear.SplitCosineLinear(in_features, out_features1+out_features2, self.args.nb_cl)\n",
        "                new_fc.fc1.weight.data[:out_features1] = tg_model.fc.fc1.weight.data\n",
        "                new_fc.fc1.weight.data[out_features1:] = tg_model.fc.fc2.weight.data\n",
        "                new_fc.sigma.data = tg_model.fc.sigma.data\n",
        "                tg_model.fc = new_fc\n",
        "                lamda_mult = (out_features1+out_features2)*1.0 / (self.args.nb_cl)\n",
        "            if iteration > start_iter:\n",
        "                cur_lamda = self.args.lamda * math.sqrt(lamda_mult)\n",
        "            else:\n",
        "                cur_lamda = self.args.lamda\n",
        "            actual_cl = order[range(last_iter*self.args.nb_cl,(iteration+1)*self.args.nb_cl)]\n",
        "            indices_train_10 = np.array([i in order[range(last_iter*self.args.nb_cl,(iteration+1)*self.args.nb_cl)] for i in Y_train_total])\n",
        "            indices_test_10 = np.array([i in order[range(last_iter*self.args.nb_cl,(iteration+1)*self.args.nb_cl)] for i in Y_valid_total])\n",
        "            X_train = X_train_total[indices_train_10]\n",
        "            X_valid = X_valid_total[indices_test_10]\n",
        "            X_valid_cumuls.append(X_valid)\n",
        "            X_train_cumuls.append(X_train)\n",
        "            X_valid_cumul = np.concatenate(X_valid_cumuls)\n",
        "            X_train_cumul = np.concatenate(X_train_cumuls)\n",
        "            Y_train = Y_train_total[indices_train_10]\n",
        "            Y_valid = Y_valid_total[indices_test_10]\n",
        "            Y_valid_cumuls.append(Y_valid)\n",
        "            Y_train_cumuls.append(Y_train)\n",
        "            Y_valid_cumul = np.concatenate(Y_valid_cumuls)\n",
        "            Y_train_cumul = np.concatenate(Y_train_cumuls)\n",
        "            print(indices_train_10)\n",
        "            if iteration == start_iter:\n",
        "                X_valid_ori = X_valid\n",
        "                Y_valid_ori = Y_valid\n",
        "            else:\n",
        "                X_protoset = np.concatenate(X_protoset_cumuls)\n",
        "                Y_protoset = np.concatenate(Y_protoset_cumuls)\n",
        "                if self.args.rs_ratio > 0:\n",
        "                    scale_factor = (len(X_train) * self.args.rs_ratio) / (len(X_protoset) * (1 - self.args.rs_ratio))\n",
        "                    rs_sample_weights = np.concatenate((np.ones(len(X_train)), np.ones(len(X_protoset))*scale_factor))\n",
        "                    rs_num_samples = int(len(X_train) / (1 - self.args.rs_ratio))\n",
        "                    print(\"X_train:{}, X_protoset:{}, rs_num_samples:{}\".format(len(X_train), len(X_protoset), rs_num_samples))\n",
        "                X_train = np.concatenate((X_train,X_protoset),axis=0)\n",
        "                Y_train = np.concatenate((Y_train,Y_protoset))\n",
        "            print('Batch of classes number {0} arrives'.format(iteration+1))\n",
        "            map_Y_train = np.array([order_list.index(i) for i in Y_train])\n",
        "            map_Y_valid_cumul = np.array([order_list.index(i) for i in Y_valid_cumul])\n",
        "            is_start_iteration = (iteration == start_iter)\n",
        "            if iteration > start_iter:\n",
        "                old_embedding_norm = tg_model.fc.fc1.weight.data.norm(dim=1, keepdim=True)\n",
        "                average_old_embedding_norm = torch.mean(old_embedding_norm, dim=0).to('cpu').type(torch.DoubleTensor)\n",
        "                tg_feature_model = nn.Sequential(*list(tg_model.children())[:-1])\n",
        "                num_features = tg_model.fc.in_features\n",
        "                novel_embedding = torch.zeros((self.args.nb_cl, num_features))\n",
        "                for cls_idx in range(iteration*self.args.nb_cl, (iteration+1)*self.args.nb_cl):\n",
        "                    cls_indices = np.array([i == cls_idx  for i in map_Y_train])\n",
        "                    assert(len(np.where(cls_indices==1)[0])==dictionary_size)\n",
        "                    self.evalset.data = X_train[cls_indices].astype('uint8')\n",
        "                    self.evalset.targets = np.zeros(self.evalset.data.shape[0])\n",
        "                    evalloader = torch.utils.data.DataLoader(self.evalset, batch_size=self.args.eval_batch_size, shuffle=False, num_workers=self.args.num_workers)\n",
        "                    num_samples = self.evalset.data.shape[0]\n",
        "                    cls_features = compute_features(tg_model, free_model, tg_feature_model, is_start_iteration, evalloader, num_samples, num_features)\n",
        "                    norm_features = F.normalize(torch.from_numpy(cls_features), p=2, dim=1)\n",
        "                    cls_embedding = torch.mean(norm_features, dim=0)\n",
        "                    novel_embedding[cls_idx-iteration*self.args.nb_cl] = F.normalize(cls_embedding, p=2, dim=0) * average_old_embedding_norm\n",
        "                tg_model.to(self.device)\n",
        "                tg_model.fc.fc2.weight.data = novel_embedding.to(self.device)\n",
        "            self.trainset.data = X_train.astype('uint8')\n",
        "            self.trainset.targets = map_Y_train\n",
        "            if iteration > start_iter and self.args.rs_ratio > 0 and scale_factor > 1:\n",
        "                print(\"Weights from sampling:\", rs_sample_weights)\n",
        "                index1 = np.where(rs_sample_weights>1)[0]\n",
        "                index2 = np.where(map_Y_train<iteration*self.args.nb_cl)[0]\n",
        "                assert((index1==index2).all())\n",
        "                train_sampler = torch.utils.data.sampler.WeightedRandomSampler(rs_sample_weights, rs_num_samples)\n",
        "                trainloader = torch.utils.data.DataLoader(self.trainset, batch_size=self.args.train_batch_size, shuffle=False, sampler=train_sampler, num_workers=self.args.num_workers)            \n",
        "            else:\n",
        "\n",
        "                trainloader = torch.utils.data.DataLoader(self.trainset, batch_size=self.args.train_batch_size,\n",
        "                    shuffle=True, num_workers=self.args.num_workers)\n",
        "            self.testset.data = X_valid_cumul.astype('uint8')\n",
        "            self.testset.targets = map_Y_valid_cumul\n",
        "            testloader = torch.utils.data.DataLoader(self.testset, batch_size=self.args.test_batch_size,\n",
        "                shuffle=False, num_workers=self.args.num_workers)\n",
        "            print('Max and min of train labels: {}, {}'.format(min(map_Y_train), max(map_Y_train)))\n",
        "            print('Max and min of valid labels: {}, {}'.format(min(map_Y_valid_cumul), max(map_Y_valid_cumul)))\n",
        "            ckp_name = osp.join(self.save_path, 'run_{}_iteration_{}_model.pth'.format(iteration_total, iteration))\n",
        "            ckp_name_free = osp.join(self.save_path, 'run_{}_iteration_{}_free_model.pth'.format(iteration_total, iteration))\n",
        "            print('Checkpoint name:', ckp_name)\n",
        "            if iteration==start_iter and self.args.resume_fg:\n",
        "                print(\"Loading first group models from checkpoint\")\n",
        "                tg_model = torch.load(self.args.ckpt_dir_fg)\n",
        "            elif self.args.resume and os.path.exists(ckp_name):\n",
        "                print(\"Loading models from checkpoint\")\n",
        "                tg_model = torch.load(ckp_name)\n",
        "            else:\n",
        "                if iteration > start_iter:\n",
        "                    ref_model = ref_model.to(self.device)\n",
        "                    ignored_params = list(map(id, tg_model.fc.fc1.parameters()))\n",
        "                    base_params = filter(lambda p: id(p) not in ignored_params, tg_model.parameters())\n",
        "                    base_params = filter(lambda p: p.requires_grad,base_params)\n",
        "                    base_params = filter(lambda p: p.requires_grad,base_params)\n",
        "                    tg_params_new =[{'params': base_params, 'lr': self.args.base_lr2, 'weight_decay': self.args.custom_weight_decay}, {'params': tg_model.fc.fc1.parameters(), 'lr': 0, 'weight_decay': 0}]\n",
        "                    tg_model = tg_model.to(self.device)\n",
        "                    tg_optimizer = optim.SGD(tg_params_new, lr=self.args.base_lr2, momentum=self.args.custom_momentum, weight_decay=self.args.custom_weight_decay)\n",
        "                else:\n",
        "                    tg_params = tg_model.parameters()\n",
        "                    tg_model = tg_model.to(self.device)\n",
        "                    tg_optimizer = optim.SGD(tg_params, lr=self.args.base_lr1, momentum=self.args.custom_momentum, weight_decay=self.args.custom_weight_decay)\n",
        "                if iteration > start_iter:\n",
        "                    tg_lr_scheduler = lr_scheduler.MultiStepLR(tg_optimizer, milestones=self.lr_strat, gamma=self.args.lr_factor)\n",
        "                else:\n",
        "                    tg_lr_scheduler = lr_scheduler.MultiStepLR(tg_optimizer, milestones=self.lr_strat_first_phase, gamma=self.args.lr_factor)           \n",
        "                print(\"Incremental train\")\n",
        "                if iteration > start_iter:\n",
        "                    tg_model = incremental_train_and_eval(self.args.epochs, tg_model, ref_model, free_model, ref_free_model, tg_optimizer, tg_lr_scheduler, trainloader, testloader, iteration, start_iter, cur_lamda, self.args.dist, self.args.K, self.args.lw_mr)   \n",
        "                else:                    \n",
        "                    tg_model = incremental_train_and_eval(self.args.epochs, tg_model, ref_model, free_model, ref_free_model, tg_optimizer, tg_lr_scheduler, trainloader, testloader, iteration, start_iter, cur_lamda, self.args.dist, self.args.K, self.args.lw_mr)            \n",
        "                torch.save(tg_model, ckp_name)\n",
        "            if self.args.dynamic_budget:\n",
        "                nb_protos_cl = self.args.nb_protos\n",
        "            else:\n",
        "                nb_protos_cl = int(np.ceil(self.args.nb_protos*100./self.args.nb_cl/(iteration+1)))\n",
        "            tg_feature_model = nn.Sequential(*list(tg_model.children())[:-1])\n",
        "            num_features = tg_model.fc.in_features\n",
        "            countt = 0\n",
        "            for iter_dico in range(last_iter*self.args.nb_cl, (iteration+1)*self.args.nb_cl):\n",
        "                countt+=1\n",
        "                self.evalset.data = prototypes[iter_dico].astype('uint8')\n",
        "                self.evalset.targets = np.zeros(self.evalset.data.shape[0])\n",
        "                evalloader = torch.utils.data.DataLoader(self.evalset, batch_size=self.args.eval_batch_size,\n",
        "                    shuffle=False, num_workers=self.args.num_workers)\n",
        "                num_samples = self.evalset.data.shape[0]            \n",
        "                mapped_prototypes = compute_features(tg_model, free_model, tg_feature_model, is_start_iteration, evalloader, num_samples, num_features)\n",
        "                D = mapped_prototypes.T\n",
        "                D = D/np.linalg.norm(D,axis=0)\n",
        "                mu  = np.mean(D,axis=1)\n",
        "                index1 = int(iter_dico/self.args.nb_cl)\n",
        "                index2 = iter_dico % self.args.nb_cl\n",
        "                alpha_dr_herding[index1,:,index2] = alpha_dr_herding[index1,:,index2]*0\n",
        "                w_t = mu\n",
        "                iter_herding     = 0\n",
        "                iter_herding_eff = 0\n",
        "                while not(np.sum(alpha_dr_herding[index1,:,index2]!=0)==min(nb_protos_cl,500)) and iter_herding_eff<1000:\n",
        "                    tmp_t   = np.dot(w_t,D)\n",
        "                    ind_max = np.argmax(tmp_t)\n",
        "\n",
        "                    iter_herding_eff += 1\n",
        "                    if alpha_dr_herding[index1,ind_max,index2] == 0:\n",
        "                        alpha_dr_herding[index1,ind_max,index2] = 1+iter_herding\n",
        "                        iter_herding += 1\n",
        "                    w_t = w_t+mu-D[:,ind_max]\n",
        "            print('da xapire: ', countt)\n",
        "            X_protoset_cumuls = []\n",
        "            Y_protoset_cumuls = []\n",
        "            class_means = np.zeros((64,100,2))\n",
        "            for iteration2 in range(iteration+1):\n",
        "                for iter_dico in range(self.args.nb_cl):\n",
        "                    current_cl = order[range(iteration2*self.args.nb_cl,(iteration2+1)*self.args.nb_cl)]\n",
        "                    self.evalset.data = prototypes[iteration2*self.args.nb_cl+iter_dico].astype('uint8')\n",
        "                    self.evalset.targets = np.zeros(self.evalset.data.shape[0]) #zero labels\n",
        "                    evalloader = torch.utils.data.DataLoader(self.evalset, batch_size=self.args.eval_batch_size,\n",
        "                        shuffle=False, num_workers=self.args.num_workers)\n",
        "                    num_samples = self.evalset.data.shape[0]\n",
        "                    mapped_prototypes = compute_features(tg_model, free_model, tg_feature_model, is_start_iteration, evalloader, num_samples, num_features)\n",
        "                    D = mapped_prototypes.T\n",
        "                    D = D/np.linalg.norm(D,axis=0)\n",
        "                    self.evalset.data = prototypes[iteration2*self.args.nb_cl+iter_dico][:,:,:,::-1].astype('uint8')\n",
        "                    evalloader = torch.utils.data.DataLoader(self.evalset, batch_size=self.args.eval_batch_size,\n",
        "                        shuffle=False, num_workers=self.args.num_workers)\n",
        "                    mapped_prototypes2 = compute_features(tg_model, free_model, tg_feature_model, is_start_iteration, evalloader, num_samples, num_features)\n",
        "                    D2 = mapped_prototypes2.T\n",
        "                    D2 = D2/np.linalg.norm(D2,axis=0)\n",
        "                    alph = alpha_dr_herding[iteration2,:,iter_dico]\n",
        "                    alph = (alph>0)*(alph<nb_protos_cl+1)*1.\n",
        "                    X_protoset_cumuls.append(prototypes[iteration2*self.args.nb_cl+iter_dico,np.where(alph==1)[0]])\n",
        "                    Y_protoset_cumuls.append(order[iteration2*self.args.nb_cl+iter_dico]*np.ones(len(np.where(alph==1)[0])))\n",
        "                    alph = alph/np.sum(alph)\n",
        "                    class_means[:,current_cl[iter_dico],0] = (np.dot(D,alph)+np.dot(D2,alph))/2\n",
        "                    class_means[:,current_cl[iter_dico],0] /= np.linalg.norm(class_means[:,current_cl[iter_dico],0])\n",
        "                    alph = np.ones(dictionary_size)/dictionary_size\n",
        "                    class_means[:,current_cl[iter_dico],1] = (np.dot(D,alph)+np.dot(D2,alph))/2\n",
        "                    class_means[:,current_cl[iter_dico],1] /= np.linalg.norm(class_means[:,current_cl[iter_dico],1])\n",
        "            current_means = class_means[:, order[range(0,(iteration+1)*self.args.nb_cl)]]\n",
        "            X_protoset_array_old = np.array(X_protoset_cumuls)\n",
        "            self.T = self.args.mnemonics_steps * self.args.mnemonics_epochs\n",
        "            self.img_size = 32\n",
        "            self.mnemonics_lrs = self.args.mnemonics_lr\n",
        "            num_classes_incremental = self.args.nb_cl\n",
        "            num_classes = self.args.nb_cl_fg\n",
        "            nb_cl = self.args.nb_cl\n",
        "            transform_proto = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5071,  0.4866,  0.4409), (0.2009,  0.1984,  0.2023)),])\n",
        "            self.mnemonics_label = []\n",
        "            if iteration == start_iter:\n",
        "                the_X_protoset_array = np.array(X_protoset_cumuls).astype('uint8')\n",
        "                the_Y_protoset_cumuls = np.array(Y_protoset_cumuls)\n",
        "            else:\n",
        "                the_X_protoset_array = np.array(X_protoset_cumuls[-num_classes_incremental:]).astype('uint8')\n",
        "                the_Y_protoset_cumuls = np.array(Y_protoset_cumuls[-num_classes_incremental:])\n",
        "            self.mnemonics_data = torch.zeros(the_X_protoset_array.shape[0], the_X_protoset_array.shape[1], 3, self.img_size, self.img_size)\n",
        "            for idx1 in range(the_X_protoset_array.shape[0]):               \n",
        "                for idx2 in range(the_X_protoset_array.shape[1]):\n",
        "                    the_img = the_X_protoset_array[idx1][idx2]\n",
        "                    the_PIL_image = Image.fromarray(the_img)\n",
        "                    the_PIL_image = transform_proto(the_PIL_image)\n",
        "                    self.mnemonics_data[idx1][idx2]=the_PIL_image\n",
        "                map_Y_label = self.map_labels(order_list, the_Y_protoset_cumuls[idx1])\n",
        "                self.mnemonics_label.append(map_Y_label)\n",
        "            \n",
        "            self.mnemonics = nn.ParameterList()\n",
        "            self.mnemonics.append(nn.Parameter(self.mnemonics_data))\n",
        "            start_iteration = start_iter\n",
        "            device = self.device\n",
        "            self.mnemonics.to(device)\n",
        "            tg_feature_model = nn.Sequential(*list(tg_model.children())[:-1])\n",
        "            tg_feature_model.eval()\n",
        "            tg_model.eval()\n",
        "            if free_model is not None:\n",
        "                free_model.eval()\n",
        "            self.mnemonics_optimizer = optim.SGD(self.mnemonics, lr=self.args.mnemonics_outer_lr, momentum=0.9, weight_decay=5e-4)\n",
        "            self.mnemonics_lr_scheduler = optim.lr_scheduler.StepLR(self.mnemonics_optimizer, step_size=self.args.mnemonics_decay_epochs, gamma=self.args.mnemonics_decay_factor)\n",
        "            current_means_new = current_means[:,:,0].T\n",
        "            print('start mnemonics')\n",
        "            for epoch in range(self.args.mnemonics_total_epochs):\n",
        "                train_loss = 0\n",
        "                self.mnemonics_lr_scheduler.step()\n",
        "                for batch_idx, (q_inputs, q_targets) in enumerate(trainloader):\n",
        "                    q_inputs, q_targets = q_inputs.to(device), q_targets.to(device)\n",
        "                    if iteration == start_iteration:\n",
        "                        q_feature = tg_feature_model(q_inputs)\n",
        "                    else:\n",
        "                        q_feature = process_inputs_fp(tg_model, free_model, q_inputs, feature_mode=True)\n",
        "                    self.mnemonics_optimizer.zero_grad()\n",
        "                    total_tr_loss = 0 \n",
        "                    if iteration == start_iteration:\n",
        "                        mnemonics_outputs = tg_feature_model(self.mnemonics[0][0])\n",
        "                    else:\n",
        "                        mnemonics_outputs = process_inputs_fp(tg_model, free_model, self.mnemonics[0][0], feature_mode=True)\n",
        "                    this_class_mean_mnemonics = torch.mean(mnemonics_outputs, dim=0)\n",
        "                    this_class_mean_mnemonics = torch.squeeze(this_class_mean_mnemonics)\n",
        "                    total_class_mean_mnemonics = this_class_mean_mnemonics.unsqueeze(dim=0)\n",
        "                    for mnemonics_idx in range(len(self.mnemonics[0])-1):\n",
        "                        if iteration == start_iteration:\n",
        "                            mnemonics_outputs = tg_feature_model(self.mnemonics[0][mnemonics_idx+1])\n",
        "                        else:\n",
        "                            mnemonics_outputs = process_inputs_fp(tg_model, free_model, self.mnemonics[0][mnemonics_idx+1], feature_mode=True)\n",
        "                        this_class_mean_mnemonics = torch.mean(mnemonics_outputs, dim=0)\n",
        "                        this_class_mean_mnemonics = torch.squeeze(this_class_mean_mnemonics)\n",
        "                        total_class_mean_mnemonics =  torch.cat((total_class_mean_mnemonics, this_class_mean_mnemonics.unsqueeze(dim=0)), dim=0)\n",
        "                    if iteration == start_iteration:\n",
        "                        all_cls_means = total_class_mean_mnemonics\n",
        "                    else:\n",
        "                        all_cls_means = torch.tensor(current_means_new).float().to(device)\n",
        "                        all_cls_means[-nb_cl:] = total_class_mean_mnemonics\n",
        "                    the_logits = F.linear(F.normalize(torch.squeeze(q_feature), p=2,dim=1), F.normalize(all_cls_means, p=2, dim=1))\n",
        "                    loss = F.cross_entropy(the_logits, q_targets)\n",
        "                    loss.backward()\n",
        "                    self.mnemonics_optimizer.step()\n",
        "                    train_loss += loss.item()\n",
        "            print('end mnemonics training')\n",
        "            X_protoset_cumuls = process_mnemonics(X_protoset_cumuls, Y_protoset_cumuls, self.mnemonics, self.mnemonics_label, order_list, self.args.nb_cl_fg, self.args.nb_cl, iteration, start_iter)        \n",
        "            X_protoset_array = np.array(X_protoset_cumuls)\n",
        "            X_protoset_cumuls_idx = 0\n",
        "            for iteration2 in range(iteration+1):\n",
        "                for iter_dico in range(self.args.nb_cl):\n",
        "                    alph = alpha_dr_herding[iteration2,:,iter_dico]\n",
        "                    alph = (alph>0)*(alph<nb_protos_cl+1)*1.\n",
        "                    this_X_protoset_array = X_protoset_array[X_protoset_cumuls_idx]\n",
        "                    X_protoset_cumuls_idx += 1\n",
        "                    this_X_protoset_array = this_X_protoset_array.astype(np.float64)\n",
        "                    prototypes[iteration2*self.args.nb_cl+iter_dico,np.where(alph==1)[0]] = this_X_protoset_array\n",
        "            class_means = np.zeros((64,100,2))\n",
        "            for iteration2 in range(iteration+1):\n",
        "                for iter_dico in range(self.args.nb_cl):\n",
        "                    current_cl = order[range(iteration2*self.args.nb_cl,(iteration2+1)*self.args.nb_cl)]\n",
        "                    self.evalset.data = prototypes[iteration2*self.args.nb_cl+iter_dico].astype('uint8')\n",
        "                    self.evalset.targets = np.zeros(self.evalset.data.shape[0]) #zero labels\n",
        "                    evalloader = torch.utils.data.DataLoader(self.evalset, batch_size=self.args.eval_batch_size,\n",
        "                        shuffle=False, num_workers=self.args.num_workers)\n",
        "                    num_samples = self.evalset.data.shape[0]\n",
        "                    mapped_prototypes = compute_features(tg_model, free_model, tg_feature_model, is_start_iteration, evalloader, num_samples, num_features)\n",
        "                    D = mapped_prototypes.T\n",
        "                    D = D/np.linalg.norm(D,axis=0)\n",
        "                    self.evalset.data = prototypes[iteration2*self.args.nb_cl+iter_dico][:,:,:,::-1].astype('uint8')\n",
        "                    evalloader = torch.utils.data.DataLoader(self.evalset, batch_size=self.args.eval_batch_size,\n",
        "                        shuffle=False, num_workers=self.args.num_workers)\n",
        "                    mapped_prototypes2 = compute_features(tg_model, free_model, tg_feature_model, is_start_iteration, evalloader, num_samples, num_features)\n",
        "                    D2 = mapped_prototypes2.T\n",
        "                    D2 = D2/np.linalg.norm(D2,axis=0)\n",
        "                    alph = alpha_dr_herding[iteration2,:,iter_dico]\n",
        "                    alph = (alph>0)*(alph<nb_protos_cl+1)*1.\n",
        "                    alph = alph/np.sum(alph)\n",
        "                    class_means[:,current_cl[iter_dico],0] = (np.dot(D,alph)+np.dot(D2,alph))/2\n",
        "                    class_means[:,current_cl[iter_dico],0] /= np.linalg.norm(class_means[:,current_cl[iter_dico],0])\n",
        "                    alph = np.ones(dictionary_size)/dictionary_size\n",
        "                    class_means[:,current_cl[iter_dico],1] = (np.dot(D,alph)+np.dot(D2,alph))/2\n",
        "                    class_means[:,current_cl[iter_dico],1] /= np.linalg.norm(class_means[:,current_cl[iter_dico],1])\n",
        "            torch.save(class_means, osp.join(self.save_path, 'run_{}_iteration_{}_class_means.pth'.format(iteration_total, iteration)))\n",
        "            current_means = class_means[:, order[range(0,(iteration+1)*self.args.nb_cl)]]\n",
        "            is_start_iteration = (iteration == start_iter)\n",
        "            map_Y_valid_ori = np.array([order_list.index(i) for i in Y_valid_ori])\n",
        "            print('Computing accuracy for first-phase classes')\n",
        "            self.evalset.data = X_valid_ori.astype('uint8')\n",
        "            self.evalset.targets = map_Y_valid_ori\n",
        "            #evalloader = torch.utils.data.DataLoader(self.evalset, batch_size=self.args.eval_batch_size, shuffle=False, num_workers=self.args.num_workers)\n",
        "            #ori_acc, fast_fc = compute_accuracy(tg_model, free_model,  tg_feature_model, current_means, X_protoset_cumuls, Y_protoset_cumuls, evalloader, order_list, is_start_iteration=is_start_iteration, maml_lr=self.args.maml_lr, maml_epoch=self.args.maml_epoch)\n",
        "            #top1_acc_list_ori[iteration, :, iteration_total] = np.array(ori_acc).T\n",
        "            #self.train_writer.add_scalar('ori_acc/LwF', float(ori_acc[0]), iteration)\n",
        "            #self.train_writer.add_scalar('ori_acc/iCaRL', float(ori_acc[1]), iteration)\n",
        "            #map_Y_valid_cumul = np.array([order_list.index(i) for i in Y_valid_cumul])\n",
        "            #print('Computing accuracy for all seen classes')\n",
        "            #self.evalset.data = X_valid_cumul.astype('uint8')\n",
        "            #self.evalset.targets = map_Y_valid_cumul\n",
        "            #evalloader = torch.utils.data.DataLoader(self.evalset, batch_size=self.args.eval_batch_size, shuffle=False, num_workers=self.args.num_workers)        \n",
        "            #cumul_acc, _ = compute_accuracy(tg_model, free_model, tg_feature_model, current_means, X_protoset_cumuls, Y_protoset_cumuls, evalloader, order_list, is_start_iteration=is_start_iteration, fast_fc=fast_fc, maml_lr=self.args.maml_lr, maml_epoch=self.args.maml_epoch)\n",
        "            #top1_acc_list_cumul[iteration, :, iteration_total] = np.array(cumul_acc).T\n",
        "            #self.train_writer.add_scalar('cumul_acc/LwF', float(cumul_acc[0]), iteration)\n",
        "            #self.train_writer.add_scalar('cumul_acc/iCaRL', float(cumul_acc[1]), iteration)\n",
        "        #torch.save(top1_acc_list_ori, osp.join(self.save_path, 'run_{}_top1_acc_list_ori.pth'.format(iteration_total)))\n",
        "        #torch.save(top1_acc_list_cumul, osp.join(self.save_path, 'run_{}_top1_acc_list_cumul.pth'.format(iteration_total)))\n",
        "        #self.train_writer.close\n",
        "        \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8x7_zxq0aFRv",
        "outputId": "5b65b87a-3bd0-4829-f1cb-710cc464a802"
      },
      "source": [
        "!python main.py --method=mnemonics --nb_cl=5\n",
        "#!python main.py --method=mnemonics --nb_cl=5\n",
        "#!python main.py --method=mnemonics --nb_cl=2\n",
        "#NEL MAIN SOSTITUIRE TUTTI I train_data CON data, train_labels CON targets. IDEM PER IL TEST"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Namespace(K=2, T=2, adapt_lamda=False, base_lr1=0.1, base_lr2=0.1, beta=0.25, ckpt_dir_fg='-', ckpt_label='01', custom_momentum=0.9, custom_weight_decay=0.0005, data_dir='data/seed_1993_subset_100_imagenet/data', dataset='cifar100', dictionary_size=500, dist=0.5, dynamic_budget=False, epochs=1, eval_batch_size=128, fusion_mode='free', gpu='0', lamda=5, less_forget=False, load_ckpt_prefix='-', load_iter=0, load_order='-', lr_factor=0.1, lw_mr=1, lw_ms=1, maml_epoch=50, maml_lr=0.1, method='mnemonics', mimic_score=False, mnemonics_decay_epochs=1, mnemonics_decay_factor=0.5, mnemonics_epochs=1, mnemonics_images_per_class_per_step=1, mnemonics_lr=1e-05, mnemonics_outer_lr=1e-05, mnemonics_steps=20, mnemonics_total_epochs=1, nb_cl=5, nb_cl_fg=50, nb_protos=20, nb_runs=1, num_classes=100, num_workers=2, phase='train', random_seed=1993, resume=False, resume_fg=False, rs_ratio=0, test_batch_size=100, train_batch_size=128)\n",
            "Using gpu: 0\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Order name:./logs/cifar100_nfg50_ncls5_nproto20_mnemonics/seed_1993_cifar100_order_run_0.pkl\n",
            "Loading orders\n",
            "[68, 56, 78, 8, 23, 84, 90, 65, 74, 76, 40, 89, 3, 92, 55, 9, 26, 80, 43, 38, 58, 70, 77, 1, 85, 19, 17, 50, 28, 53, 13, 81, 45, 82, 6, 59, 83, 16, 15, 44, 91, 41, 72, 60, 79, 52, 20, 10, 31, 54, 37, 95, 14, 71, 96, 98, 97, 2, 64, 66, 42, 22, 35, 86, 24, 34, 87, 21, 99, 0, 88, 27, 18, 94, 11, 12, 47, 25, 30, 46, 62, 69, 36, 61, 7, 63, 75, 5, 32, 4, 51, 48, 73, 93, 39, 67, 29, 49, 57, 33]\n",
            "tcmalloc: large alloc 1228800000 bytes == 0x2083c000 @  0x7f45dbccd001 0x7f45d980b4ff 0x7f45d985bab8 0x7f45d985fbb7 0x7f45d98fe003 0x50a4a5 0x50beb4 0x507be4 0x509900 0x50a2fd 0x50beb4 0x507be4 0x50ad03 0x634e72 0x634f27 0x6386df 0x639281 0x4b0dc0 0x7f45db8c8bf7 0x5b259a\n",
            "la shape:  (100, 500, 32, 32, 3)\n",
            "start iter:  9\n",
            "Out_features: 50\n",
            "[ True False False ...  True False False]\n",
            "Batch of classes number 10 arrives\n",
            "Max and min of train labels: 0, 49\n",
            "Max and min of valid labels: 0, 49\n",
            "Checkpoint name: ./logs/cifar100_nfg50_ncls5_nproto20_mnemonics/run_0_iteration_9_model.pth\n",
            "Incremental train\n",
            "\n",
            "Epoch: 0, LR: [0.1]\n",
            "Train set: 196, Train Loss: 3.4753 Acc: 9.4440\n",
            "Test set: 50 Test Loss: 3.2355 Acc: 15.2400\n",
            "the X (50, 40, 32, 32, 3)\n",
            "the Y (50, 40)\n",
            "\n",
            "current:  (50, 64)\n",
            "start mnemonics\n",
            "mnemonics shape torch.Size([50, 40, 3, 32, 32])\n",
            "end mnemonics training\n",
            "Computing accuracy for first-phase classes\n",
            "Fusion Mode: free\n",
            "Out_features: 50\n",
            "[False False False ... False False False]\n",
            "Batch of classes number 11 arrives\n",
            "Max and min of train labels: 0, 54\n",
            "Max and min of valid labels: 0, 54\n",
            "Checkpoint name: ./logs/cifar100_nfg50_ncls5_nproto20_mnemonics/run_0_iteration_10_model.pth\n",
            "Incremental train\n",
            "\n",
            "Epoch: 0, LR: [0.0010000000000000002, 0.0]\n",
            "Train set: 36, Train Loss1: 0.0239, Train Loss2: 2.8446,                Train Loss: 2.8685 Acc: 27.8222\n",
            "Test set: 55 Test Loss: 3.4406 Acc: 11.4727\n",
            "the X (5, 37, 32, 32, 3)\n",
            "the Y (5, 37)\n",
            "\n",
            "current:  (55, 64)\n",
            "start mnemonics\n",
            "mnemonics shape torch.Size([5, 37, 3, 32, 32])\n",
            "end mnemonics training\n",
            "Computing accuracy for first-phase classes\n",
            "Out_features: 55\n",
            "[False False False ... False False False]\n",
            "Batch of classes number 12 arrives\n",
            "Max and min of train labels: 0, 59\n",
            "Max and min of valid labels: 0, 59\n",
            "Checkpoint name: ./logs/cifar100_nfg50_ncls5_nproto20_mnemonics/run_0_iteration_11_model.pth\n",
            "Incremental train\n",
            "\n",
            "Epoch: 0, LR: [0.0010000000000000002, 0.0]\n",
            "Train set: 36, Train Loss1: 0.0441, Train Loss2: 3.0957,                Train Loss: 3.1398 Acc: 15.3032\n",
            "Test set: 60 Test Loss: 3.5937 Acc: 11.9500\n",
            "the X (5, 34, 32, 32, 3)\n",
            "the Y (5, 34)\n",
            "\n",
            "current:  (60, 64)\n",
            "start mnemonics\n",
            "mnemonics shape torch.Size([5, 34, 3, 32, 32])\n",
            "end mnemonics training\n",
            "Computing accuracy for first-phase classes\n",
            "Out_features: 60\n",
            "[False False False ... False False False]\n",
            "Batch of classes number 13 arrives\n",
            "Max and min of train labels: 0, 64\n",
            "Max and min of valid labels: 0, 64\n",
            "Checkpoint name: ./logs/cifar100_nfg50_ncls5_nproto20_mnemonics/run_0_iteration_12_model.pth\n",
            "Incremental train\n",
            "\n",
            "Epoch: 0, LR: [0.0010000000000000002, 0.0]\n",
            "Train set: 36, Train Loss1: 0.0143, Train Loss2: 3.3128,                Train Loss: 3.3271 Acc: 15.0881\n",
            "Test set: 65 Test Loss: 3.7103 Acc: 11.0462\n",
            "the X (5, 31, 32, 32, 3)\n",
            "the Y (5, 31)\n",
            "\n",
            "current:  (65, 64)\n",
            "start mnemonics\n",
            "mnemonics shape torch.Size([5, 31, 3, 32, 32])\n",
            "end mnemonics training\n",
            "Computing accuracy for first-phase classes\n",
            "Out_features: 65\n",
            "[False False  True ... False False False]\n",
            "Batch of classes number 14 arrives\n",
            "Max and min of train labels: 0, 69\n",
            "Max and min of valid labels: 0, 69\n",
            "Checkpoint name: ./logs/cifar100_nfg50_ncls5_nproto20_mnemonics/run_0_iteration_13_model.pth\n",
            "Incremental train\n",
            "\n",
            "Epoch: 0, LR: [0.0010000000000000002, 0.0]\n",
            "Traceback (most recent call last):\n",
            "  File \"main.py\", line 78, in <module>\n",
            "    trainer.train()\n",
            "  File \"/content/trainer/mnemonics.py\", line 240, in train\n",
            "    tg_model = incremental_train_and_eval(self.args.epochs, tg_model, ref_model, free_model, ref_free_model, tg_optimizer, tg_lr_scheduler, trainloader, testloader, iteration, start_iter, cur_lamda, self.args.dist, self.args.K, self.args.lw_mr)   \n",
            "  File \"/content/trainer/incremental.py\", line 45, in incremental_train_and_eval\n",
            "    tg_optimizer.step()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py\", line 67, in wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/autograd/grad_mode.py\", line 26, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/optim/sgd.py\", line 95, in step\n",
            "    if p.grad is None:\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/tensor.py\", line 943, in grad\n",
            "    if type(self) is not Tensor and has_torch_function(relevant_args):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/overrides.py\", line 1087, in has_torch_function\n",
            "    type(a) is not torch.Tensor and\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-hgEWFL2kZZG"
      },
      "source": [
        ""
      ],
      "execution_count": 12,
      "outputs": []
    }
  ]
}