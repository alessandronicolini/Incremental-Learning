{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mnemonics.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "42618ade859746d4b81c6893601def43": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_f09c4efb555d434bac45e7c59014f6ac",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_4f04fcb8344e45cb8eced90063ef2b3e",
              "IPY_MODEL_a0387061266a49898fa7cecfe4659898"
            ]
          }
        },
        "f09c4efb555d434bac45e7c59014f6ac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4f04fcb8344e45cb8eced90063ef2b3e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_69741978e1ef49b8852d7d95c62f83e0",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_36faba75789949f38a85d765a2b1c098"
          }
        },
        "a0387061266a49898fa7cecfe4659898": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_b0a728c741ec4a86ab91e4ae08113166",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1/1 [01:17&lt;00:00, 77.18s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4951f8f7304240c198301c2d2a1832b1"
          }
        },
        "69741978e1ef49b8852d7d95c62f83e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "36faba75789949f38a85d765a2b1c098": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b0a728c741ec4a86ab91e4ae08113166": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4951f8f7304240c198301c2d2a1832b1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alessandronicolini/IncrementalLearning/blob/main/mnemonics_prima_prova_funziona_GPU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WH7YHXeh0hFj",
        "outputId": "e2819942-c978-4ba8-b65b-e017e9cb2aed"
      },
      "source": [
        "!pip3 install 'import_ipynb'\n",
        "!pip3 install 'tqdm'\n",
        "\n",
        "!rm -r IncrementalLearning\n",
        "# upload work files from your git hub repository\n",
        "import sys\n",
        "\n",
        "!git clone https://github.com/alessandronicolini/IncrementalLearning.git # clone proj repository\n",
        "!rm -rf IncrementalLearning/README.md \n",
        "!rm -rf IncrementalLearning/baselines.ipynb\n",
        "\n",
        "path = 'IncrementalLearning/'\n",
        "if path not in sys.path:\n",
        "    sys.path.append('IncrementalLearning/')\n",
        "\n",
        "!pip3 install import_ipynb"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: import_ipynb in /usr/local/lib/python3.6/dist-packages (0.1.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.41.1)\n",
            "Cloning into 'IncrementalLearning'...\n",
            "remote: Enumerating objects: 47, done.\u001b[K\n",
            "remote: Counting objects: 100% (47/47), done.\u001b[K\n",
            "remote: Compressing objects: 100% (46/46), done.\u001b[K\n",
            "remote: Total 521 (delta 27), reused 0 (delta 0), pack-reused 474\u001b[K\n",
            "Receiving objects: 100% (521/521), 510.85 KiB | 5.21 MiB/s, done.\n",
            "Resolving deltas: 100% (302/302), done.\n",
            "Requirement already satisfied: import_ipynb in /usr/local/lib/python3.6/dist-packages (0.1.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5O4jUchQ1EAI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6da3792a-2b80-417b-ba45-44c70a8cb6a9"
      },
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import math\n",
        "from sklearn.preprocessing import normalize\n",
        "import copy\n",
        "import torchvision.datasets as dsets\n",
        "import torchvision.models as models\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.gridspec as gridspec\n",
        "from torch.utils.data import Subset, DataLoader, Dataset\n",
        "import random\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pickle\n",
        "import pandas as pd\n",
        "\n",
        "import import_ipynb\n",
        "from IncrementalLearning.cifar100 import ilCIFAR100\n",
        "from google.colab import output\n",
        "\n",
        "from IncrementalLearning.resnet_cifar import resnet32\n",
        "from tqdm.notebook import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "importing Jupyter notebook from /content/IncrementalLearning/cifar100.ipynb\n",
            "Files already downloaded and verified\n",
            "importing Jupyter notebook from /content/IncrementalLearning/resnet_cifar.ipynb\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HzqwQeHB1Tg-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "outputId": "c97ff8d9-fd55-452a-ac78-129c94d79711"
      },
      "source": [
        "#class tempDataset(Dataset):\n",
        "  \n",
        "  #def __init__(self, rgbData, targets):\n",
        "\n",
        "   # super(tempDataset, self).__init__()\n",
        "\n",
        "    #self.data = rgbData\n",
        "    #self.targets = targets\n",
        "    #self.transform = transforms.Compose(\n",
        "     # transforms.ToTensor(),\n",
        "      #transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))\n",
        "    #)\n",
        "\n",
        "\n",
        "  \n",
        "  #def __len__(self):\n",
        "   # return len(self.data)\n",
        "  \n",
        "  #def __get_item__(self, idx):\n",
        "   # img = self.tranform(self.data[ixd])\n",
        "    #return img, self.targets[idx]\n",
        "def process_inputs_fp(tg_model, inputs, fusion_mode=False, feature_mode=False):\n",
        "  tg_model_group1 = [tg_model.conv1, tg_model.bn1, tg_model.relu, tg_model.layer1]\n",
        "  tg_model_group1 = nn.Sequential(*tg_model_group1)\n",
        "  tg_fp1 = tg_model_group1(inputs)\n",
        "  fp1 = tg_fp1\n",
        "  tg_model_group2 = tg_model.layer2\n",
        "  tg_fp2 = tg_model_group2(fp1)\n",
        "  fp2 = tg_fp2\n",
        "  tg_model_group3 = [tg_model.layer3, tg_model.avgpool]\n",
        "  tg_model_group3 = nn.Sequential(*tg_model_group3)\n",
        "  tg_fp3 = tg_model_group3(fp2)\n",
        "  fp3 = tg_fp3\n",
        "  fp3 = fp3.view(fp3.size(0), -1)\n",
        "  if feature_mode:\n",
        "      return fp3\n",
        "  else:\n",
        "      outputs = tg_model.fc(fp3)\n",
        "      feature = fp3\n",
        "      return outputs, feature\n",
        "\n",
        "class mnemonics():\n",
        "  def __init__(self, randomseed):\n",
        "    self.model = resnet32(num_classes=100).to('cuda')\n",
        "    self.feature_extractor = self.model.features\n",
        "    self.lr = 2\n",
        "    self.gamma = 0.2\n",
        "    self.weight_decay = 1e-5 \n",
        "    self.milestones = [49,63]\n",
        "    self.batch_size = 128\n",
        "    self.numepochs = 1\n",
        "    self.n_classes = 0\n",
        "    self.n_known = 0\n",
        "    self.feature_size=64\n",
        "    self.momentum=0.9\n",
        "    self.criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    self.NUM_BATCHES=10\n",
        "    self.randomseed=randomseed\n",
        "    self.trainloader=None\n",
        "    self.testloader=None\n",
        "    self.CLASSES_PER_BATCH=10\n",
        "    self.original_training_set = ilCIFAR100(self.CLASSES_PER_BATCH,self.randomseed)\n",
        "    self.original_test_set = ilCIFAR100(self.CLASSES_PER_BATCH,self.randomseed, train=False)\n",
        "    #self.current_training_set = tempDataset()\n",
        "    self.exemplar_sets = []\n",
        "    self.exemplar_labels = []\n",
        "    self.last_test = None\n",
        "    self.y_pred = []\n",
        "    self.y_test = []\n",
        "\n",
        "    self.cumulative_class_mean = []\n",
        "\n",
        "    self.classes_seen=0\n",
        "    self.diz = self.original_training_set.get_dict()\n",
        "\n",
        "    self.mn_exemplar_means = None\n",
        "    # lista di liste, ogni lista contiene gli exemplars di una classe\n",
        "    self.mn_exemplar_sets = [] \n",
        "    # lista di liste, ogni lista contiene le labels dell'elemento corrispondente\n",
        "    self.mn_exemplar_labels = []\n",
        "\n",
        "  def update_params(self, train_data, trainable_params, epochs):\n",
        "    pass\n",
        "\n",
        "  def model_level_optimization(self):\n",
        "    old_model = copy.deepcopy(self.model)\n",
        "    old_model.eval()\n",
        "    old_model.to('cuda')\n",
        "    n_classes = self.classes_seen+self.CLASSES_PER_BATCH\n",
        "    print(n_classes)\n",
        "    optimizer = optim.SGD(self.model.parameters(), lr=self.lr, momentum=self.momentum, weight_decay=self.weight_decay)\n",
        "    scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=self.milestones, gamma=self.gamma)\n",
        "    for epoch in tqdm(range(self.numepochs)):\n",
        "        \n",
        "      for _, inputs, labels in self.trainloader:\n",
        "        inputs = inputs.float().cuda()\n",
        "        labels = torch.tensor([self.diz[c.item()] for c in labels])\n",
        "\n",
        "        labels=labels.to('cuda')\n",
        "        optimizer.zero_grad()\n",
        "        outputs=self.model(inputs)\n",
        "\n",
        "        labels_encoded = F.one_hot(labels,100).float().cuda() #CAMBIARE ONE_HOT\n",
        "        \n",
        "        if self.classes_seen:\n",
        "          old_target = old_model(inputs).cuda()\n",
        "          old_target = torch.sigmoid(old_target).cuda()\n",
        "          \n",
        "          target = torch.cat((old_target[:,:self.classes_seen], labels_encoded[:, self.classes_seen:]), dim=1)\n",
        "          loss = self.criterion(outputs, target)\n",
        "        else:\n",
        "          loss = self.criterion(outputs,labels_encoded) \n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "      \n",
        "      scheduler.step()\n",
        "\n",
        "  def exemplar_level_optimization(self, new_mn_exemplars):\n",
        "    pass\n",
        "\n",
        "\n",
        "  def classify_nme(self, input_batch):\n",
        "    min_distances = float('inf')*torch.ones(len(input_batch)).cuda() # shape: batch_size --> 128\n",
        "    y_pred = torch.zeros(len(input_batch), dtype=torch.int8).cuda() # shape: batch_size --> 128\n",
        "    input_features = self.model.features(input_batch) # shape: (batch_size, feature_size) --> (128, 64)\n",
        "\n",
        "    for i in range(len(self.exemplar_sets)):\n",
        "      ex_mean = self.exemplar_means[i,:]\n",
        "\n",
        "      # compute distances between inputs features and exemplar set means\n",
        "      pdist = nn.PairwiseDistance(p=2)\n",
        "      distances = pdist(input_features, ex_mean) # shape: batch_size --> 128\n",
        "\n",
        "      # update min distancies and predicted labels\n",
        "      mask = distances < min_distances\n",
        "      min_distances[mask] = distances[mask]\n",
        "      y_pred[mask] = self.exemplar_labels[i]\n",
        "\n",
        "    return y_pred\n",
        "    \n",
        "\n",
        "  def get_new_exemplars(self, batch, m):\n",
        "    loader = torch.utils.data.DataLoader(batch, batch_size=self.batch_size,shuffle=False, num_workers=4)\n",
        "    features = np.zeros((0,self.feature_size))\n",
        "    indices = np.zeros((0), dtype=int)\n",
        "    with torch.no_grad():\n",
        "      for indexes, images, labels in loader:\n",
        "        images = images.cuda()\n",
        "        feature = self.feature_extractor(images).data.cpu().numpy()\n",
        "        feature = normalize(feature, axis=1, norm='l2')\n",
        "        features = np.concatenate((features,feature), axis=0)\n",
        "        indices = np.concatenate((indices,indexes), axis=0)\n",
        "\n",
        "    class_mean = np.mean(features, axis=0)\n",
        "    class_mean = class_mean / np.linalg.norm(class_mean)  # Normalize\n",
        "\n",
        "    self.cumulative_class_mean.append(class_mean)\n",
        "\n",
        "    exemplar_set = []\n",
        "    exemplar_features = np.zeros((0,self.feature_size))\n",
        "\n",
        "    for k in range(1, int(m)+1):\n",
        "        S = np.sum(exemplar_features, axis=0)\n",
        "        phi = features\n",
        "        mu = class_mean\n",
        "        mu_p = 1.0 / k * (phi + S)\n",
        "        mu_p = normalize(mu_p, axis=1, norm='l2')\n",
        "        i = np.argmin(np.sqrt(np.sum((mu - mu_p) ** 2, axis=1)))\n",
        "        exemplar_set.append(indices[i])\n",
        "        addfeature =  np.expand_dims(features[i], axis=0)\n",
        "        exemplar_features = np.concatenate((exemplar_features,addfeature), axis=0)\n",
        "\n",
        "        #remove duplicates\n",
        "        features = np.delete(features, i, 0)\n",
        "        indices = np.delete(indices, i, 0)\n",
        "        \n",
        "    self.exemplar_sets.append(exemplar_set)\n",
        "        \n",
        "  def reduce_old_exemplars(self, m):\n",
        "    for y, P_y in enumerate(self.exemplar_sets):\n",
        "            self.exemplar_sets[y] = P_y[:int(m)]\n",
        "\n",
        "\n",
        "  def __accuracy_fc(self, dl, mapper):\n",
        "    total = 0.0\n",
        "    correct = 0.0\n",
        "    for  _, images, labels in dl:\n",
        "      labels = torch.tensor([torch.tensor(mapper[c.item()]) for c in labels])\n",
        "      labels = labels.cuda()\n",
        "      images = images.cuda()\n",
        "      outputs = self.model(images)\n",
        "      _, preds = torch.max(outputs, dim=1)\n",
        "      total += len(labels)\n",
        "      correct += torch.sum(preds == labels).item()\n",
        "\n",
        "    acc = correct / total\n",
        "    return acc\n",
        "\n",
        "\n",
        "  def __accuracy_nme(self, dl):\n",
        "    \n",
        "    total = 0.0\n",
        "    correct = 0.0\n",
        "    \n",
        "    for  _, images, labels in dl:\n",
        "      labels = labels.cuda()\n",
        "      images = images.cuda()\n",
        "      preds = self.classify_nme(images)\n",
        "      total += len(labels)\n",
        "      correct += torch.sum(preds == labels).item()\n",
        "\n",
        "      if self.last_test:\n",
        "        self.y_pred += preds.tolist()\n",
        "        self.y_test += labels.tolist()\n",
        "\n",
        "    acc = correct / total\n",
        "    return acc\n",
        "\n",
        "\n",
        "  def plot_confusion_matrix(self):\n",
        " \n",
        "    cm = confusion_matrix(self.y_test, self.y_pred)\n",
        "    cm = np.log(cm+1)\n",
        "    fig, ax = plt.subplots(figsize=(7,7))\n",
        "    sns.heatmap(cm, square=True, cbar=False, ax=ax, cmap=plt.get_cmap('seismic'))\n",
        "    ax.set_xticks(np.linspace(19,99,5))\n",
        "    ax.set_yticks(np.linspace(19,99,5))\n",
        "    ax.set_xticklabels([20,40,60,80,100], rotation=0)\n",
        "    ax.set_yticklabels([20,40,60,80,100], rotation=0)\n",
        "    ax.set_title(\"iCaRL\")\n",
        "    ax.set_xlabel(\"Predicted class\")\n",
        "    ax.set_ylabel(\"True class\")\n",
        "    plt.savefig(\"iCaRL_\"+str(self.randomseed)+\"_cm.png\")\n",
        "    plt.show()\n",
        "    return cm\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  def trainer(self):\n",
        "\n",
        "    train_indices = self.original_training_set.get_batch_indexes()\n",
        "    test_indices = self.original_test_set.get_batch_indexes()\n",
        "    batches=self.original_training_set.getbatches()\n",
        "    current_test_indexes=[]\n",
        "    test_acc = []\n",
        "    self.last_test = False\n",
        "\n",
        "    for i in range(self.NUM_BATCHES):\n",
        "      \n",
        "      if i == self.NUM_BATCHES-1:\n",
        "        self.last_test = True\n",
        "\n",
        "      for exemplar_set in self.exemplar_sets:\n",
        "        train_indices[i]=np.concatenate([train_indices[i], np.array(exemplar_set)])\n",
        "        #print(exemplars)\n",
        "\n",
        "      train_dataset = Subset(self.original_training_set, train_indices[i])\n",
        "      current_test_indexes += test_indices[i].tolist()\n",
        "      test_dataset = Subset(self.original_test_set,current_test_indexes)\n",
        "      self.trainloader = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True, num_workers=4, drop_last=True)\n",
        "      self.testloader = DataLoader(test_dataset, batch_size=self.batch_size, shuffle=False, num_workers=4, drop_last=True)        \n",
        "      self.model.train()\n",
        "      self.model_level_optimization()    \n",
        "      self.classes_seen += 10\n",
        "      #self.model.eval() # Set Network to evaluation mode\n",
        "\n",
        "      # update exemplars number\n",
        "      m=int(2000/(int(i*10+10)))\n",
        "\n",
        "      # reduce the number of each exemplars set\n",
        "      #self.reduce_old_exemplars(m) \n",
        "      \n",
        "      # randomly choose m indexes for each new class and load the corresponding \n",
        "      # data images as mn_exemplars\n",
        "      current_mn_exemplars = [] # 10 classi, m immagini per clasee, dimesioni delle immagini\n",
        "\n",
        "\n",
        "      for classlabel in batches[i]:\n",
        "        indexes_class = self.original_training_set.get_class_indexes(classlabel)\n",
        "        current_class = Subset(self.original_training_set, indexes_class)\n",
        "        #mn_indexes = np.random.choice(indexes_class, size=m, replace=False)\n",
        "        self.get_new_exemplars(current_class, m)\n",
        "        \n",
        "      print(self.original_training_set.dataset.data[1].shape)\n",
        "      self.img_size = 32\n",
        "      self.mnemonics_lrs = 0.01\n",
        "      num_classes_incremental = 10\n",
        "      num_classes = 10\n",
        "      nb_cl = 10\n",
        "      transform_proto = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5071,  0.4866,  0.4409), (0.2009,  0.1984,  0.2023)),])\n",
        "      self.mnemonics_label = []\n",
        "      exemplar_indices = np.array([])\n",
        "      prototypes = np.zeros((10, m, 3, 32, 32))\n",
        "      prototypes_label = np.zeros((10,m))\n",
        "      for xx, exemplar_set in enumerate(self.exemplar_sets[-10:]):\n",
        "        for j, el in enumerate(exemplar_set):\n",
        "          prototypes[xx][j] = self.original_training_set.__getitem__(int(el))[1]\n",
        "          prototypes_label[xx, j] = self.original_training_set.__getitem__(int(el))[2]\n",
        "      \n",
        "        #exemplar_indices = np.concatenate([exemplar_indices, np.array(exemplar_set)])\n",
        "      \n",
        "      #for xx in range(10):\n",
        "       # for j, el in enumerate(exemplar_indices):\n",
        "    \n",
        "      #print()  \n",
        "      self.mnemonics = nn.ParameterList()\n",
        "      self.mnemonics.append(nn.Parameter(torch.Tensor(prototypes)))\n",
        "      device = 'cuda'\n",
        "      self.mnemonics.to(device)\n",
        "      tg_feature_model = nn.Sequential(*list(self.model.children())[:-1])\n",
        "      tg_feature_model.eval()\n",
        "      self.model.eval()\n",
        "\n",
        "      self.mnemonics_optimizer = optim.SGD(self.mnemonics, lr=self.mnemonics_lrs, momentum=0.9, weight_decay=5e-4)\n",
        "      self.mnemonics_lr_scheduler = optim.lr_scheduler.StepLR(self.mnemonics_optimizer, step_size=5, gamma=0.2)\n",
        "      current_means_new = self.cumulative_class_mean\n",
        "\n",
        "      start_iteration = 0\n",
        "      for epoch in range(1):\n",
        "          \n",
        "          train_loss = 0\n",
        "          self.mnemonics_lr_scheduler.step()\n",
        "          for _, q_inputs, q_targets in self.trainloader:\n",
        "              q_targets = torch.tensor([self.diz[c.item()] for c in q_targets])\n",
        "              \n",
        "              q_inputs, q_targets = q_inputs.to(device), q_targets.to(device)\n",
        "              if i == start_iteration:\n",
        "                  q_feature = tg_feature_model(q_inputs)\n",
        "              else:\n",
        "                  q_feature = process_inputs_fp(tg_model, q_inputs, feature_mode=True)\n",
        "              self.mnemonics_optimizer.zero_grad()\n",
        "              total_tr_loss = 0 \n",
        "              if i == start_iteration:\n",
        "                  mnemonics_outputs = tg_feature_model(self.mnemonics[0][0])\n",
        "              else:\n",
        "                  mnemonics_outputs = process_inputs_fp(tg_model, self.mnemonics[0][0], feature_mode=True)\n",
        "              this_class_mean_mnemonics = torch.mean(mnemonics_outputs, dim=0)\n",
        "              this_class_mean_mnemonics = torch.squeeze(this_class_mean_mnemonics)\n",
        "              total_class_mean_mnemonics = this_class_mean_mnemonics.unsqueeze(dim=0)\n",
        "              for mnemonics_idx in range(len(self.mnemonics[0])-1):\n",
        "                  if i == start_iteration:\n",
        "                      mnemonics_outputs = tg_feature_model(self.mnemonics[0][mnemonics_idx+1])\n",
        "                  else:\n",
        "                      mnemonics_outputs = process_inputs_fp(tg_model, free_model, self.mnemonics[0][mnemonics_idx+1], feature_mode=True)\n",
        "                  this_class_mean_mnemonics = torch.mean(mnemonics_outputs, dim=0)\n",
        "                  this_class_mean_mnemonics = torch.squeeze(this_class_mean_mnemonics)\n",
        "                  total_class_mean_mnemonics =  torch.cat((total_class_mean_mnemonics, this_class_mean_mnemonics.unsqueeze(dim=0)), dim=0)\n",
        "              if i == start_iteration:\n",
        "                  all_cls_means = total_class_mean_mnemonics\n",
        "              else:\n",
        "                  all_cls_means = torch.tensor(current_means_new).float().to(device)\n",
        "                  all_cls_means[-nb_cl:] = total_class_mean_mnemonics\n",
        "              the_logits = F.linear(F.normalize(torch.squeeze(q_feature), p=2,dim=1), F.normalize(all_cls_means, p=2, dim=1))\n",
        "              loss = F.cross_entropy(the_logits, q_targets)\n",
        "              #loss = nn.CrossEntropyLoss(the_logits, q_targets)\n",
        "              loss.backward()\n",
        "              self.mnemonics_optimizer.step()\n",
        "              train_loss += loss.item()\n",
        "              print('sborro')\n",
        "'''\n",
        "      #PER NME CLASSIFIER\n",
        "      # compute means of exemplar set\n",
        "      # cycle for each exemplar set\n",
        "      self.exemplar_means = torch.zeros((0, self.feature_size), dtype=torch.float).cuda()\n",
        "      self.exemplar_labels = []\n",
        "      for i in range(len(self.exemplar_sets)):\n",
        "        exemplars_dataset = Subset(self.original_training_set, self.exemplar_sets[i])\n",
        "        exemplars_loader = torch.utils.data.DataLoader(exemplars_dataset, batch_size=self.batch_size, shuffle=False, num_workers=4)\n",
        "        ex_features = torch.zeros((0, self.feature_size), dtype=torch.float).cuda() # alla fine shape: (len(exemplar_set), feature_size) --> (m, 64)\n",
        "      \n",
        "        with torch.no_grad():\n",
        "          _, _, exemplar_label = self.original_training_set.__getitem__(self.exemplar_sets[i][0]) \n",
        "          self.exemplar_labels.append(exemplar_label)\n",
        "          # cycle for each batch in the current exemplar set\n",
        "          for _,  exemplars, _ in exemplars_loader:\n",
        "          \n",
        "            # get exemplars features\n",
        "            exemplars = exemplars.cuda()\n",
        "            features = self.model.features(exemplars) # shape: (len(exemplars), feature_size)\n",
        "          \n",
        "            # normalize \n",
        "            feature_norms = torch.norm(features, p=2, dim=1) # shape: len(exemplars)\n",
        "            feature_norms.unsqueeze_(1) # shape: (len(exemplars), 1)\n",
        "            features = features/feature_norms\n",
        "          \n",
        "            # concatenate over columns\n",
        "            ex_features = torch.cat((ex_features, features), dim=0)\n",
        "          \n",
        "        # compute current exemplar set mean and normalize it\n",
        "        ex_mean = torch.mean(ex_features, dim=0) # shape: feature_size --> 64\n",
        "        ex_mean = ex_mean/torch.norm(ex_mean)\n",
        "        ex_mean.unsqueeze_(0) # shape: (1, feature_size) --> (1, 64)\n",
        "        self.exemplar_means = torch.cat((self.exemplar_means, ex_mean), dim=0) # shape: (n_examplar set, feature size)\n",
        "      \n",
        "\n",
        "      print('accuracy on training set:', 100*self.__accuracy_fc(self.trainloader,self.diz))\n",
        "      # print('accuracy on test set:', self.__accuracy_on(self.testloader,self,self.diz))\n",
        "      current_test_acc = self.__accuracy_nme(self.testloader, last_test)\n",
        "      print('accuracy on test set:', 100*current_test_acc)\n",
        "      print('-' * 80)\n",
        "      test_acc.append(current_test_acc)\n",
        "\n",
        "    # compute comfusion matrix and save results\n",
        "    cm = self.plot_confusion_matrix()\n",
        "    with open('iCaRL_'+str(self.randomseed)+\"_cm\", 'wb') as file:\n",
        "      pickle.dump(cm, file, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "    with open('iCaRL_'+str(self.randomseed)+\"_testacc\", 'wb') as file:\n",
        "      pickle.dump(test_acc, file, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "'''"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n      #PER NME CLASSIFIER\\n      # compute means of exemplar set\\n      # cycle for each exemplar set\\n      self.exemplar_means = torch.zeros((0, self.feature_size), dtype=torch.float).cuda()\\n      self.exemplar_labels = []\\n      for i in range(len(self.exemplar_sets)):\\n        exemplars_dataset = Subset(self.original_training_set, self.exemplar_sets[i])\\n        exemplars_loader = torch.utils.data.DataLoader(exemplars_dataset, batch_size=self.batch_size, shuffle=False, num_workers=4)\\n        ex_features = torch.zeros((0, self.feature_size), dtype=torch.float).cuda() # alla fine shape: (len(exemplar_set), feature_size) --> (m, 64)\\n      \\n        with torch.no_grad():\\n          _, _, exemplar_label = self.original_training_set.__getitem__(self.exemplar_sets[i][0]) \\n          self.exemplar_labels.append(exemplar_label)\\n          # cycle for each batch in the current exemplar set\\n          for _,  exemplars, _ in exemplars_loader:\\n          \\n            # get exemplars features\\n            exemplars = exemplars.cuda()\\n            features = self.model.features(exemplars) # shape: (len(exemplars), feature_size)\\n          \\n            # normalize \\n            feature_norms = torch.norm(features, p=2, dim=1) # shape: len(exemplars)\\n            feature_norms.unsqueeze_(1) # shape: (len(exemplars), 1)\\n            features = features/feature_norms\\n          \\n            # concatenate over columns\\n            ex_features = torch.cat((ex_features, features), dim=0)\\n          \\n        # compute current exemplar set mean and normalize it\\n        ex_mean = torch.mean(ex_features, dim=0) # shape: feature_size --> 64\\n        ex_mean = ex_mean/torch.norm(ex_mean)\\n        ex_mean.unsqueeze_(0) # shape: (1, feature_size) --> (1, 64)\\n        self.exemplar_means = torch.cat((self.exemplar_means, ex_mean), dim=0) # shape: (n_examplar set, feature size)\\n      \\n\\n      print(\\'accuracy on training set:\\', 100*self.__accuracy_fc(self.trainloader,self.diz))\\n      # print(\\'accuracy on test set:\\', self.__accuracy_on(self.testloader,self,self.diz))\\n      current_test_acc = self.__accuracy_nme(self.testloader, last_test)\\n      print(\\'accuracy on test set:\\', 100*current_test_acc)\\n      print(\\'-\\' * 80)\\n      test_acc.append(current_test_acc)\\n\\n    # compute comfusion matrix and save results\\n    cm = self.plot_confusion_matrix()\\n    with open(\\'iCaRL_\\'+str(self.randomseed)+\"_cm\", \\'wb\\') as file:\\n      pickle.dump(cm, file, protocol=pickle.HIGHEST_PROTOCOL)\\n    with open(\\'iCaRL_\\'+str(self.randomseed)+\"_testacc\", \\'wb\\') as file:\\n      pickle.dump(test_acc, file, protocol=pickle.HIGHEST_PROTOCOL)\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581,
          "referenced_widgets": [
            "42618ade859746d4b81c6893601def43",
            "f09c4efb555d434bac45e7c59014f6ac",
            "4f04fcb8344e45cb8eced90063ef2b3e",
            "a0387061266a49898fa7cecfe4659898",
            "69741978e1ef49b8852d7d95c62f83e0",
            "36faba75789949f38a85d765a2b1c098",
            "b0a728c741ec4a86ab91e4ae08113166",
            "4951f8f7304240c198301c2d2a1832b1"
          ]
        },
        "id": "OYzLuYGDLr15",
        "outputId": "e769ef6e-0872-4411-b980-6d3533eb3665"
      },
      "source": [
        "model = mnemonics(randomseed=203)\n",
        "model.trainer()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:434: UserWarning: Setting attributes on ParameterList is not supported.\n",
            "  warnings.warn(\"Setting attributes on ParameterList is not supported.\")\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "sborro\n",
            "sborro\n",
            "sborro\n",
            "sborro\n",
            "sborro\n",
            "sborro\n",
            "sborro\n",
            "sborro\n",
            "sborro\n",
            "sborro\n",
            "sborro\n",
            "sborro\n",
            "sborro\n",
            "sborro\n",
            "sborro\n",
            "sborro\n",
            "sborro\n",
            "sborro\n",
            "sborro\n",
            "sborro\n",
            "sborro\n",
            "sborro\n",
            "sborro\n",
            "sborro\n",
            "sborro\n",
            "sborro\n",
            "sborro\n",
            "sborro\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dQCpuLH9-gwr",
        "outputId": "d4f39a6b-d47a-4ae8-9b8e-addf721b230f"
      },
      "source": [
        "prototypes = np.zeros((10, 1, 3, 32, 32))\r\n",
        "prototypes[0,:,:,:,:] = 1\r\n",
        "print(prototypes)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[[[[1. 1. 1. ... 1. 1. 1.]\n",
            "    [1. 1. 1. ... 1. 1. 1.]\n",
            "    [1. 1. 1. ... 1. 1. 1.]\n",
            "    ...\n",
            "    [1. 1. 1. ... 1. 1. 1.]\n",
            "    [1. 1. 1. ... 1. 1. 1.]\n",
            "    [1. 1. 1. ... 1. 1. 1.]]\n",
            "\n",
            "   [[1. 1. 1. ... 1. 1. 1.]\n",
            "    [1. 1. 1. ... 1. 1. 1.]\n",
            "    [1. 1. 1. ... 1. 1. 1.]\n",
            "    ...\n",
            "    [1. 1. 1. ... 1. 1. 1.]\n",
            "    [1. 1. 1. ... 1. 1. 1.]\n",
            "    [1. 1. 1. ... 1. 1. 1.]]\n",
            "\n",
            "   [[1. 1. 1. ... 1. 1. 1.]\n",
            "    [1. 1. 1. ... 1. 1. 1.]\n",
            "    [1. 1. 1. ... 1. 1. 1.]\n",
            "    ...\n",
            "    [1. 1. 1. ... 1. 1. 1.]\n",
            "    [1. 1. 1. ... 1. 1. 1.]\n",
            "    [1. 1. 1. ... 1. 1. 1.]]]]\n",
            "\n",
            "\n",
            "\n",
            " [[[[0. 0. 0. ... 0. 0. 0.]\n",
            "    [0. 0. 0. ... 0. 0. 0.]\n",
            "    [0. 0. 0. ... 0. 0. 0.]\n",
            "    ...\n",
            "    [0. 0. 0. ... 0. 0. 0.]\n",
            "    [0. 0. 0. ... 0. 0. 0.]\n",
            "    [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            "   [[0. 0. 0. ... 0. 0. 0.]\n",
            "    [0. 0. 0. ... 0. 0. 0.]\n",
            "    [0. 0. 0. ... 0. 0. 0.]\n",
            "    ...\n",
            "    [0. 0. 0. ... 0. 0. 0.]\n",
            "    [0. 0. 0. ... 0. 0. 0.]\n",
            "    [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            "   [[0. 0. 0. ... 0. 0. 0.]\n",
            "    [0. 0. 0. ... 0. 0. 0.]\n",
            "    [0. 0. 0. ... 0. 0. 0.]\n",
            "    ...\n",
            "    [0. 0. 0. ... 0. 0. 0.]\n",
            "    [0. 0. 0. ... 0. 0. 0.]\n",
            "    [0. 0. 0. ... 0. 0. 0.]]]]\n",
            "\n",
            "\n",
            "\n",
            " [[[[0. 0. 0. ... 0. 0. 0.]\n",
            "    [0. 0. 0. ... 0. 0. 0.]\n",
            "    [0. 0. 0. ... 0. 0. 0.]\n",
            "    ...\n",
            "    [0. 0. 0. ... 0. 0. 0.]\n",
            "    [0. 0. 0. ... 0. 0. 0.]\n",
            "    [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            "   [[0. 0. 0. ... 0. 0. 0.]\n",
            "    [0. 0. 0. ... 0. 0. 0.]\n",
            "    [0. 0. 0. ... 0. 0. 0.]\n",
            "    ...\n",
            "    [0. 0. 0. ... 0. 0. 0.]\n",
            "    [0. 0. 0. ... 0. 0. 0.]\n",
            "    [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            "   [[0. 0. 0. ... 0. 0. 0.]\n",
            "    [0. 0. 0. ... 0. 0. 0.]\n",
            "    [0. 0. 0. ... 0. 0. 0.]\n",
            "    ...\n",
            "    [0. 0. 0. ... 0. 0. 0.]\n",
            "    [0. 0. 0. ... 0. 0. 0.]\n",
            "    [0. 0. 0. ... 0. 0. 0.]]]]\n",
            "\n",
            "\n",
            "\n",
            " ...\n",
            "\n",
            "\n",
            "\n",
            " [[[[0. 0. 0. ... 0. 0. 0.]\n",
            "    [0. 0. 0. ... 0. 0. 0.]\n",
            "    [0. 0. 0. ... 0. 0. 0.]\n",
            "    ...\n",
            "    [0. 0. 0. ... 0. 0. 0.]\n",
            "    [0. 0. 0. ... 0. 0. 0.]\n",
            "    [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            "   [[0. 0. 0. ... 0. 0. 0.]\n",
            "    [0. 0. 0. ... 0. 0. 0.]\n",
            "    [0. 0. 0. ... 0. 0. 0.]\n",
            "    ...\n",
            "    [0. 0. 0. ... 0. 0. 0.]\n",
            "    [0. 0. 0. ... 0. 0. 0.]\n",
            "    [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            "   [[0. 0. 0. ... 0. 0. 0.]\n",
            "    [0. 0. 0. ... 0. 0. 0.]\n",
            "    [0. 0. 0. ... 0. 0. 0.]\n",
            "    ...\n",
            "    [0. 0. 0. ... 0. 0. 0.]\n",
            "    [0. 0. 0. ... 0. 0. 0.]\n",
            "    [0. 0. 0. ... 0. 0. 0.]]]]\n",
            "\n",
            "\n",
            "\n",
            " [[[[0. 0. 0. ... 0. 0. 0.]\n",
            "    [0. 0. 0. ... 0. 0. 0.]\n",
            "    [0. 0. 0. ... 0. 0. 0.]\n",
            "    ...\n",
            "    [0. 0. 0. ... 0. 0. 0.]\n",
            "    [0. 0. 0. ... 0. 0. 0.]\n",
            "    [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            "   [[0. 0. 0. ... 0. 0. 0.]\n",
            "    [0. 0. 0. ... 0. 0. 0.]\n",
            "    [0. 0. 0. ... 0. 0. 0.]\n",
            "    ...\n",
            "    [0. 0. 0. ... 0. 0. 0.]\n",
            "    [0. 0. 0. ... 0. 0. 0.]\n",
            "    [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            "   [[0. 0. 0. ... 0. 0. 0.]\n",
            "    [0. 0. 0. ... 0. 0. 0.]\n",
            "    [0. 0. 0. ... 0. 0. 0.]\n",
            "    ...\n",
            "    [0. 0. 0. ... 0. 0. 0.]\n",
            "    [0. 0. 0. ... 0. 0. 0.]\n",
            "    [0. 0. 0. ... 0. 0. 0.]]]]\n",
            "\n",
            "\n",
            "\n",
            " [[[[0. 0. 0. ... 0. 0. 0.]\n",
            "    [0. 0. 0. ... 0. 0. 0.]\n",
            "    [0. 0. 0. ... 0. 0. 0.]\n",
            "    ...\n",
            "    [0. 0. 0. ... 0. 0. 0.]\n",
            "    [0. 0. 0. ... 0. 0. 0.]\n",
            "    [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            "   [[0. 0. 0. ... 0. 0. 0.]\n",
            "    [0. 0. 0. ... 0. 0. 0.]\n",
            "    [0. 0. 0. ... 0. 0. 0.]\n",
            "    ...\n",
            "    [0. 0. 0. ... 0. 0. 0.]\n",
            "    [0. 0. 0. ... 0. 0. 0.]\n",
            "    [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            "   [[0. 0. 0. ... 0. 0. 0.]\n",
            "    [0. 0. 0. ... 0. 0. 0.]\n",
            "    [0. 0. 0. ... 0. 0. 0.]\n",
            "    ...\n",
            "    [0. 0. 0. ... 0. 0. 0.]\n",
            "    [0. 0. 0. ... 0. 0. 0.]\n",
            "    [0. 0. 0. ... 0. 0. 0.]]]]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SC5IFnEh-hcl",
        "outputId": "efa6c4f8-a236-4c7a-e845-205fa914217e"
      },
      "source": [
        "for a in enumerate([14,4,4]):\r\n",
        "  print(a)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(0, 14)\n",
            "(1, 4)\n",
            "(2, 4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V5gt_kwxDiBi"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}