{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Immagini sintetiche integrate nell'allenamento.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "8a9b660641d54c47bbcfe4e7b7e05559": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_231c89fb8ad74b6e9ee17ed065bcc49b",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_422a21bc6637418697da730a562664e0",
              "IPY_MODEL_ebbacd8f79ca463b8cc04863a08b71f6"
            ]
          }
        },
        "231c89fb8ad74b6e9ee17ed065bcc49b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "422a21bc6637418697da730a562664e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_fb42262e8e7d4063ad7f18fcedc53491",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 70,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 70,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4801bc6e119044e7896ac6f04c8f4eee"
          }
        },
        "ebbacd8f79ca463b8cc04863a08b71f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_bbdbceab2ecc4e0492387fbec20a2779",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 70/70 [04:32&lt;00:00,  3.90s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4240c00beb144caba4d19f3ff0f98398"
          }
        },
        "fb42262e8e7d4063ad7f18fcedc53491": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4801bc6e119044e7896ac6f04c8f4eee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "bbdbceab2ecc4e0492387fbec20a2779": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4240c00beb144caba4d19f3ff0f98398": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d453cfe60a4445bc8b9720b60ab825b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_1a786c29900b49dabe222c9615c38c3c",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_3cc388d7927645c5b50f841eadacbcd0",
              "IPY_MODEL_ccd2e8755d8f4e11854bd89b29459010"
            ]
          }
        },
        "1a786c29900b49dabe222c9615c38c3c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3cc388d7927645c5b50f841eadacbcd0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_9b6987eaca1743b3bbbf51c521838fdb",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 2000,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 2000,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_809b1b9ef55343a7bcb5fb047e0a464a"
          }
        },
        "ccd2e8755d8f4e11854bd89b29459010": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_ca92211a11a34e87b2f68ec2339278a0",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 2000/2000 [10:08&lt;00:00,  3.29it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_edac0f26ecd9454393b657c571ac4f7b"
          }
        },
        "9b6987eaca1743b3bbbf51c521838fdb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "809b1b9ef55343a7bcb5fb047e0a464a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ca92211a11a34e87b2f68ec2339278a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "edac0f26ecd9454393b657c571ac4f7b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "46793bcc78c54ee287aadd37ae35d6a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_b3b7443e3312487291d1732ba6108ff4",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_d6ce9a0eaaac44b4a82ace7eadb31631",
              "IPY_MODEL_d899909d635745fc872ff662e7b47cfb"
            ]
          }
        },
        "b3b7443e3312487291d1732ba6108ff4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d6ce9a0eaaac44b4a82ace7eadb31631": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_81ead15fddd545b3b5d6e3b152642a90",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 70,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 70,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_48854589e5844e0aa45f60badaddd4f2"
          }
        },
        "d899909d635745fc872ff662e7b47cfb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_9eecd3c8c9714b18ae4c855e36b7db15",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 70/70 [20:41&lt;00:00, 17.73s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_bb8635d8039049c6b45eab0c0c47f310"
          }
        },
        "81ead15fddd545b3b5d6e3b152642a90": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "48854589e5844e0aa45f60badaddd4f2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9eecd3c8c9714b18ae4c855e36b7db15": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "bb8635d8039049c6b45eab0c0c47f310": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c465e2f3bb3c4aabbe1808aaf2dc10da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_279a75ced88f4f27911f40002732b82e",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_ab2b81b93afa41de8bae496bfb9e2f91",
              "IPY_MODEL_6805c3102397405897214cfe6cb0e8b8"
            ]
          }
        },
        "279a75ced88f4f27911f40002732b82e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ab2b81b93afa41de8bae496bfb9e2f91": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_92c62f85026e41dd8e3574e5a7250077",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 70,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 70,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a238202280264cdb899e73456400f836"
          }
        },
        "6805c3102397405897214cfe6cb0e8b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_9034c74b912f45758c2d20f2055e4a8e",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 70/70 [15:58&lt;00:00, 13.70s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2eced2ecd43449dabd20be7e1289e73d"
          }
        },
        "92c62f85026e41dd8e3574e5a7250077": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a238202280264cdb899e73456400f836": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9034c74b912f45758c2d20f2055e4a8e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2eced2ecd43449dabd20be7e1289e73d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alessandronicolini/IncrementalLearning/blob/main/Immagini_sintetiche_integrate_nell'allenamento.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WH7YHXeh0hFj",
        "outputId": "df9214fe-0c1d-42cd-b763-2c87fcd6718f"
      },
      "source": [
        "!pip3 install 'import_ipynb'\n",
        "!pip3 install 'tqdm'\n",
        " \n",
        "!rm -r IncrementalLearning\n",
        "# upload work files from your git hub repository\n",
        "import sys\n",
        " \n",
        "!git clone https://github.com/alessandronicolini/IncrementalLearning.git # clone proj repository\n",
        "!rm -rf IncrementalLearning/README.md \n",
        "!rm -rf IncrementalLearning/baselines.ipynb\n",
        " \n",
        "path = 'IncrementalLearning/'\n",
        "if path not in sys.path:\n",
        "    sys.path.append('IncrementalLearning/')\n",
        " \n",
        "!pip3 install import_ipynb"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: import_ipynb in /usr/local/lib/python3.6/dist-packages (0.1.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.41.1)\n",
            "Cloning into 'IncrementalLearning'...\n",
            "remote: Enumerating objects: 166, done.\u001b[K\n",
            "remote: Counting objects: 100% (166/166), done.\u001b[K\n",
            "remote: Compressing objects: 100% (165/165), done.\u001b[K\n",
            "remote: Total 640 (delta 102), reused 0 (delta 0), pack-reused 474\u001b[K\n",
            "Receiving objects: 100% (640/640), 860.31 KiB | 8.12 MiB/s, done.\n",
            "Resolving deltas: 100% (377/377), done.\n",
            "Requirement already satisfied: import_ipynb in /usr/local/lib/python3.6/dist-packages (0.1.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UEF9KBox0cAd"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import VisionDataset\n",
        "from PIL import Image\n",
        "import random\n",
        "import torchvision\n",
        "ROOT = './data'\n",
        "class ilCIFAR100(VisionDataset):\n",
        "    \"\"\"\n",
        "    Extends CIFAR100 class. Split the dataset into 10 batches, each one containing 10 classes.\n",
        "    You can retrieve the batches from the attribute \"batches\", it has different structure according to\n",
        "    test and train CIFAR100 splits:\n",
        "        - train -> batches is a dictionary {0:{'train':indexes, 'val':indexes}...} \n",
        "        - test -> batches is a dictionary {0:indexes...}\n",
        "    where the keys are the batch number.\n",
        "\n",
        "    Args:\n",
        "        root (string): Root directory of dataset where directory\n",
        "            `cifar-10-batches-py` exists or will be saved to if download is set to True.\n",
        "        seed(int): used to ensure reproducibility in shuffling operations.\n",
        "        val_size(float, optional): between 0 and 1, fraction of data used for validation.\n",
        "        train (bool, optional): If True, creates dataset from training set, otherwise\n",
        "            creates from test set.\n",
        "        transform (callable, optional): A function/transform that takes in an PIL image\n",
        "            and returns a transformed version. E.g, `transforms.RandomCrop`\n",
        "        target_transform (callable, optional): A function/transform that takes in the\n",
        "            target and transforms it.\n",
        "        download (bool, optional): If true, downloads the dataset from the internet and\n",
        "            puts it in root directory. If dataset is already downloaded, it is not\n",
        "            downloaded again.\n",
        "    \"\"\"\n",
        "    def __init__(self,classes_per_batch, seed, val_size=0.1, train=True, transform=None, target_transform=False, \n",
        "    download=True):\n",
        "        \n",
        "        super(ilCIFAR100, self).__init__(root=0)\n",
        "        self.classes_per_batch=classes_per_batch\n",
        "\n",
        "        \n",
        "        self.__rs = seed # set random seed \n",
        "        self.train=train\n",
        "        self.__transform_train = transforms.Compose([\n",
        "            transforms.RandomCrop(32, padding=4),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))\n",
        "        ])\n",
        "\n",
        "        self.__transform_test = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))\n",
        "        ])\n",
        "        # if train:\n",
        "        #     self.batches = self.__make_train_batches(val_size)\n",
        "        # else:\n",
        "        #     self.batches = self.__make_test_batches()\n",
        "        if self.train == 'train':\n",
        "          self.dataset = torchvision.datasets.CIFAR100(root=ROOT, train=True,\n",
        "                                            download=True, transform=self.__transform_train)\n",
        "        elif self.train == 'exemplars':\n",
        "          self.dataset = torchvision.datasets.CIFAR100(root=ROOT, train=True,\n",
        "                                            download=True, transform=self.__transform_test)\n",
        "        else:\n",
        "          self.dataset = torchvision.datasets.CIFAR100(root=ROOT, train=False,\n",
        "                                        download=True, transform=self.__transform_test)\n",
        "          \n",
        "        self.targets = np.array(self.dataset.targets) # make targets an array to exploit masking\n",
        "        random.seed(seed)\n",
        "        self.classes = random.sample(range(0, 100), 100)\n",
        "        #self.classes = self.classes.reshape((10, -1)) # each row contains the classes for the corrisponding batch\n",
        "        #print(self.classes)\n",
        "        self.__dictionary = {}\n",
        "        for i, c in enumerate(self.classes):\n",
        "          self.__dictionary[c] = i\n",
        "\n",
        "\n",
        "    def get_dict(self):\n",
        "      return self.__dictionary\n",
        "    def __getitem__(self, index):\n",
        "        return index,self.dataset.__getitem__(index)[0],self.dataset.__getitem__(index)[1]\n",
        "    def __len__(self):\n",
        "        return self.dataset.__len__()\n",
        "    def getbatches(self):\n",
        "      classlist=self.classes\n",
        "      batches=[]\n",
        "      for i in range(0,int(100/self.classes_per_batch)):\n",
        "        #print(i)\n",
        "        batch=classlist[int(i*self.classes_per_batch):int(i*10+self.classes_per_batch)]\n",
        "        batches.append(batch)\n",
        "      return batches\n",
        "    def get_batch_indexes(self):\n",
        "      classlist=self.classes\n",
        "      numclass=self.classes_per_batch\n",
        "      batch_indexes=[]\n",
        "      for i in range(0,int(100/self.classes_per_batch)):\n",
        "        batch=classlist[int(i*numclass):int(i*numclass+numclass)]\n",
        "        mask=np.isin(self.targets,batch)\n",
        "        indexes=np.array(np.arange(len(self.dataset.targets)))\n",
        "        indexes=indexes[mask]\n",
        "        batch_indexes.append(indexes)\n",
        "      return batch_indexes\n",
        "    def get_class_indexes(self,label):\n",
        "      indexes = np.array(np.arange(len(self.dataset.targets)))\n",
        "      labels = self.dataset.targets\n",
        "      mask = np.isin(labels, label)\n",
        "      indexes = indexes[mask]\n",
        "\n",
        "      return indexes\n",
        "    def get_train_val(self,valid):\n",
        "      batches=self.get_batch_indexes()\n",
        "      train=[]\n",
        "      val=[]\n",
        "      for batch in batches:\n",
        "        #print(type(batch))\n",
        "        random.shuffle(batch)\n",
        "        valbatch=batch[0:int(valid*len(batch))]\n",
        "        trainbatch=batch[int(valid*len(batch)):]\n",
        "        train.append(trainbatch)\n",
        "        val.append(valbatch)\n",
        "      return train,val\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import time\n",
        "import os\n",
        "import argparse\n",
        "import numpy as np\n",
        "\n",
        "def tensor2im(input_image, imtype=np.uint8):\n",
        "    #CIFAR100\n",
        "    mean = [0.5071, 0.4867, 0.4408] \n",
        "    std = [0.2675, 0.2565, 0.2761]\n",
        "    #CIFAR10\n",
        "    #mean = [0.4914, 0.4822, 0.4465]\n",
        "    #std = [0.2023, 0.1994, 0.2010]\n",
        "    if not isinstance(input_image, np.ndarray):\n",
        "        if isinstance(input_image, torch.Tensor):\n",
        "            image_tensor = input_image.data\n",
        "        else:\n",
        "            return input_image\n",
        "        image_numpy = image_tensor.cpu().detach().float().numpy()\n",
        "        if image_numpy.shape[0] == 1:\n",
        "            image_numpy = np.tile(image_numpy, (3, 1, 1))\n",
        "        for i in range(len(mean)): \n",
        "            image_numpy[i] = image_numpy[i] * std[i] + mean[i]\n",
        "        image_numpy = image_numpy * 255\n",
        "        image_numpy = np.transpose(image_numpy, (1, 2, 0))\n",
        "    else:\n",
        "        image_numpy = input_image\n",
        "    return image_numpy.astype(imtype)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5O4jUchQ1EAI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73aa82c2-b6f4-4d57-8ea2-26beb782e82e"
      },
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import math\n",
        "from sklearn.preprocessing import normalize\n",
        "import copy\n",
        "import torchvision.datasets as dsets\n",
        "import torchvision.models as models\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.gridspec as gridspec\n",
        "from torch.utils.data import Subset, DataLoader, Dataset\n",
        "import random\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pickle\n",
        "import pandas as pd\n",
        "\n",
        "import import_ipynb\n",
        "#from IncrementalLearning.cifar100 import ilCIFAR100\n",
        "\n",
        "from IncrementalLearning.resnet_cifar import resnet32\n",
        "from tqdm.notebook import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "importing Jupyter notebook from /content/IncrementalLearning/resnet_cifar.ipynb\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 132
        },
        "id": "RAoV7Gq_aBW3",
        "outputId": "b3f098eb-b783-45d9-b59d-a7da72ff806c"
      },
      "source": [
        "class mnemonics():\n",
        "  def __init__(self, randomseed):\n",
        "    self.device = 'cuda'\n",
        "    self.model = resnet32(num_classes=100).to(self.device)\n",
        "    self.feature_extractor = self.model.features\n",
        "    self.temp_model = None\n",
        "    self.lr = 2\n",
        "    self.gamma = 0.2\n",
        "    self.weight_decay = 1e-5 \n",
        "    self.milestones = [49,63]\n",
        "    self.batch_size = 128\n",
        "    self.numepochs = 70\n",
        "    self.n_classes = 0\n",
        "    self.n_known = 0\n",
        "    self.feature_size=64\n",
        "    self.momentum=0.9\n",
        "    self.criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    self.NUM_BATCHES=10\n",
        "    self.randomseed=randomseed\n",
        "    self.trainloader=None\n",
        "    self.testloader=None\n",
        "    self.CLASSES_PER_BATCH=10\n",
        "\n",
        "    self.original_training_set = ilCIFAR100(self.CLASSES_PER_BATCH, self.randomseed, train = 'train')\n",
        "    self.original_exemplar_set = ilCIFAR100(self.CLASSES_PER_BATCH, self.randomseed, train = 'exemplars')\n",
        "    self.original_test_set = ilCIFAR100(self.CLASSES_PER_BATCH,self.randomseed, train= 'test')\n",
        "\n",
        "    self.last_test = None\n",
        "    self.y_pred = []\n",
        "    self.y_test = []\n",
        "\n",
        "    self.cumulative_class_mean = []\n",
        "\n",
        "    self.classes_seen=0\n",
        "    self.diz = self.original_training_set.get_dict()\n",
        "\n",
        "    self.exemplar_features_mean = None\n",
        "    # lista di liste, ogni lista contiene gli exemplars di una classe\n",
        "    self.exemplar_sets_idxs = [] # mn_exemplat_sets\n",
        "    # lista unica, tutti gli indici degli exemplar\n",
        "    self.exemplar_idxs = []\n",
        "\n",
        "  '''\n",
        "  def update_params(self, \n",
        "                    m,\n",
        "                    finetuning_idxs, \n",
        "                    training_idxs, \n",
        "                    mnemonics_to_optimize, \n",
        "                    batch_size,\n",
        "                    new=True,\n",
        "                    lr=10, \n",
        "                    momentum=0.9, \n",
        "                    weight_decay=1e-5, \n",
        "                    milestones=[10, 20, 30, 40],\n",
        "                    gamma=0.5, \n",
        "                    tuning_epochs=4,\n",
        "                    updating_epochs=50):\n",
        "    \n",
        "    \"\"\"\n",
        "    finetuning_idxs = indexes of current task elements\n",
        "    mnemonics_idxs = indexes of exemplar elements\n",
        "    mnemonics_to_optimize = the optimized parameters in the update phase\n",
        "    \"\"\"\n",
        "\n",
        "    # make a copy of the model\n",
        "    model_copy = copy.deepcopy(self.model)\n",
        "    model_copy.train()\n",
        "    model_copy.to(self.device)\n",
        "\n",
        "    # define the loss\n",
        "    # criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    # FINE TUNING FOR 1 EPOCH eq. 8 --------------------------------------------\n",
        "    \n",
        "    # define optimizer and scheduler for fine tuning phase\n",
        "    optimizer = optim.SGD(model_copy.parameters(), lr=2, momentum=momentum, weight_decay=weight_decay)\n",
        "    \n",
        "    # create the subset dataset to load the data you want, and the loader\n",
        "    finetuning_labels = np.array([self.original_training_set.__getitem__(idx)[2] for idx in finetuning_idxs], dtype=int)\n",
        "    meta_idxs = [i for i in range(len(finetuning_idxs))]\n",
        "    random.shuffle(meta_idxs)\n",
        "\n",
        "    # split the meta idxs in batches\n",
        "    n_batches = int(np.floor(len(finetuning_idxs)/batch_size))\n",
        "    meta_idxs_batches = []\n",
        "    for i in range(n_batches):\n",
        "      meta_idxs_batches.append(np.array(meta_idxs[batch_size*i:batch_size*(i+1)]))\n",
        "    meta_idxs_batches.append(np.array(meta_idxs[batch_size*n_batches:]))\n",
        "\n",
        "    # now fine tune the copied model\n",
        "    for epoch in range(tuning_epochs):\n",
        "      for meta_idxs_batch in meta_idxs_batches:\n",
        "        inputs = mnemonics_to_optimize[0][meta_idxs_batch] # are already in cuda\n",
        "        labels = finetuning_labels[meta_idxs_batch]\n",
        "        labels = torch.tensor([self.diz[c] for c in labels])\n",
        "        labels = labels.to(self.device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model_copy(inputs)\n",
        "        labels_encoded = F.one_hot(labels,100).float().to(self.device)\n",
        "        loss = self.criterion(outputs, labels_encoded)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    \n",
        "\n",
        "    # UPDATE THE MNEMONICS eq.9/10 ---------------------------------------------\n",
        "    \n",
        "    model_copy.eval()\n",
        "    \n",
        "    optimizer = optim.SGD(mnemonics_to_optimize, lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
        "    scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=milestones, gamma=gamma)\n",
        "    \n",
        "\n",
        "    if new:\n",
        "      exlvl_training = Subset(self.original_training_set, training_idxs)\n",
        "      exlvl_loader = DataLoader(exlvl_training, batch_size=batch_size, shuffle=True, num_workers=4)\n",
        "\n",
        "      current_task_labels = set([self.original_training_set.__getitem__(idx)[2] for idx in training_idxs])\n",
        "      new_dict = {label:new_label for label, new_label in zip(current_task_labels, range(10))}\n",
        "\n",
        "      new_class_mean = {new_dict[key] : value for key, value in self.cumulative_class_mean.items()}\n",
        "      means_ready = torch.Tensor(list(new_class_mean.values())).to(self.device)\n",
        "\n",
        "    \n",
        "    else:\n",
        "      exlvl_training = Subset(self.original_exemplar_set, training_idxs)\n",
        "      exlvl_loader = DataLoader(exlvl_training, batch_size=batch_size, shuffle=True, num_workers=4)\n",
        "\n",
        "    print('lunghezza del exemplar update:', len(training_idxs))\n",
        "    for epoch in tqdm(range(updating_epochs)):\n",
        "\n",
        "      for _, inputs, labels in exlvl_loader:\n",
        "\n",
        "        if new:\n",
        "          labels = torch.tensor([new_dict[c.item()] for c in labels])\n",
        "        else:\n",
        "          labels = torch.tensor([self.diz[c.item()] for c in labels])\n",
        "\n",
        "        labels = labels.to(self.device)\n",
        "        inputs = inputs.to(self.device)\n",
        "        optimizer.zero_grad()\n",
        "        out_features = model_copy.features(inputs)\n",
        "        # compute features mean of mnemonics for each class\n",
        "        if new:\n",
        "          the_logits = F.linear(F.normalize(out_features, p=2, dim=1), F.normalize(means_ready, p=2, dim=1))\n",
        "        else:\n",
        "          ##Da capire questa cosa!!!!!!! se provassi a mettere self?\n",
        "          #OPPURE uso un altro tensore copiando all_class_means_calcolato_fuori?  \n",
        "          #dov'Ã¨ il gradiente?\n",
        "          n_classes = int(len(finetuning_labels)/m)\n",
        "          all_class_means = torch.zeros((0, 64))\n",
        "          all_class_means = all_class_means.to(self.device)\n",
        "          for i in range(n_classes): # how many classes\n",
        "            mnemonics_features = model_copy.features(mnemonics_to_optimize[0][i*m:(i+1)*m])\n",
        "            this_class_means = torch.mean(mnemonics_features, dim=0) # size 64\n",
        "            this_class_means = torch.unsqueeze(this_class_means, dim=0) # add the second dimension\n",
        "            all_class_means = torch.cat((all_class_means, this_class_means), dim=0)\n",
        "          the_logits = F.linear(F.normalize(out_features, p=2, dim=1), F.normalize(all_class_means, p=2, dim=1))\n",
        "\n",
        "        #labels_encoded = F.one_hot(labels,100).float().cuda()\n",
        "\n",
        "        loss = F.cross_entropy(the_logits, labels) # al secondo batch di classi per i new mnemonics le uscite sono sempre 10 ma le label vanno da 10 a 19\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "  def exemplar_level_optimization(self, m, task_num, current_task_indices):  \n",
        "    \n",
        "    # UPDATING NEW EXEMPLAR-----------------------------------------------------\n",
        "\n",
        "    # isola gli indici dei nuovi exemplars\n",
        "    new_exemplar_idxs = []\n",
        "    for idxs in self.exemplar_sets_idxs[-10:]:\n",
        "      new_exemplar_idxs += idxs\n",
        "\n",
        "    # ora ottieni gli mnemonics che poi sono da ottimizzare\n",
        "    new_mnemonics_data = torch.zeros((10*m, 3, 32, 32))\n",
        "\n",
        "    for i, idx in enumerate(new_exemplar_idxs):\n",
        "      new_mnemonics_data[i, :, :, :] = self.original_training_set.__getitem__(int(idx))[1]\n",
        "\n",
        "    new_mnemonics = nn.ParameterList()\n",
        "    new_mnemonics.append(nn.Parameter(new_mnemonics_data))\n",
        "    new_mnemonics.to(self.device)\n",
        "    \n",
        "    #print(new_mnemonics[0][0])\n",
        "\n",
        "    options_new ={'finetuning_idxs': new_exemplar_idxs, \n",
        "                  'training_idxs': current_task_indices, \n",
        "                  'mnemonics_to_optimize':  new_mnemonics,  \n",
        "                  'batch_size':128,\n",
        "                  'm':m}\n",
        "\n",
        "    print('---start mnemonics updating---')\n",
        "\n",
        "    self.update_params(**options_new)    \n",
        "\n",
        "    for i, idx in enumerate(new_exemplar_idxs):\n",
        "      self.original_exemplar_set.dataset.data[idx] = tensor2im(new_mnemonics[0][i])\n",
        "\n",
        "    \n",
        "    # UPDATING OLD EXEMPLARS ---------------------------------------------------\n",
        "\n",
        "    if task_num:\n",
        "      # decidi quanti elementi ha ogni exemlar set in a e in b a seconda se m Ã¨ \n",
        "      # pari o dispari\n",
        "      if m%2:\n",
        "        l_a = int((m+1)/2)\n",
        "      else:\n",
        "        l_a = int(m/2)\n",
        "      l_b = int(m-l_a)\n",
        "\n",
        "      # isola gli indici dei vecchi exemplars, dividendoli in due parti\n",
        "      # ogni classe deve avere circa la metÃ  degli exemplar originali\n",
        "      old_exemplar_idxs_a = []\n",
        "      old_exemplar_idxs_b = []\n",
        "      \n",
        "      for idxs in self.exemplar_sets_idxs[:-10]:\n",
        "        old_exemplar_idxs_a += idxs[:l_a]\n",
        "        old_exemplar_idxs_b += idxs[l_a:]\n",
        "\n",
        "      old_mnemonics_data_a = torch.zeros((task_num*10*l_a, 3, 32, 32))\n",
        "      old_mnemonics_data_b = torch.zeros((task_num*10*l_b, 3, 32, 32))\n",
        "\n",
        "      for i, idx in enumerate(old_exemplar_idxs_a):\n",
        "        old_mnemonics_data_a[i, :, :, :] = self.original_training_set.__getitem__(int(idx))[1]\n",
        "      \n",
        "      for i, idx in enumerate(old_exemplar_idxs_b):\n",
        "          old_mnemonics_data_b[i, :, :, :] = self.original_training_set.__getitem__(int(idx))[1]\n",
        "      \n",
        "      old_mnemonics_a = nn.ParameterList()\n",
        "      old_mnemonics_a.append(nn.Parameter(old_mnemonics_data_a))\n",
        "      old_mnemonics_a.to(self.device)\n",
        "      old_mnemonics_b = nn.ParameterList()\n",
        "      old_mnemonics_b.append(nn.Parameter(old_mnemonics_data_b))\n",
        "      old_mnemonics_b.to(self.device)\n",
        "\n",
        "      options_old_a = {'finetuning_idxs':old_exemplar_idxs_a, \n",
        "                       'training_idxs':old_exemplar_idxs_b, \n",
        "                       'mnemonics_to_optimize':old_mnemonics_a, \n",
        "                       'batch_size':128,\n",
        "                       'm': l_a,\n",
        "                       'new':False}\n",
        "\n",
        "      options_old_b = {'finetuning_idxs':old_exemplar_idxs_b, \n",
        "                       'training_idxs':old_exemplar_idxs_a, \n",
        "                       'mnemonics_to_optimize':old_mnemonics_b, \n",
        "                       'batch_size':128,\n",
        "                       'm':l_b,\n",
        "                       'new':False}\n",
        "\n",
        "      self.update_params(**options_old_a) \n",
        "      self.update_params(**options_old_b)\n",
        "\n",
        "      # CONVERT AND STORE UPDATED EXEMPLAR as numpy array\n",
        "\n",
        "      for i, idx in enumerate(old_exemplar_idxs_a):\n",
        "        self.original_exemplar_set.dataset.data[idx] = tensor2im(old_mnemonics_a[0][i])\n",
        "\n",
        "      for i, idx in enumerate(old_exemplar_idxs_b):\n",
        "        self.original_exemplar_set.dataset.data[idx] = tensor2im(old_mnemonics_b[0][i])\n",
        "    \n",
        "\n",
        "\n",
        "    # FINE TUNE THE CURRENT NET ON ALL THE EXEMPLARS COLLECTED 'TILL NOW\n",
        "   \n",
        "  '''\n",
        "\n",
        "  def model_level_optimization(self):\n",
        "    \n",
        "    old_model = copy.deepcopy(self.model)\n",
        "    old_model.eval()\n",
        "    old_model.to(self.device)\n",
        "    n_classes = self.classes_seen+self.CLASSES_PER_BATCH\n",
        "    print(n_classes)\n",
        "    optimizer = optim.SGD(self.model.parameters(), lr=self.lr, momentum=self.momentum, weight_decay=self.weight_decay)\n",
        "    scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=self.milestones, gamma=self.gamma)\n",
        "    for epoch in tqdm(range(self.numepochs)):\n",
        "        \n",
        "      for _, inputs, labels in self.trainloader:\n",
        "        inputs = inputs.float().to(self.device)\n",
        "        labels = torch.tensor([self.diz[c.item()] for c in labels])\n",
        "\n",
        "        labels=labels.to(self.device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs=self.model(inputs)\n",
        "\n",
        "        labels_encoded = F.one_hot(labels,100).float().to(self.device) #CAMBIARE ONE_HOT\n",
        "        \n",
        "        if self.classes_seen:\n",
        "          old_target = old_model(inputs).to(self.device)\n",
        "          old_target = torch.sigmoid(old_target).to(self.device)\n",
        "          \n",
        "          target = torch.cat((old_target[:,:self.classes_seen], labels_encoded[:, self.classes_seen:]), dim=1)\n",
        "          loss = self.criterion(outputs, target)\n",
        "        else:\n",
        "          loss = self.criterion(outputs, labels_encoded) \n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "      \n",
        "      scheduler.step()\n",
        "\n",
        "\n",
        "  def classify_nme(self, input_batch):\n",
        "    min_distances = float('inf')*torch.ones(len(input_batch)).to(self.device) # shape: batch_size --> 128\n",
        "    y_pred = torch.zeros(len(input_batch), dtype=torch.int8).to(self.device) # shape: batch_size --> 128\n",
        "    input_features = self.model.features(input_batch) # shape: (batch_size, feature_size) --> (128, 64)\n",
        "\n",
        "    for i in range(len(self.exemplar_sets_idxs)):\n",
        "      ex_mean = self.exemplar_means[i,:]\n",
        "\n",
        "      # compute distances between inputs features and exemplar set means\n",
        "      pdist = nn.PairwiseDistance(p=2)\n",
        "      distances = pdist(input_features, ex_mean) # shape: batch_size --> 128\n",
        "\n",
        "      # update min distancies and predicted labels\n",
        "      mask = distances < min_distances\n",
        "      min_distances[mask] = distances[mask]\n",
        "      y_pred[mask] = self.exemplar_labels[i]\n",
        "\n",
        "    return y_pred\n",
        "    \n",
        "\n",
        "\n",
        "  def get_new_exemplars(self, batch, m):\n",
        "    loader = torch.utils.data.DataLoader(batch, batch_size=self.batch_size,shuffle=False, num_workers=4)\n",
        "    features = np.zeros((0,self.feature_size))\n",
        "    indices = np.zeros((0), dtype=int)\n",
        "    with torch.no_grad():\n",
        "      for indexes, images, labels in loader:\n",
        "        images = images.cuda()\n",
        "        feature = self.feature_extractor(images).data.cpu().numpy()\n",
        "        feature = normalize(feature, axis=1, norm='l2')\n",
        "        features = np.concatenate((features,feature), axis=0)\n",
        "        indices = np.concatenate((indices,indexes), axis=0)\n",
        "\n",
        "    class_mean = np.mean(features, axis=0)\n",
        "    class_mean = class_mean / np.linalg.norm(class_mean)  # Normalize\n",
        "\n",
        "    #self.cumulative_class_mean.append(class_mean)\n",
        "\n",
        "    exemplar_set = []\n",
        "    exemplar_features = np.zeros((0,self.feature_size))\n",
        "\n",
        "    for k in range(1, int(m)+1):\n",
        "        S = np.sum(exemplar_features, axis=0)\n",
        "        phi = features\n",
        "        mu = class_mean\n",
        "        mu_p = 1.0 / k * (phi + S)\n",
        "        mu_p = normalize(mu_p, axis=1, norm='l2')\n",
        "        i = np.argmin(np.sqrt(np.sum((mu - mu_p) ** 2, axis=1)))\n",
        "        exemplar_set.append(int(indices[i]))\n",
        "        addfeature =  np.expand_dims(features[i], axis=0)\n",
        "        exemplar_features = np.concatenate((exemplar_features,addfeature), axis=0)\n",
        "\n",
        "        #remove duplicates\n",
        "        features = np.delete(features, i, 0)\n",
        "        indices = np.delete(indices, i, 0)\n",
        "        \n",
        "    self.exemplar_sets_idxs.append(exemplar_set)\n",
        "    #self.exemplar_sets_idxs.append(random.sample(list(batch), m))\n",
        "\n",
        "\n",
        "  def reduce_old_exemplars(self, m):\n",
        "    for i, set_i in enumerate(self.exemplar_sets_idxs):\n",
        "      self.exemplar_sets_idxs[i] = random.sample(set_i, m)\n",
        "\n",
        "  def __accuracy_fc(self, dl, mapper):\n",
        "    total = 0.0\n",
        "    correct = 0.0\n",
        "    for  _, images, labels in dl:\n",
        "      labels = torch.tensor([torch.tensor(mapper[c.item()]) for c in labels])\n",
        "      labels = labels.to(self.device)\n",
        "      images = images.to(self.device)\n",
        "      outputs = self.model(images)\n",
        "      _, preds = torch.max(outputs, dim=1)\n",
        "      total += len(labels)\n",
        "      correct += torch.sum(preds == labels).item()\n",
        "\n",
        "    acc = correct / total\n",
        "    return acc\n",
        "\n",
        "\n",
        "  def __accuracy_nme(self, dl):\n",
        "    \n",
        "    total = 0.0\n",
        "    correct = 0.0\n",
        "    \n",
        "    for  _, images, labels in dl:\n",
        "      labels = labels.to(self.device)\n",
        "      images = images.to(self.device)\n",
        "      preds = self.classify_nme(images)\n",
        "      total += len(labels)\n",
        "      correct += torch.sum(preds == labels).item()\n",
        "\n",
        "      if self.last_test:\n",
        "        self.y_pred += preds.tolist()\n",
        "        self.y_test += labels.tolist()\n",
        "\n",
        "    acc = correct / total\n",
        "    return acc\n",
        "\n",
        "\n",
        "  def plot_confusion_matrix(self):\n",
        " \n",
        "    cm = confusion_matrix(self.y_test, self.y_pred)\n",
        "    cm = np.log(cm+1)\n",
        "    fig, ax = plt.subplots(figsize=(7,7))\n",
        "    sns.heatmap(cm, square=True, cbar=False, ax=ax, cmap=plt.get_cmap('seismic'))\n",
        "    ax.set_xticks(np.linspace(19,99,5))\n",
        "    ax.set_yticks(np.linspace(19,99,5))\n",
        "    ax.set_xticklabels([20,40,60,80,100], rotation=0)\n",
        "    ax.set_yticklabels([20,40,60,80,100], rotation=0)\n",
        "    ax.set_title(\"iCaRL\")\n",
        "    ax.set_xlabel(\"Predicted class\")\n",
        "    ax.set_ylabel(\"True class\")\n",
        "    plt.savefig(\"iCaRL_\"+str(self.randomseed)+\"_cm.png\")\n",
        "    plt.show()\n",
        "    return cm\n",
        "\n",
        "  def plot_data(self, train_dl):\n",
        "\n",
        "    from sklearn.manifold import TSNE\n",
        "    print('------plot data------')\n",
        "\n",
        "    #Data points\n",
        "    train_labels_array = torch.zeros(0).to('cuda')\n",
        "    train_dataset_to_reduce = np.zeros((0,64), dtype = float)\n",
        "\n",
        "    for  _, images, labels in train_dl:\n",
        "      labels = labels.to(self.device)\n",
        "      images = images.to(self.device)\n",
        "      train_dataset_to_reduce = np.concatenate((train_dataset_to_reduce, self.feature_extractor(images).cpu().detach().numpy()))\n",
        "      train_labels_array = torch.cat((train_labels_array, labels))\n",
        "\n",
        "    \n",
        "    #EX e MN loaders \n",
        "    current_exemplar_indices = np.array([], dtype=int)\n",
        "\n",
        "    for exemplar_set in self.exemplar_sets_idxs:\n",
        "      current_exemplar_indices = np.concatenate([current_exemplar_indices, np.array(exemplar_set)])\n",
        "\n",
        "    exemplar_dataset = Subset(self.original_training_set, current_exemplar_indices)\n",
        "    ex_dl = DataLoader(exemplar_dataset, batch_size=self.batch_size, shuffle=False, num_workers=4, drop_last=True) #usato solo per plottare\n",
        "\n",
        "    mn_dataset = Subset(self.original_exemplar_set, current_exemplar_indices)\n",
        "    mn_dl = DataLoader(mn_dataset, batch_size=self.batch_size, shuffle=False, num_workers=4, drop_last=True) #usato solo per plottare\n",
        "\n",
        "\n",
        "    #Exemplars\n",
        "\n",
        "    ex_labels_array = torch.zeros(0).to('cuda')\n",
        "    ex_dataset_to_reduce = np.zeros((0,64), dtype = float)\n",
        "\n",
        "    for  _, images, labels in ex_dl:\n",
        "      labels = labels.to(self.device)\n",
        "      images = images.to(self.device)\n",
        "      ex_dataset_to_reduce = np.concatenate((ex_dataset_to_reduce, self.feature_extractor(images).cpu().detach().numpy()))\n",
        "      ex_labels_array = torch.cat((ex_labels_array, labels), dim = 0)\n",
        "\n",
        "\n",
        "    #Mnemonics\n",
        "\n",
        "\n",
        "    mn_labels_array = torch.zeros(0).to('cuda')\n",
        "    mn_dataset_to_reduce = np.zeros((0,64), dtype = float)\n",
        "\n",
        "    for  _, images, labels in mn_dl:\n",
        "      labels = labels.to(self.device)\n",
        "      images = images.to(self.device)\n",
        "      mn_dataset_to_reduce = np.concatenate((mn_dataset_to_reduce, self.feature_extractor(images).cpu().detach().numpy()))\n",
        "      mn_labels_array = torch.cat((mn_labels_array, labels), dim = 0)\n",
        "\n",
        "    #PLOT'''\n",
        "    total_data_w_exemplars = np.concatenate((train_dataset_to_reduce, ex_dataset_to_reduce))\n",
        "    total_data_w_mn =  np.concatenate((train_dataset_to_reduce, mn_dataset_to_reduce))\n",
        "\n",
        "    total_transformed_ex = TSNE(n_components=2).fit_transform(total_data_w_exemplars)\n",
        "    X_transformed_w_ex = total_transformed_ex[:train_dataset_to_reduce.shape[0]]\n",
        "    ex_transformed = total_transformed_ex[train_dataset_to_reduce.shape[0]:]\n",
        "\n",
        "    total_transformed_mn = TSNE(n_components=2).fit_transform(total_data_w_mn)\n",
        "    X_transformed_w_mn = total_transformed_mn[:train_dataset_to_reduce.shape[0]]\n",
        "    mn_transformed = total_transformed_mn[train_dataset_to_reduce.shape[0]:]\n",
        "\n",
        "    fig, (ax1, ax2) = plt.subplots(1,2, figsize=(24,12))\n",
        "    ax1.scatter(X_transformed_w_ex[:,0], X_transformed_w_ex[:,1], c = train_labels_array.cpu(), alpha = 0.1)\n",
        "    ax1.scatter(ex_transformed[:,0], ex_transformed[:,1], c = ex_labels_array.cpu(), alpha = 1)\n",
        "    #ax1.title('EXEMPLARS')\n",
        "\n",
        "    ax2.scatter(X_transformed_w_mn[:,0], X_transformed_w_mn[:,1], c = train_labels_array.cpu(), alpha = 0.1)\n",
        "    ax2.scatter(mn_transformed[:,0], mn_transformed[:,1], c = mn_labels_array.cpu(), alpha = 1)\n",
        "    #ax2.title('MNEMONICS')\n",
        "    plt.show()\n",
        "\n",
        "  def trainer(self):\n",
        "    \n",
        "    train_indices = self.original_training_set.get_batch_indexes()\n",
        "    test_indices = self.original_test_set.get_batch_indexes()\n",
        "    batches=self.original_training_set.getbatches()\n",
        "    current_test_indexes=[]\n",
        "    test_acc = []\n",
        "    self.last_test = False\n",
        "\n",
        "    for i in range(self.NUM_BATCHES):\n",
        "      print('current batches', batches[i])\n",
        "      if i == self.NUM_BATCHES-1:\n",
        "        self.last_test = True\n",
        "\n",
        "      current_exemplar_indices = np.array([], dtype=int)\n",
        "    \n",
        "      for exemplar_set in self.exemplar_sets_idxs:\n",
        "        current_exemplar_indices = np.concatenate([current_exemplar_indices, np.array(exemplar_set)])\n",
        "\n",
        "      exemplar_dataset = Subset(self.original_training_set, current_exemplar_indices) \n",
        "      #DA CAMBIARE CON SELF.ORIGINAL EXEMPLAR SET\n",
        "      if i > 1: #FINETUNING\n",
        "        print('----inizio finetuning----')\n",
        "        print('numbero of classes in the exemplar sets', len(self.exemplar_sets_idxs))\n",
        "        self.numepochs = 10\n",
        "        self.lr = 0.2\n",
        "        temporary_classes_seen = self.classes_seen\n",
        "        self.classes_seen = 0\n",
        "        self.trainloader = DataLoader(exemplar_dataset, batch_size=self.batch_size, shuffle=False, num_workers=4, drop_last=True) \n",
        "        print('accuracy on exemplar set before finetuining:', 100*self.__accuracy_fc(self.trainloader,self.diz))\n",
        "        self.model.train()\n",
        "        self.model_level_optimization()\n",
        "        #BACK TO THE NORMAL PARAMETERS\n",
        "        self.model.eval()\n",
        "        self.numepochs = 70\n",
        "        self.lr = 2\n",
        "        self.classes_seen = temporary_classes_seen\n",
        "        print('accuracy on exemplar set after finetuining:', 100*self.__accuracy_fc(self.trainloader,self.diz))\n",
        "        current_test_acc = self.__accuracy_nme(self.testloader)\n",
        "        print('accuracy on test set after finetuning:', 100*current_test_acc)\n",
        "        print('-----fine finetuning------')\n",
        "        print('-'*80)\n",
        "\n",
        "      train_dataset = Subset(self.original_training_set, train_indices[i])\n",
        "      current_test_indexes += test_indices[i].tolist()\n",
        "      test_dataset = Subset(self.original_test_set,current_test_indexes)\n",
        "      self.train_loader = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True, num_workers=4, drop_last=True)\n",
        "      self.testloader = DataLoader(test_dataset, batch_size=self.batch_size, shuffle=False, num_workers=4, drop_last=True)        \n",
        "      \n",
        "\n",
        "      if i == 0:\n",
        "        self.trainloader = self.train_loader\n",
        "      else:\n",
        "        self.trainloader = DataLoader(torch.utils.data.ConcatDataset([train_dataset, exemplar_dataset]), batch_size=self.batch_size, shuffle=True,\n",
        "          num_workers=4, pin_memory=True)\n",
        "\n",
        "        \n",
        "      self.model.train()\n",
        "      self.model_level_optimization()    \n",
        "      self.classes_seen += 10\n",
        "      self.model.eval() # Set Network to evaluation mode\n",
        "      m=int(2000/(int(i*10+10)))\n",
        "\n",
        "      \n",
        "      current_test_acc = self.__accuracy_fc(self.testloader, self.diz)\n",
        "      print('accuracy on test set:', 100*current_test_acc)\n",
        "      break\n",
        "      \n",
        "      return self.model, batches[i]\n",
        "'''\n",
        "      #NUOVO PAPER DEL PORCODDIO\n",
        "      labels_of_modified = torch.zeros(0, dtype = int).to('cuda')\n",
        "      for label in batches[i]:\n",
        "        labels = torch.LongTensor([self.diz[label]]*m).to('cuda')\n",
        "        labels_of_modified = torch.cat((labels_of_modified, labels), dim=0)\n",
        "      print('labels to be created', labels_of_modified)\n",
        "      print('len to be created', len(labels_of_modified))\n",
        "      number_of_images_created = m*10\n",
        "      net_student = resnet32(num_classes=100).to(self.device)\n",
        "      data_type = torch.float\n",
        "      inputs = torch.randn((number_of_images_created, 3, 32, 32), requires_grad=True, device='cuda', dtype=data_type)\n",
        "\n",
        "      net_student = copy.deepcopy(self.model)\n",
        "      net_student.eval() #important, otherwise generated images will be non natural\n",
        "      \n",
        "      train_writer = None  # tensorboard writter\n",
        "      global_iteration = 0\n",
        "      di_lr = 0.05\n",
        "      optimizer_di = optim.Adam([inputs], lr=di_lr)\n",
        "\n",
        "      print(\"Starting model inversion\")\n",
        "      batch_idx = 0\n",
        "      inputs = get_images(net=self.model, bs=200, epochs=2000, idx=batch_idx,\n",
        "                        net_student=net_student,\n",
        "                        train_writer=train_writer, use_amp=False,\n",
        "                        optimizer=optimizer_di, inputs=inputs, \n",
        "                        var_scale=0.00005, labels=labels)\n",
        "\n",
        "\n",
        "      plt.imshow(tensor2im(inputs[0]))\n",
        "      plt.show()\n",
        "      plt.imshow(tensor2im(inputs[55]))\n",
        "      plt.show()\n",
        "      print('deepinversion finshed')\n",
        "      # update exemplars number\n",
        "      \n",
        "\n",
        "      # reduce the number of each exemplars set\n",
        "      self.reduce_old_exemplars(m) \n",
        "\n",
        "      self.cumulative_class_mean = {}\n",
        "\n",
        "      for classlabel in batches[i]:\n",
        "        indexes_class = self.original_training_set.get_class_indexes(classlabel)\n",
        "        current_class = Subset(self.original_training_set, indexes_class)\n",
        "        #self.get_new_exemplars(indexes_class, m)\n",
        "        self.get_new_exemplars(current_class, m)\n",
        "        indexes_class = self.original_training_set.get_class_indexes(classlabel)\n",
        "        current_class = Subset(self.original_training_set, indexes_class)\n",
        "\n",
        "        loader = torch.utils.data.DataLoader(current_class, batch_size=self.batch_size,shuffle=False, num_workers=4)\n",
        "        features = np.zeros((0, 64))\n",
        "        with torch.no_grad():\n",
        "          for indexes, images, labels in loader:\n",
        "            images = images.cuda()\n",
        "            feature = self.feature_extractor(images).data.cpu().numpy()\n",
        "            feature = normalize(feature, axis=1, norm='l2')\n",
        "            features = np.concatenate((features,feature), axis=0)\n",
        "\n",
        "        class_mean = np.mean(features, axis=0)\n",
        "        class_mean = class_mean / np.linalg.norm(class_mean)  # Normalize\n",
        "\n",
        "        self.cumulative_class_mean[classlabel] = class_mean\n",
        "        \n",
        "      #plt.imshow(self.original_exemplar_set.dataset.data[self.exemplar_sets_idxs[0][0]])\n",
        "      #plt.show()\n",
        "      \n",
        "  \n",
        "      #self.exemplar_level_optimization(m, i, self.original_training_set.get_batch_indexes()[i])\n",
        "\n",
        "      # compute means of exemplar set\n",
        "      # cycle for each exemplar set\n",
        "      self.exemplar_means = torch.zeros((0, self.feature_size), dtype=torch.float).to(self.device)\n",
        "      self.exemplar_labels = []\n",
        "      for j in range(len(self.exemplar_sets_idxs)):\n",
        "        exemplars_dataset = Subset(self.original_training_set, self.exemplar_sets_idxs[j])\n",
        "        exemplars_loader = torch.utils.data.DataLoader(exemplars_dataset, batch_size=self.batch_size, shuffle=False, num_workers=4)\n",
        "        ex_features = torch.zeros((0, self.feature_size), dtype=torch.float).to(self.device) # alla fine shape: (len(exemplar_set), feature_size) --> (m, 64)\n",
        "      \n",
        "        with torch.no_grad():\n",
        "          _, _, exemplar_label = self.original_training_set.__getitem__(self.exemplar_sets_idxs[j][0]) \n",
        "          self.exemplar_labels.append(exemplar_label)\n",
        "          # cycle for each batch in the current exemplar set\n",
        "          for _,  exemplars, _ in exemplars_loader:\n",
        "          \n",
        "            # get exemplars features\n",
        "            exemplars = exemplars.to(self.device)\n",
        "            features = self.model.features(exemplars) # shape: (len(exemplars), feature_size)\n",
        "          \n",
        "            # normalize \n",
        "            feature_norms = torch.norm(features, p=2, dim=1) # shape: len(exemplars)\n",
        "            feature_norms.unsqueeze_(1) # shape: (len(exemplars), 1)\n",
        "            features = features/feature_norms\n",
        "          \n",
        "            # concatenate over columns\n",
        "            ex_features = torch.cat((ex_features, features), dim=0)\n",
        "          \n",
        "        # compute current exemplar set mean and normalize it\n",
        "        ex_mean = torch.mean(ex_features, dim=0) # shape: feature_size --> 64\n",
        "        ex_mean = ex_mean/torch.norm(ex_mean)\n",
        "        ex_mean.unsqueeze_(0) # shape: (1, feature_size) --> (1, 64)\n",
        "        self.exemplar_means = torch.cat((self.exemplar_means, ex_mean), dim=0) # shape: (n_examplar sets, feature size)\n",
        "      \n",
        "      #if i == 0:\n",
        "       # self.plot_data(self.trainloader)\n",
        "      print('accuracy on training set:', 100*self.__accuracy_fc(self.trainloader,self.diz))\n",
        "      # print('accuracy on test set:', self.__accuracy_on(self.testloader,self,self.diz))\n",
        "      current_test_acc = self.__accuracy_nme(self.testloader)\n",
        "      print('accuracy on test set:', 100*current_test_acc)\n",
        "      print('-' * 80)\n",
        "      test_acc.append(current_test_acc)\n",
        "\n",
        "    # compute comfusion matrix and save results\n",
        "    cm = self.plot_confusion_matrix()\n",
        "    with open('iCaRL_'+str(self.randomseed)+\"_cm\", 'wb') as file:\n",
        "      pickle.dump(cm, file, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "    with open('iCaRL_'+str(self.randomseed)+\"_testacc\", 'wb') as file:\n",
        "      pickle.dump(test_acc, file, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "      print('PRINT IMAGES')\n",
        "      print('with data augmentation')\n",
        "      plt.imshow(self.original_training_set.dataset.data[self.exemplar_sets_idxs[0][0]])\n",
        "      plt.show()\n",
        "      print('without data augmentation')\n",
        "      plt.imshow(self.original_exemplar_set.dataset.data[self.exemplar_sets_idxs[0][0]])\n",
        "      plt.show()\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n      #NUOVO PAPER DEL PORCODDIO\\n      labels_of_modified = torch.zeros(0, dtype = int).to(\\'cuda\\')\\n      for label in batches[i]:\\n        labels = torch.LongTensor([self.diz[label]]*m).to(\\'cuda\\')\\n        labels_of_modified = torch.cat((labels_of_modified, labels), dim=0)\\n      print(\\'labels to be created\\', labels_of_modified)\\n      print(\\'len to be created\\', len(labels_of_modified))\\n      number_of_images_created = m*10\\n      net_student = resnet32(num_classes=100).to(self.device)\\n      data_type = torch.float\\n      inputs = torch.randn((number_of_images_created, 3, 32, 32), requires_grad=True, device=\\'cuda\\', dtype=data_type)\\n\\n      net_student = copy.deepcopy(self.model)\\n      net_student.eval() #important, otherwise generated images will be non natural\\n      \\n      train_writer = None  # tensorboard writter\\n      global_iteration = 0\\n      di_lr = 0.05\\n      optimizer_di = optim.Adam([inputs], lr=di_lr)\\n\\n      print(\"Starting model inversion\")\\n      batch_idx = 0\\n      inputs = get_images(net=self.model, bs=200, epochs=2000, idx=batch_idx,\\n                        net_student=net_student,\\n                        train_writer=train_writer, use_amp=False,\\n                        optimizer=optimizer_di, inputs=inputs, \\n                        var_scale=0.00005, labels=labels)\\n\\n\\n      plt.imshow(tensor2im(inputs[0]))\\n      plt.show()\\n      plt.imshow(tensor2im(inputs[55]))\\n      plt.show()\\n      print(\\'deepinversion finshed\\')\\n      # update exemplars number\\n      \\n\\n      # reduce the number of each exemplars set\\n      self.reduce_old_exemplars(m) \\n\\n      self.cumulative_class_mean = {}\\n\\n      for classlabel in batches[i]:\\n        indexes_class = self.original_training_set.get_class_indexes(classlabel)\\n        current_class = Subset(self.original_training_set, indexes_class)\\n        #self.get_new_exemplars(indexes_class, m)\\n        self.get_new_exemplars(current_class, m)\\n        indexes_class = self.original_training_set.get_class_indexes(classlabel)\\n        current_class = Subset(self.original_training_set, indexes_class)\\n\\n        loader = torch.utils.data.DataLoader(current_class, batch_size=self.batch_size,shuffle=False, num_workers=4)\\n        features = np.zeros((0, 64))\\n        with torch.no_grad():\\n          for indexes, images, labels in loader:\\n            images = images.cuda()\\n            feature = self.feature_extractor(images).data.cpu().numpy()\\n            feature = normalize(feature, axis=1, norm=\\'l2\\')\\n            features = np.concatenate((features,feature), axis=0)\\n\\n        class_mean = np.mean(features, axis=0)\\n        class_mean = class_mean / np.linalg.norm(class_mean)  # Normalize\\n\\n        self.cumulative_class_mean[classlabel] = class_mean\\n        \\n      #plt.imshow(self.original_exemplar_set.dataset.data[self.exemplar_sets_idxs[0][0]])\\n      #plt.show()\\n      \\n  \\n      #self.exemplar_level_optimization(m, i, self.original_training_set.get_batch_indexes()[i])\\n\\n      # compute means of exemplar set\\n      # cycle for each exemplar set\\n      self.exemplar_means = torch.zeros((0, self.feature_size), dtype=torch.float).to(self.device)\\n      self.exemplar_labels = []\\n      for j in range(len(self.exemplar_sets_idxs)):\\n        exemplars_dataset = Subset(self.original_training_set, self.exemplar_sets_idxs[j])\\n        exemplars_loader = torch.utils.data.DataLoader(exemplars_dataset, batch_size=self.batch_size, shuffle=False, num_workers=4)\\n        ex_features = torch.zeros((0, self.feature_size), dtype=torch.float).to(self.device) # alla fine shape: (len(exemplar_set), feature_size) --> (m, 64)\\n      \\n        with torch.no_grad():\\n          _, _, exemplar_label = self.original_training_set.__getitem__(self.exemplar_sets_idxs[j][0]) \\n          self.exemplar_labels.append(exemplar_label)\\n          # cycle for each batch in the current exemplar set\\n          for _,  exemplars, _ in exemplars_loader:\\n          \\n            # get exemplars features\\n            exemplars = exemplars.to(self.device)\\n            features = self.model.features(exemplars) # shape: (len(exemplars), feature_size)\\n          \\n            # normalize \\n            feature_norms = torch.norm(features, p=2, dim=1) # shape: len(exemplars)\\n            feature_norms.unsqueeze_(1) # shape: (len(exemplars), 1)\\n            features = features/feature_norms\\n          \\n            # concatenate over columns\\n            ex_features = torch.cat((ex_features, features), dim=0)\\n          \\n        # compute current exemplar set mean and normalize it\\n        ex_mean = torch.mean(ex_features, dim=0) # shape: feature_size --> 64\\n        ex_mean = ex_mean/torch.norm(ex_mean)\\n        ex_mean.unsqueeze_(0) # shape: (1, feature_size) --> (1, 64)\\n        self.exemplar_means = torch.cat((self.exemplar_means, ex_mean), dim=0) # shape: (n_examplar sets, feature size)\\n      \\n      #if i == 0:\\n       # self.plot_data(self.trainloader)\\n      print(\\'accuracy on training set:\\', 100*self.__accuracy_fc(self.trainloader,self.diz))\\n      # print(\\'accuracy on test set:\\', self.__accuracy_on(self.testloader,self,self.diz))\\n      current_test_acc = self.__accuracy_nme(self.testloader)\\n      print(\\'accuracy on test set:\\', 100*current_test_acc)\\n      print(\\'-\\' * 80)\\n      test_acc.append(current_test_acc)\\n\\n    # compute comfusion matrix and save results\\n    cm = self.plot_confusion_matrix()\\n    with open(\\'iCaRL_\\'+str(self.randomseed)+\"_cm\", \\'wb\\') as file:\\n      pickle.dump(cm, file, protocol=pickle.HIGHEST_PROTOCOL)\\n    with open(\\'iCaRL_\\'+str(self.randomseed)+\"_testacc\", \\'wb\\') as file:\\n      pickle.dump(test_acc, file, protocol=pickle.HIGHEST_PROTOCOL)\\n\\n      print(\\'PRINT IMAGES\\')\\n      print(\\'with data augmentation\\')\\n      plt.imshow(self.original_training_set.dataset.data[self.exemplar_sets_idxs[0][0]])\\n      plt.show()\\n      print(\\'without data augmentation\\')\\n      plt.imshow(self.original_exemplar_set.dataset.data[self.exemplar_sets_idxs[0][0]])\\n      plt.show()\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HzqwQeHB1Tg-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc5e0f26-5c93-435a-9c2f-92008eb47d1d"
      },
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "from __future__ import unicode_literals\n",
        "\n",
        "import argparse\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "# import torch.nn.parallel\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.optim as optim\n",
        "# import torch.utils.data\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.utils as vutils\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import glob\n",
        "import collections\n",
        "\n",
        "#from resnet_cifar import ResNet34, ResNet18\n",
        "\n",
        "try:\n",
        "    from apex.parallel import DistributedDataParallel as DDP\n",
        "    from apex import amp, optimizers\n",
        "    USE_APEX = True\n",
        "except ImportError:\n",
        "    print(\"Please install apex from https://www.github.com/nvidia/apex to run this example.\")\n",
        "    print(\"will attempt to run without it\")\n",
        "    USE_APEX = False\n",
        "\n",
        "#provide intermeiate information\n",
        "debug_output = False\n",
        "debug_output = True\n",
        "\n",
        "\n",
        "class DeepInversionFeatureHook():\n",
        "    '''\n",
        "    Implementation of the forward hook to track feature statistics and compute a loss on them.\n",
        "    Will compute mean and variance, and will use l2 as a loss\n",
        "    '''\n",
        "\n",
        "    def __init__(self, module):\n",
        "        self.hook = module.register_forward_hook(self.hook_fn)\n",
        "\n",
        "    def hook_fn(self, module, input, output):\n",
        "        # hook co compute deepinversion's feature distribution regularization\n",
        "        nch = input[0].shape[1]\n",
        "\n",
        "        mean = input[0].mean([0, 2, 3])\n",
        "        var = input[0].permute(1, 0, 2, 3).contiguous().view([nch, -1]).var(1, unbiased=False)\n",
        "\n",
        "        # forcing mean and variance to match between two distributions\n",
        "        # other ways might work better, e.g. KL divergence\n",
        "        r_feature = torch.norm(module.running_var.data.type(var.type()) - var, 2) + torch.norm(\n",
        "            module.running_mean.data.type(var.type()) - mean, 2)\n",
        "\n",
        "        self.r_feature = r_feature\n",
        "        # must have no output\n",
        "\n",
        "    def close(self):\n",
        "        self.hook.remove()\n",
        "\n",
        "def get_images(net, bs=256, epochs=1000, idx=-1, var_scale=0.00005, competitive_scale=0.01,\n",
        "               net_student=None, prefix=None, train_writer = None, global_iteration=None,\n",
        "               use_amp=False, bn_reg_scale = 0.0,\n",
        "               optimizer = None, inputs = None, labels = False, l2_coeff=0.0):\n",
        "    '''\n",
        "    Function returns inverted images from the pretrained model, parameters are tight to CIFAR dataset\n",
        "    args in:\n",
        "        net: network to be inverted\n",
        "        bs: batch size\n",
        "        epochs: total number of iterations to generate inverted images, training longer helps a lot!\n",
        "        idx: an external flag for printing purposes: only print in the first round, set as -1 to disable\n",
        "        var_scale: the scaling factor for variance loss regularization. this may vary depending on bs\n",
        "            larger - more blurred but less noise\n",
        "        net_student: model to be used for Adaptive DeepInversion\n",
        "        prefix: defines the path to store images\n",
        "        competitive_scale: coefficient for Adaptive DeepInversion\n",
        "        train_writer: tensorboardX object to store intermediate losses\n",
        "        global_iteration: indexer to be used for tensorboard\n",
        "        use_amp: boolean to indicate usage of APEX AMP for FP16 calculations - twice faster and less memory on TensorCores\n",
        "        optimizer: potimizer to be used for model inversion\n",
        "        inputs: data place holder for optimization, will be reinitialized to noise\n",
        "        bn_reg_scale: weight for r_feature_regularization\n",
        "        random_labels: sample labels from random distribution or use columns of the same class\n",
        "        l2_coeff: coefficient for L2 loss on input\n",
        "    return:\n",
        "        A tensor on GPU with shape (bs, 3, 32, 32) for CIFAR\n",
        "    '''\n",
        "\n",
        "    kl_loss = nn.KLDivLoss(reduction='batchmean').cuda()\n",
        "\n",
        "    # preventing backpropagation through student for Adaptive DeepInversion\n",
        "    net_student.eval()\n",
        "\n",
        "    best_cost = 1e6\n",
        "\n",
        "    # initialize gaussian inputs\n",
        "    inputs.data = torch.randn((bs, 3, 32, 32), requires_grad=True, device='cuda')\n",
        "    # if use_amp:\n",
        "    #     inputs.data = inputs.data.half()\n",
        "\n",
        "    # set up criteria for optimization\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    optimizer.state = collections.defaultdict(dict)  # Reset state of optimizer\n",
        "\n",
        "    # target outputs to generate\n",
        "    #if labels:\n",
        "    targets = labels\n",
        "    #else:\n",
        "     #   targets = torch.LongTensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9] * 25 + [0, 1, 2, 3, 4, 5]).to('cuda')\n",
        "\n",
        "    outputs=net(inputs.data)\n",
        "    _, predicted_teach = outputs.max(1)\n",
        "\n",
        "    outputs_student=net_student(inputs.data)\n",
        "    _, predicted_std = outputs_student.max(1)\n",
        "\n",
        "    if idx == 0:\n",
        "        print('Teacher correct out of {}: {}, loss at {}'.format(bs, predicted_teach.eq(targets).sum().item(), criterion(outputs, targets).item()))\n",
        "        print('Student correct out of {}: {}, loss at {}'.format(bs, predicted_std.eq(targets).sum().item(), criterion(outputs_student, targets).item()))\n",
        "\n",
        "    ## Create hooks for feature statistics catching\n",
        "    loss_r_feature_layers = []\n",
        "    for module in net.modules():\n",
        "        if isinstance(module, nn.BatchNorm2d):\n",
        "            loss_r_feature_layers.append(DeepInversionFeatureHook(module))\n",
        "\n",
        "    # setting up the range for jitter\n",
        "    lim_0, lim_1 = 2, 2\n",
        "\n",
        "    for epoch in tqdm(range(epochs)):\n",
        "        # apply random jitter offsets\n",
        "        off1 = random.randint(-lim_0, lim_0)\n",
        "        off2 = random.randint(-lim_1, lim_1)\n",
        "        inputs_jit = torch.roll(inputs, shifts=(off1,off2), dims=(2,3))\n",
        "\n",
        "        # foward with jit images\n",
        "        optimizer.zero_grad()\n",
        "        net.zero_grad()\n",
        "        outputs = net(inputs_jit)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss_target = loss.item()\n",
        "\n",
        "        # competition loss, Adaptive DeepInvesrion\n",
        "        if competitive_scale != 0.0:\n",
        "            net_student.zero_grad()\n",
        "            outputs_student = net_student(inputs_jit)\n",
        "            T = 3.0\n",
        "\n",
        "            if 1:\n",
        "                # jensen shanon divergence:\n",
        "                # another way to force KL between negative probabilities\n",
        "                P = F.softmax(outputs_student / T, dim=1)\n",
        "                Q = F.softmax(outputs / T, dim=1)\n",
        "                M = 0.5 * (P + Q)\n",
        "\n",
        "                P = torch.clamp(P, 0.01, 0.99)\n",
        "                Q = torch.clamp(Q, 0.01, 0.99)\n",
        "                M = torch.clamp(M, 0.01, 0.99)\n",
        "                eps = 0.0\n",
        "                # loss_verifier_cig = 0.5 * kl_loss(F.log_softmax(outputs_verifier / T, dim=1), M) +  0.5 * kl_loss(F.log_softmax(outputs/T, dim=1), M)\n",
        "                loss_verifier_cig = 0.5 * kl_loss(torch.log(P + eps), M) + 0.5 * kl_loss(torch.log(Q + eps), M)\n",
        "                # JS criteria - 0 means full correlation, 1 - means completely different\n",
        "                loss_verifier_cig = 1.0 - torch.clamp(loss_verifier_cig, 0.0, 1.0)\n",
        "\n",
        "                loss = loss + competitive_scale * loss_verifier_cig\n",
        "\n",
        "        # apply total variation regularization\n",
        "        diff1 = inputs_jit[:,:,:,:-1] - inputs_jit[:,:,:,1:]\n",
        "        diff2 = inputs_jit[:,:,:-1,:] - inputs_jit[:,:,1:,:]\n",
        "        diff3 = inputs_jit[:,:,1:,:-1] - inputs_jit[:,:,:-1,1:]\n",
        "        diff4 = inputs_jit[:,:,:-1,:-1] - inputs_jit[:,:,1:,1:]\n",
        "        loss_var = torch.norm(diff1) + torch.norm(diff2) + torch.norm(diff3) + torch.norm(diff4)\n",
        "        loss = loss + var_scale*loss_var\n",
        "\n",
        "        # R_feature loss\n",
        "        loss_distr = sum([mod.r_feature for mod in loss_r_feature_layers])\n",
        "        loss = loss + bn_reg_scale*loss_distr # best for noise before BN\n",
        "\n",
        "        # l2 loss\n",
        "        if 1:\n",
        "            loss = loss + l2_coeff * torch.norm(inputs_jit, 2)\n",
        "\n",
        "        if debug_output and epoch % 200==0:\n",
        "            print(f\"It {epoch}\\t Losses: total: {loss.item():3.3f},\\ttarget: {loss_target:3.3f} \\tR_feature_loss unscaled:\\t {loss_distr.item():3.3f}\")\n",
        "            #vutils.save_image(inputs.data.clone(),\n",
        "             #                 './{}/output_{}.png'.format(prefix, epoch//200),\n",
        "              #                normalize=True, scale_each=True, nrow=10)\n",
        "\n",
        "        if best_cost > loss.item():\n",
        "            best_cost = loss.item()\n",
        "            best_inputs = inputs.data\n",
        "\n",
        "        # backward pass\n",
        "        if use_amp:\n",
        "            with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
        "                scaled_loss.backward()\n",
        "        else:\n",
        "            loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "    outputs=net(best_inputs)\n",
        "    _, predicted_teach = outputs.max(1)\n",
        "\n",
        "    outputs_student=net_student(best_inputs)\n",
        "    _, predicted_std = outputs_student.max(1)\n",
        "\n",
        "    if idx == 0:\n",
        "        print('Teacher correct out of {}: {}, loss at {}'.format(bs, predicted_teach.eq(targets).sum().item(), criterion(outputs, targets).item()))\n",
        "        print('Student correct out of {}: {}, loss at {}'.format(bs, predicted_std.eq(targets).sum().item(), criterion(outputs_student, targets).item()))\n",
        "\n",
        "    name_use = \"best_images\"\n",
        "    if prefix is not None:\n",
        "        name_use = prefix + name_use\n",
        "    next_batch = len(glob.glob(\"./%s/*.png\" % name_use)) // 1\n",
        "\n",
        "    #vutils.save_image(best_inputs[:20].clone(),\n",
        "     #                 './{}/output_{}.png'.format(name_use, next_batch),\n",
        "      #                normalize=True, scale_each = True, nrow=10)\n",
        "\n",
        "    #if train_writer is not None:\n",
        "     #   train_writer.add_scalar('gener_teacher_criteria', criterion(outputs, targets), global_iteration)\n",
        "      #  train_writer.add_scalar('gener_student_criteria', criterion(outputs_student, targets), global_iteration)\n",
        "\n",
        "       # train_writer.add_scalar('gener_teacher_acc', predicted_teach.eq(targets).sum().item() / bs, global_iteration)\n",
        "       # train_writer.add_scalar('gener_student_acc', predicted_std.eq(targets).sum().item() / bs, global_iteration)\n",
        "\n",
        "        #train_writer.add_scalar('gener_loss_total', loss.item(), global_iteration)\n",
        "        #train_writer.add_scalar('gener_loss_var', (var_scale*loss_var).item(), global_iteration)\n",
        "\n",
        "    net_student.train()\n",
        "\n",
        "    return best_inputs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Please install apex from https://www.github.com/nvidia/apex to run this example.\n",
            "will attempt to run without it\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 148,
          "referenced_widgets": [
            "8a9b660641d54c47bbcfe4e7b7e05559",
            "231c89fb8ad74b6e9ee17ed065bcc49b",
            "422a21bc6637418697da730a562664e0",
            "ebbacd8f79ca463b8cc04863a08b71f6",
            "fb42262e8e7d4063ad7f18fcedc53491",
            "4801bc6e119044e7896ac6f04c8f4eee",
            "bbdbceab2ecc4e0492387fbec20a2779",
            "4240c00beb144caba4d19f3ff0f98398"
          ]
        },
        "id": "OYzLuYGDLr15",
        "outputId": "1f762651-1ee0-4756-9fa7-043cd3864498"
      },
      "source": [
        "method = mnemonics(randomseed=203)\n",
        "model, batch = method.trainer()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "current batches [11, 5, 62, 76, 27, 3, 96, 33, 78, 30]\n",
            "10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8a9b660641d54c47bbcfe4e7b7e05559",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=70.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AhzQjA4HT2wH"
      },
      "source": [
        "!git clone https://github.com/huyvnphan/PyTorch_CIFAR10.git\n",
        "\n",
        "! cp -r /content/cifar10_models/state_dicts /content\n",
        "! cp -r /content/PyTorch_CIFAR10/cifar10_models/resnet.py /content\n",
        "\n",
        "!pip install pytorch_lightning\n",
        "!python /content/PyTorch_CIFAR10/train.py --download_weights 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UTxouRaITy8I"
      },
      "source": [
        "#DOPO AVER IMPORTATO IL CONTENUTO DEL TIPO, SCEGLOERE LA RETE CHE SI VUOLE\n",
        "from resnet import resnet50, resnet18, resnet34\n",
        "\n",
        "trials = resnet34(pretrained = True).to('cuda')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4PNt5gwYHm7Q"
      },
      "source": [
        "#QUI FACCIO FINETUINING SULLA NUOVA RETE, CON LE MIE CLASSI\n",
        "\n",
        "train_dataset = Subset(ilCIFAR100(10, 203, train = 'train'), ilCIFAR100(10, 203, train = 'train').get_batch_indexes()[0])\n",
        "test_dataset = Subset(ilCIFAR100(10, 203, train = 'test'), ilCIFAR100(10, 203, train = 'test').get_batch_indexes()[0])\n",
        "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=4, drop_last=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=True, num_workers=4, drop_last=True)\n",
        "diz = ilCIFAR100(10, 203, train = 'train').get_dict()\n",
        "\n",
        "\n",
        "# Prepare Training\n",
        "optimizer = optim.SGD(trials.parameters(), lr=0.02, momentum=0.9, weight_decay= 1e-5)\n",
        "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[10,20], gamma=0.2)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "trials.eval()\n",
        "total = 0.0\n",
        "correct = 0.0\n",
        "for  _, images, labels in test_loader:\n",
        "  labels = torch.tensor([torch.tensor(diz[c.item()]) for c in labels])\n",
        "  labels = labels.to('cuda')\n",
        "  images = images.to('cuda')\n",
        "  outputs = trials(images)\n",
        "  _, preds = torch.max(outputs, dim=1)\n",
        "  total += len(labels)\n",
        "  correct += torch.sum(preds == labels).item()\n",
        "\n",
        "acc = correct / total\n",
        "print('test accuracy before finetuning', acc)\n",
        "\n",
        "\n",
        "trials.train()\n",
        "for epoch in tqdm(range(25)):\n",
        "  tot_loss = 0.0\n",
        "  for _, inputs, labels in train_loader:\n",
        "    inputs = inputs.float().to('cuda')\n",
        "    labels = torch.tensor([diz[c.item()] for c in labels])\n",
        "\n",
        "    labels=labels.to('cuda')\n",
        "    optimizer.zero_grad()\n",
        "    outputs=trials(inputs)\n",
        "\n",
        "    labels_encoded = F.one_hot(labels,10).float().to('cuda') #CAMBIARE ONE_HOT\n",
        "    loss = criterion(outputs, labels_encoded) \n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    tot_loss+=loss.item()\n",
        "  \n",
        "  scheduler.step()\n",
        "  print(f'loss at epoch{epoch}', tot_loss)\n",
        "\n",
        "\n",
        "trials.eval()\n",
        "total = 0.0\n",
        "correct = 0.0\n",
        "for  _, images, labels in test_loader:\n",
        "  labels = torch.tensor([torch.tensor(diz[c.item()]) for c in labels])\n",
        "  labels = labels.to('cuda')\n",
        "  images = images.to('cuda')\n",
        "  outputs = trials(images)\n",
        "  _, preds = torch.max(outputs, dim=1)\n",
        "  total += len(labels)\n",
        "  correct += torch.sum(preds == labels).item()\n",
        "\n",
        "acc = correct / total\n",
        "print('test accuracy after finetuning', acc)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zJ4w-tWhY6XC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 842,
          "referenced_widgets": [
            "d453cfe60a4445bc8b9720b60ab825b6",
            "1a786c29900b49dabe222c9615c38c3c",
            "3cc388d7927645c5b50f841eadacbcd0",
            "ccd2e8755d8f4e11854bd89b29459010",
            "9b6987eaca1743b3bbbf51c521838fdb",
            "809b1b9ef55343a7bcb5fb047e0a464a",
            "ca92211a11a34e87b2f68ec2339278a0",
            "edac0f26ecd9454393b657c571ac4f7b"
          ]
        },
        "outputId": "4cab2659-30b3-4334-fc9a-32e0bb086a2d"
      },
      "source": [
        "# CODICE PER CREARE LE IMMAGINI SINTETICHE\n",
        "\n",
        "labels_of_modified = torch.zeros(0, dtype = int).to('cuda')\n",
        "for label in batch:\n",
        "  labels = torch.LongTensor([diz[label]]*20).to('cuda')\n",
        "  labels_of_modified = torch.cat((labels_of_modified, labels), dim=0)\n",
        "print('len to be created', len(labels_of_modified))\n",
        "number_of_images_created = 200\n",
        "\n",
        "#SE VOGLIO USARE COME TEACHER LA NOSTRA RETE USARE IL CODICE QUI\n",
        "\n",
        "#teacher = copy.deepcopy(fake_model)\n",
        "#net_teacher = resnet32(num_classes=10).to('cuda')\n",
        "#net_teacher.load_state_dict(teacher.state_dict())\n",
        "#net_teacher.eval()\n",
        "\n",
        "#net_student = resnet32(num_classes=10).to('cuda')\n",
        "net_student = resnet18().to('cuda')\n",
        "\n",
        "trials.eval()\n",
        "\n",
        "inputs = torch.randn((number_of_images_created, 3, 32, 32), requires_grad=True, device='cuda', dtype=torch.float)\n",
        "train_writer = None  # tensorboard writter\n",
        "global_iteration = 0\n",
        "di_lr = 0.05\n",
        "optimizer_di = optim.Adam([inputs], lr=di_lr)\n",
        "plt.imshow(tensor2im(inputs[0]))\n",
        "plt.show()\n",
        "print(\"Starting model inversion\")\n",
        "batch_idx = 0\n",
        "inputs = get_images(net=trials, bs=len(labels_of_modified), epochs=2000, idx=batch_idx, \n",
        "                  net_student=net_student, competitive_scale=10.0, l2_coeff = 3e-8, bn_reg_scale = 5.0,\n",
        "                  train_writer=train_writer, use_amp=False,\n",
        "                  optimizer=optimizer_di, inputs=inputs, \n",
        "                  var_scale=0.001, labels=labels_of_modified) #2.5e-5\n",
        "trials.eval()\n",
        "plt.imshow(tensor2im(inputs[0]))\n",
        "plt.show()\n",
        "print('deepinversion finshed')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "len to be created 200\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Starting model inversion\n",
            "Teacher correct out of 200: 20, loss at 6.08786678314209\n",
            "Student correct out of 200: 25, loss at 2.430589199066162\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d453cfe60a4445bc8b9720b60ab825b6",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=2000.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "It 0\t Losses: total: 56.083,\ttarget: 6.141 \tR_feature_loss unscaled:\t 7.357\n",
            "It 200\t Losses: total: 12.096,\ttarget: 0.001 \tR_feature_loss unscaled:\t 0.687\n",
            "It 400\t Losses: total: 10.231,\ttarget: 0.001 \tR_feature_loss unscaled:\t 0.380\n",
            "It 600\t Losses: total: 9.812,\ttarget: 0.000 \tR_feature_loss unscaled:\t 0.359\n",
            "It 800\t Losses: total: 9.336,\ttarget: 0.000 \tR_feature_loss unscaled:\t 0.262\n",
            "It 1000\t Losses: total: 9.458,\ttarget: 0.000 \tR_feature_loss unscaled:\t 0.235\n",
            "It 1200\t Losses: total: 9.373,\ttarget: 0.000 \tR_feature_loss unscaled:\t 0.234\n",
            "It 1400\t Losses: total: 9.698,\ttarget: 0.010 \tR_feature_loss unscaled:\t 0.293\n",
            "It 1600\t Losses: total: 9.391,\ttarget: 0.000 \tR_feature_loss unscaled:\t 0.231\n",
            "It 1800\t Losses: total: 9.357,\ttarget: 0.000 \tR_feature_loss unscaled:\t 0.248\n",
            "\n",
            "Teacher correct out of 200: 200, loss at 0.0003732972254510969\n",
            "Student correct out of 200: 0, loss at 2.69510555267334\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "deepinversion finshed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 264
        },
        "id": "sokaKsjfV9_n",
        "outputId": "640d53ff-9394-4bdc-cb34-9d55de2e04f0"
      },
      "source": [
        "plt.imshow(tensor2im(inputs[178]))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD5CAYAAADhukOtAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO2da4xkZ5nf/0/dq/p+m77N2DOesfEa4wuZWCYQwu4K5KCNjEVE8AfkD2iNokUK0uaDRaRApHxgowDiE9EQrPVGhEsWCA5BiR3vSoZINh4GX8YeD567e/p+rerq6ro++dA1ydh5/6fbM9PVw57/T2p19fv0e8573nOeOlXv/zzPY+4OIcTffRJ7PQAhRGeQswsRE+TsQsQEObsQMUHOLkRMkLMLERNS19PZzB4C8C0ASQD/0d2/FvX/hXzW+3vz4W1FKIDpVDrYnkjy96pWi2+wZU1qa7a4LZtIBtsj1csm317kGMFttUZEv3p4f5bic1WN2F4yYq7SEQfOekUccsQRA8kkv1Rbzahz3Qq2J1J8e0lyngGg3qzzfUUcXLMRcR14eIzN+j20T3fX68H2cqWBaq1pIZtdq85uZkkAvwPwcQBTAF4C8Ki7v8H6TIz2+xce/VjQlqw16L7GRiaD7V19XbRPpVKhtnKqTG3FcpHaDhXC+2vU+cVhq2vUtl6rUVu5UaW26RV+wRVnwseWGc7QPueWwhcbAAwkV6ltvMHHsUou/HL4OgQA1Iy/IQ30D1BbaY2Po2Lh6yA/PEL7DPb0Utvsyiy1rVf4+Vxd5NdBpRY+Z2vzb9M+//DvfSDY/swL01heqwYn+Xo+xj8A4Iy7n3P3GoAfAHj4OrYnhNhFrsfZJwFc/dYz1W4TQtyE7PoCnZk9bmbHzez4RsTHHCHE7nI9zn4ZwIGr/t7fbnsH7n7M3Y+6+9FCnn9vFELsLtfj7C8BuN3MDplZBsBnATx9Y4YlhLjRXLP05u4NM/sigP+JLentSXcP6wFtGo0W5hc3graD77ud9lsoXgq2p/J9tE+6u4faVqf5iuq+AjVhKBdeET7xBl81HezKUdvEMF9h3mxxNaGZ4Cva6c3wKr6F1UsAQKaxRG21yjq1nXH+tayLHHeEgAKv8vlAgqs1I4N8Poqb3cH26Tl+zEtnLlJbX+8gtW1uLFNbsrVJbefPvhJsn5s9R/t8+qPh9kbEN+Xr0tnd/RcAfnE92xBCdAY9QSdETJCzCxET5OxCxAQ5uxAxQc4uREy4rtX4a8FI4E1ylUsh+/LhoIX1dR7Ec+HMaWrLdPN+ay0u//xuJSxDbRgPxMiVuO3y5gq1dfUOU9utGR5BNTQeHv+F4hztM5Lj87FR5LLWRpGPYxHh87mR7qd98nm+PZ/lEmDPSIReSq63Qxn+gFexwINkKlUuoeUbfK6qDS4dIpkNNr95epF2aVTDx+zO79+6swsRE+TsQsQEObsQMUHOLkRMkLMLERM6uhrvbqg3wrtcX+AP/SeH7wy2v3aR9zk4xiM/shEro4VmRDBDIpyuaCzHp7HCF+MxtVqitvrSeWobHBqjtoOD4RRZd+zbR/tM9vLV7MXBcOASACwt8RRe9Y3wNjczfDUeLb4vL/F9pVJ8/KlUeIU82eLnrG84HDwDAPk6VwU2ajzoCU2+zeRUOC1YaZmnJss0whFFCa3GCyHk7ELEBDm7EDFBzi5ETJCzCxET5OxCxISOSm8tNFDGfNB2eoHLDL3LYRmqd5LnmauX+fb6u3jAQiLBq8VkPCzx5HNcVkmM8X11j/Kgm4tv87x2C0snqW22FM7L9/4Jvq9sgcuUvTZKbV2DPAdgdTMcMFKPkBszvVy6qq5zyW4jzYNaujwss5rzqi/VKg/I6enmAUo9Db7NlWkemFVeCucbTD/LE8qlsyTXYIIHNenOLkRMkLMLERPk7ELEBDm7EDFBzi5ETJCzCxETrkt6M7MLAEoAmgAa7n406v9bAMoejsrqTudpv55WOCooW+UhZakUfx9bB5fsevLh8QHApalwRNzIAZ6zbLDBZaGDeX7M47fdSm0X3+aRecuL4Vxzb73GZZyJwzyKrkhKbwFAby+X83pJaa5EjUtDq3V+zvr2c8mr0OLSYX01HC3XTIWvKQDYyPPt1dIRUWUpLh2OpHh5s4F9twTbC/+V56DrsnD0XQJcNrwROvsfujsflRDipkAf44WICdfr7A7gGTP7jZk9fiMGJITYHa73Y/xH3P2yme0D8KyZvenuz1/9D+03gccBIF/g31+FELvLdd3Z3f1y+/c8gJ8CeCDwP8fc/ai7H81mO16TQgjR5pqd3cy6zKznymsAnwDAIzSEEHvK9dxqRwH81MyubOc/u/v/iOpQrzcwMxsuC3RLV7gEDgBUe8PJ9cqLvGTU2ASXT7pSXDJqznKJqtgIR3I1Zy/SPpsD/P00neSS1/jgJLXlwRXO5ez/Dra/fn6N9pl+m0cI1usRSSCbPPliKheWynrTEVFjzsdRnpvl/fJ8m408iURLcIk13eKRirk0v043M/xcj+Z4Uszf9oXH39X7S9qnkgiPsQU+9mt2dnc/B+Dea+0vhOgskt6EiAlydiFigpxdiJggZxciJsjZhYgJHX3KJYUURhJDQVtfWF0DADQRTlJYBk8OuVTkyRCzWf4kX924JNOWGf8/pqd4HFBlIXy8ANAzxvvV61yG6ssNUtu+obBAstbi0lW1weWacpHXxSuuz1BbsrYS7oNwckUAGCxwCW0oxxNOOh8+PBmWUjM5LnvCeeSYZ/h11RfxgGijwK+rGsKS7gCpUwcA9Vb4Ps1jCnVnFyI2yNmFiAlydiFigpxdiJggZxciJnQ25jQFOFlILtT5ymO6J9ypWeR5xEYyPL/bRB9f2V1P8Px01hte0e6p8eCIVokHkizMcjWhvMnnIznKl30LPeFV38l9E7RPc4Pn8luy8Ko6AOR7D1FbZY4EKfXyuSoaVyC8zsfY3cfLLiUzYZknkeX3uXyTKwae4+MoZ/m1k4hYJy9kw9fjcD9XDApHwuczMc0VEt3ZhYgJcnYhYoKcXYiYIGcXIibI2YWICXJ2IWJCR6W3ZCKJAZITLJPkclIyEw5MOPS+Udrn7v08YKFgPJCkOshll9tLYbnjUpJLaDOXzlJbrsCnf2UlHPwDAF1pHpySTIXnN5/gcmOyxedx/37er5kIB3AAQG1fWDZq5vkxJ0nQCgDUK1yWq5BjBoByIzxXmzV+nrsjSjxV03wcuSov/2Q9/LhbJK9dYmSc9unu+SfB9mSS70d3diFigpxdiJggZxciJsjZhYgJcnYhYoKcXYiYsK30ZmZPAvgTAPPufne7bRDADwEcBHABwGfcnYdHtUkkU8gPjgRtmQJPQneAKBBH+nhUkNV5lNHwCI9cGqtzGerSxmqwvW9imfYZmOT5zM5f/DW1pdZ4zrJyhZe2qlwOjzHTy6XN4RyXjLJNbusf5RFshVo4IrHiXPKyOr/3zBf5pfr6mxeorbEevizrxssxDRb4NTB0C88p2G38mksYl4Lfvz98ge/v49Lmx//Ze79P76THXwJ46F1tTwB4zt1vB/Bc+28hxE3Mts7errf+7lvXwwCear9+CsCnbvC4hBA3mGv9zj7q7lei5GexVdFVCHETc90LdO7uiEhXbWaPm9lxMzu+GfHIoxBid7lWZ58zs3EAaP+eZ//o7sfc/ai7H83l+YKOEGJ3uVZnfxrAY+3XjwH42Y0ZjhBit9iJ9PZ9AB8DMGxmUwC+AuBrAH5kZp8HcBHAZ3ays6Ql0JcMSx75LL/rj+T2Bdu7unnCyUMTXJ5Kg8suMwt8m61SOCqrZ4AntzzUz0saJXL/gNrOrT1PbXPlBWrLNMMSW36Ny40Z59Fm+RyPvMqs8HPWP0oiC8s8mu/MDLe9cOIytc0s8NJW9VY4cefEKJfCTk6/TW09a+vUdvDwAT6OiA+1y62wHDl0K4/m686Hz3Miwe/f2zq7uz9KTH+8XV8hxM2DnqATIibI2YWICXJ2IWKCnF2ImCBnFyImdDjhZAq9+XACw7GI+msZC0eOpXLdtM9vXwhHfwHAL3/5N9R2MUJaaeXCYzw6ycd+173cNtr3B9SWeZBHy6V/9QK1LVTCUlO5xpNiple5TFkyntyyYkeozavhqL38AJfyilM8amxkkEdFFgbvobZ9E+F6dIXGGu2z3sPr7E32ctk2PxqO6ASA8yl+Pt+/Hp6ryxsXaJ+u1ECwPWE8WlJ3diFigpxdiJggZxciJsjZhYgJcnYhYoKcXYiY0FnpLZXA4GhYLsvwICQMjfQE2ystLhm9cPIlanvm1deorZbgcl5XM5x8o6vEk3J0ZXmCRdzN32v7srdS28B9fJuV16eC7RuLPOqtVJijtmydR17VK4vUlimH5auhFI8MG+viyRwb+7l0NZENR0UCQGUqLEVVIiL9mqQ+HAAM9/EIx+7R26gtWeDnOr8QTupZ3eDj6BkLn5dkWtKbELFHzi5ETJCzCxET5OxCxAQ5uxAxoaOr8amUYWAwnDurv8DLDFWb4SCIviRfUW1sFqltPKIUUqnKA2HS2aVgezbNV4MvrvMp7l7lK+T1MW7r65qkNnwgnML//PFTtEslxd/zrThNbbUWX/ldzIaVkorxFXfP8vx/iYhV8PVlXn5ruCd8borrfH67C1zlWdzkwTqly7xc08RYRADNwYlge28fVyBarfBcpZI8iEd3diFigpxdiJggZxciJsjZhYgJcnYhYoKcXYiYsJPyT08C+BMA8+5+d7vtqwD+FMCVOkRfdvdfbLetVgOokMpF+/dzOaxeCEsQyUEux9x/zx3UVl2ZobZl53JHLhuWXZrGg2cQEVSxWeLy4OFBnrtuqc7llUIpnHvvA/dyafPSWW5bqXApst7kATmzi+Ex5pLhQB0AGEmE86oBwHqF7+sjD36I2vp6wxLb6Te49NacP09tpxe59JbIc3daqXCZcpOUcrJ1njfQquF9tZoR46OW/8dfAngo0P5Nd7+v/bOtowsh9pZtnd3dnwfAn1oQQvxecD3f2b9oZq+a2ZNmxj9/CSFuCq7V2b8N4DCA+wDMAPg6+0cze9zMjpvZ8VKZfwcRQuwu1+Ts7j7n7k13bwH4DoAHIv73mLsfdfejPV080b8QYne5Jmc3s6vLejwC4OSNGY4QYrfYifT2fQAfAzBsZlMAvgLgY2Z2HwAHcAHAF3ays3qtirnLvwvaGlWehO7w/nCU1+ED99I+jbu4bbXMS/9UF7k0tL4aln+WElzuSLe49OabPHdd0XgE1b5BPldzlVKwvW8qHLEHAMleLgH2Go/WerM+SG2lSnj8S+cj8taN8rlqbGapbbHF5/EU2d+5c3zNuXjxIrVVIko8peYvUVsjz2XiViJ83PXL/LrKk+i2RpVHgm7r7O7+aKD5u9v1E0LcXOgJOiFigpxdiJggZxciJsjZhYgJcnYhYkJHE042Wi3MboSlgfl5HvVWWz4XbM+TckwAkNrHn+Ad2f8+arNBnhBx+uUzwfambfDtGY/WcnCpqbrBI/OS/fy0DU2EEyyWcQvtk+nnslamyOW1wxES5nwlLA++ep6Xmppa5hGHyPLzcubSm9S2shIe4/SFcHQgANTWubQ51ORJMScHeamsVBe/viu1sK3mYRkVABbL88H2RovPoe7sQsQEObsQMUHOLkRMkLMLERPk7ELEBDm7EDGho9JbE0mUm0SeWOPyz5tdYbkmcZZHax1c5xLP66dOU1tvlkskJ6bCUU2T/eH6agDQl+Hvp8WIWm/5DJd/JtJc8mpUwskj80dupX2qZT7GIR70hp4VbkuWwolKLgzxY/YGH0e1wqP21utcZl1bDEuf5YhoxH09/Br40CcfpLahSR7ZdmmGJ+5cnQlfx92j3CeSjXBC0mSau7Tu7ELEBDm7EDFBzi5ETJCzCxET5OxCxISOrsYnvYXeRniVNj+R5v1K4YCAqSUezDB6kAe7FAb4SjcSPPigkA6X8Nlc4EErY4fHqS3Zx/PM5RN8qXv+VERZoMlwwEhE1SUMTI5RWy+vNIWNbp4tOLUWVgUOV0n9LwBT67wMVW2N566bXePbLNbCuebSA7y81j/6xIepbV9vONAIAP773/C8q2ffOkVti+WwYjDexd3zwPv2B9tbPG2d7uxCxAU5uxAxQc4uREyQswsRE+TsQsQEObsQMWEn5Z8OAPgrAKPYKvd0zN2/ZWaDAH4I4CC2SkB9xt0jQiO2HtLvmwjnNOtxnqvt9MULwfY777yf9hmIyPn1xkw4fxcA3HEHl6FuPxQee22aB90M7eM53BoNLvNlGlxWXIkoyVSaCctQY5N8HPkSDyhKDg9TW6vZQ23pejjwYyDLpav5BpfQaktcmq0UeZBJshXOGbd/gsuGa8uz1PbSiXD5MgA48Vo4RyEAtFpcZk0lw+dzpcXHeHsqLAMnIqTSndzZGwD+3N3vAvAggD8zs7sAPAHgOXe/HcBz7b+FEDcp2zq7u8+4+4n26xKAUwAmATwM4Kn2vz0F4FO7NUghxPXznr6zm9lBAPcDeBHAqLtfeXRsFlsf84UQNyk7dnYz6wbwYwBfcvd3fMlzd8fW9/lQv8fN7LiZHd/Y4N/LhRC7y46c3czS2HL077n7T9rNc2Y23raPAwiuern7MXc/6u5HCwWeyUMIsbts6+xmZtiqx37K3b9xlelpAI+1Xz8G4Gc3fnhCiBvFTqLePgzgcwBeM7OX221fBvA1AD8ys88DuAjgM9ttKJfN4Y4jdwdtv37mad4xHZbRMsbziJ08+Qa1vfwGz0F36qUT1HbP0SPB9v79vOzPZpmXhqrOcdvkES55bUYcd9rD5bVSXPlBq9JNbbWI/HQDA3wcFZK8LgN+XN1T4bEDwFCOz9VaiX89rJPhNyNy0C1V+L5aLV5ybCjH8+sVnc/x5ED4E28lyd0zlwyPw8LfpgHswNnd/VcAmHr3x9v1F0LcHOgJOiFigpxdiJggZxciJsjZhYgJcnYhYkJHE066ORqJsExyucwjlzKbYUlmrcolkiNDPPznoQ/9fWrL5ni/lWpYWjHwsa+v8si2Hq7YodnLky8ut3iCy4FEWNqqGi8llCqHI6gAIBEhJ23k+LGlKuHElwNNfsnVeieobWWVl7yaXwgnlQSA4UJ4Phz8uApZ/vDXQB+/5s7V+bWzvsL7TZEnS8dv45GKD370E8H2H/63X9I+urMLERPk7ELEBDm7EDFBzi5ETJCzCxET5OxCxISOSm/1VhNzlbBcU1y7RPv1pcMRPsODh2ifbI7X8soP8iipxjKvKVZfIQkRk+GkhgCQyPBEidU0l2pmlrmcNJjjml3Fw4kqs0ku41idJ7fszvB9bRT55VNohc9zq8jnPl3mNezGRw9T22pUMsrVsORV52ojKnku840M8ai9O+7j2+w5fZba0rmBYPsjn/407bPeFU722Uzwc6I7uxAxQc4uREyQswsRE+TsQsQEObsQMaGjq/GtagXr508GbaWNC7TfgdvvDLa/PcMDQhar3JaKyCOW42nQkEyEg0maNb6K7E0ecFF3Pv37+nj5qsUN/h5dSIRXpnv7eCDMUqlJbY2IPHme4vNYyoZX8b2Hr5xPX+Kr4Bs5vq+uiG2iGlZKKutcFVhNc3UineW55LLk+gCAO27hZRWW5sIJAi+/PUX7WFd4Bb9BgrUA3dmFiA1ydiFigpxdiJggZxciJsjZhYgJcnYhYsK20puZHQDwV9gqyewAjrn7t8zsqwD+FMCV6JAvu/svorbVaDawuBoOJnHneb+SHg5q6cMI7TN2B5eurMxlnJUlLjVlq+HSOktFXnLHmrzMULLBZZxKhgfJeIvLK94Xzv12eZrLOD2FMWpbWObzAePzyGYkkeyifeo5Po/14gq1bVYjpMhc2NaMiIRZWuJBSF15Lq9VnB9bX+84tQ31hHMYtmbCMjUAVCycr69Z5edkJzp7A8Cfu/sJM+sB8Bsze7Zt+6a7//sdbEMIscfspNbbDICZ9uuSmZ0CMLnbAxNC3Fje03d2MzsI4H4AL7abvmhmr5rZk2YWfqRHCHFTsGNnN7NuAD8G8CV3LwL4NoDDAO7D1p3/66Tf42Z23MyOV0j+dyHE7rMjZzezNLYc/Xvu/hMAcPc5d2+6ewvAdwA8EOrr7sfc/ai7H83n+KKZEGJ32dbZzcwAfBfAKXf/xlXtVy8vPgKALx0KIfacnazGfxjA5wC8ZmYvt9u+DOBRM7sPWyrLBQBf2G5DLSSxiXA0lDX4XX/h8rlg+9rbb9I+2VeK1Nas8/e4ZLKP2grZcHRVK8G3N5rmEWU2cIDaEnUu42xu8uiwiYlwrrlTp+dpn74BLidZgZ+XjdIctSWzYcmxluTbWy1zmW9tOeJ8ZgrU5s2wnFev8X3lIspQLS0sUdtmlp+Xt2a5BDs8EZY+a0RGBYBKIxwp58bzIe5kNf5XAEKib6SmLoS4udATdELEBDm7EDFBzi5ETJCzCxET5OxCxISOJpxsNKqYWzwftG06j+QaGT4YbM83uXyyERH9s7YcjjICgMWLPLpqfjYsNZXX+dhbRT6OYosnqqw7fx9uVnlCxH0T4fJE6TQvh5XpibD188jCwYgIsGxXeB6zXVwmq7X45Zjr5ZJdqsGfzCySBJyFPH+6u7TIS4BlMnwcLeORm8VVfs1Nl8KlsrqiEnC2wlJemZRXA3RnFyI2yNmFiAlydiFigpxdiJggZxciJsjZhYgJna31hgQ2iTwxceAI7dc9QhIiLl6iffq7eIK/fO9t1LZvkMth994TjqCqPsKnsRyR3LIwyBNO5lJc1kpkuBy2UQzLcqkmj74rf5wnepyf48kXKxV+3IWBcNReIsPlqQwiIuyavDZbtsGTRw71hsex2uD3ueExPsZcmkuHpQqXyjL9fIzJQjgSNFfg+6r3hfucOM5lQ93ZhYgJcnYhYoKcXYiYIGcXIibI2YWICXJ2IWJCR6W3jBkOJHJBmzuPXBrpDfdJJcPJFQGgWORRRtk0l0HWjU9JpRqWf1ZWuSxUW+dJGWfP8uSAC0Uu41xe5DLa3Eo4sWQuQvKqpPg4aiV+bKXlqAircLRZ1fn8ZiOi6PL9YakJAHLJ8PUBAHVyiXdHbG+cJO0EgGTEvgw8+rFnmB9bVzp8z621uGzb3R8eY63Mz5fu7ELEBDm7EDFBzi5ETJCzCxET5OxCxARz50EQAGBmOQDPA8hia/X+r939K2Z2CMAPAAwB+A2Az3nUkjqAgZ6s/+H94dLu9912B+03PtwTHluKr+wmMuHVYABolXmeubUiLzO0UAnnvGtV+RxGlWpai6hqm0rx8a9Mc6WhGK4KhN4hPsZzS/yYu1pkgwD6GnwVv3Dg1mB7qcJXmEf2H6a2UomXXcp381JZzVp4f4UcL63U08VLgK2s87yH+V4euLIZMf51cj02s/y4hgbDPvHznx/H4mIpVMFpR3f2KoA/cvd7sVWe+SEzexDAXwD4prsfAbAC4PM72JYQYo/Y1tl9iyu3knT7xwH8EYC/brc/BeBTuzJCIcQNYaf12ZPtCq7zAJ4FcBbAqrtfyWc7BSD8+VwIcVOwI2d396a73wdgP4AHANy50x2Y2eNmdtzMjlfr/MkvIcTu8p5W4919FcDfAvgQgH6z//ts6X4Al0mfY+5+1N2PRj2mKoTYXbZ1djMbMbP+9us8gI8DOIUtp/+n7X97DMDPdmuQQojrZyeBMOMAnjKzJLbeHH7k7j83szcA/MDM/i2A3wL47nYbyubSuO0PwrnhRod5CaLBXFiCqKR4cEejHi6PAwC1IS55DY1x2SW5RmSXZS7HVCo8cKJV5P1ySf4pqDnAT1tzISzx9CUj8qrl+THXNnleuKUSH//ZU8EPetgocblu/yK3pbL8vpRM8HJYlg7n+evqmaZ9zm+epbbcAJfs1i5yCdM8qIYBALweDl7J9vB8iNPT4TJqlQoPhNnW2d39VQD3B9rPYev7uxDi9wA9QSdETJCzCxET5OxCxAQ5uxAxQc4uREzYNurthu7MbAHAxfafwwB4rZrOoXG8E43jnfy+jeNWdw/WB+uos79jx2bH3f3onuxc49A4YjgOfYwXIibI2YWICXvp7Mf2cN9Xo3G8E43jnfydGceefWcXQnQWfYwXIibsibOb2UNmdtrMzpjZE3sxhvY4LpjZa2b2spkd7+B+nzSzeTM7eVXboJk9a2ZvtX8P7NE4vmpml9tz8rKZfbID4zhgZn9rZm+Y2etm9i/a7R2dk4hxdHROzCxnZr82s1fa4/g37fZDZvZi229+aGY8JDGEu3f0B0ASW2mtbgOQAfAKgLs6PY72WC4AGN6D/X4UwAcBnLyq7d8BeKL9+gkAf7FH4/gqgH/Z4fkYB/DB9useAL8DcFen5yRiHB2dEwAGoLv9Og3gRQAPAvgRgM+22/8DgH/+Xra7F3f2BwCccfdzvpV6+gcAHt6DcewZ7v48gOV3NT+MrcSdQIcSeJJxdBx3n3H3E+3XJWwlR5lEh+ckYhwdxbe44Ule98LZJwG8fdXfe5ms0gE8Y2a/MbPH92gMVxh195n261kAo3s4li+a2avtj/m7/nXiaszsILbyJ7yIPZyTd40D6PCc7EaS17gv0H3E3T8I4B8D+DMz++heDwjYemfH1hvRXvBtAIexVSNgBsDXO7VjM+sG8GMAX3L3d6R96eScBMbR8Tnx60jyytgLZ78M4MBVf9NklbuNu19u/54H8FPsbeadOTMbB4D273Ch9V3G3efaF1oLwHfQoTkxszS2HOx77v6TdnPH5yQ0jr2ak/a+33OSV8ZeOPtLAG5vryxmAHwWwNOdHoSZdZlZz5XXAD4B4GR0r13laWwl7gT2MIHnFedq8wg6MCdmZtjKYXjK3b9xlamjc8LG0ek52bUkr51aYXzXauMnsbXSeRbAv9qjMdyGLSXgFQCvd3IcAL6PrY+DdWx99/o8tmrmPQfgLQD/C8DgHo3jPwF4DcCr2HK28Q6M4yPY+oj+KoCX2z+f7PScRIyjo3MC4B5sJXF9FVtvLP/6qmv21wDOAPgvALLvZbt6gk6ImBD3BTohYoOcXYiYIGcXIibI2YWICXJ2IWKCnHkHirgAAAATSURBVF2ImCBnFyImyNmFiAn/B84VbjPnv2AWAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0fG-9jw4UAgZ"
      },
      "source": [
        "#IN MODO DA AVERE UN DIZIONARIO UNICO PER IL FINETUNING SINTETICI + IMMAGINI\n",
        "fake_diz = {0:11, 1:5, 2:62, 3:76, 4:27, 5:3, 6:96, 7:33, 8:78, 9:30}\n",
        "labels_of_modified = torch.tensor([fake_diz[c.item()] for c in labels_of_modified]).to('cuda')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3g__5sQg-uzp"
      },
      "source": [
        "# CREO UN DATALOADER UNICO CON DATI+IMMAGINI SINTETICHE\n",
        "inputs = torch.tensor(inputs, requires_grad=False)\n",
        "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=4, drop_last=True)\n",
        "\n",
        "porcoddio = torch.utils.data.TensorDataset(torch.randn(len(inputs.data), requires_grad=False).cpu(), inputs.cpu(), labels_of_modified.cpu())\n",
        "porcoddiol = DataLoader(porcoddio, batch_size = 128, shuffle=False, drop_last=False)#, num_workers=4, drop_last=False)\n",
        "\n",
        "tt = DataLoader(torch.utils.data.ConcatDataset((train_dataset, porcoddio)), batch_size=128, shuffle=False, num_workers=4, pin_memory=True)\n",
        "for param in model.parameters():\n",
        "  param.requires_grad = False\n",
        "\n",
        "model.train('False')\n",
        "count = 0\n",
        "for _, image, label in tt:\n",
        "  labels = torch.tensor(torch.tensor([diz[c.item()] for c in label]))\n",
        "  labels = labels.to('cuda')\n",
        "  image = image.to('cuda')\n",
        "  outputs = model(image)\n",
        "  _, preds = torch.max(outputs, dim=1)\n",
        "  total += len(labels)\n",
        "  correct += torch.sum(preds == labels).item()\n",
        "\n",
        "acc = correct / total\n",
        "model.train('False')\n",
        "print('test accuracy with syntetic exemplars', acc)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NalwamQY5jQ8",
        "outputId": "5af43eba-beae-412d-db4f-d8490c23bc77"
      },
      "source": [
        "#PER VERIFICARE COME LA NOSTRA RETE CLASSIFICA LE IMMAGINI SINTETICHE\n",
        "trials.eval()\n",
        "\n",
        "total = 200.0\n",
        "correct = 0.0\n",
        "\n",
        "label = torch.tensor([torch.tensor(diz[c.item()]) for c in labels_of_modified]).to('cuda')\n",
        "outputs = model(inputs.data)\n",
        "_, preds = torch.max(outputs, dim=1)\n",
        "correct += torch.sum(preds == label).item()\n",
        "#total += len(label)\n",
        "acc = correct / total\n",
        "print('test accuracy with syntetic exemplars', acc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test accuracy with syntetic exemplars 0.81\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FKLFf0sLiKFe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "46793bcc78c54ee287aadd37ae35d6a9",
            "b3b7443e3312487291d1732ba6108ff4",
            "d6ce9a0eaaac44b4a82ace7eadb31631",
            "d899909d635745fc872ff662e7b47cfb",
            "81ead15fddd545b3b5d6e3b152642a90",
            "48854589e5844e0aa45f60badaddd4f2",
            "9eecd3c8c9714b18ae4c855e36b7db15",
            "bb8635d8039049c6b45eab0c0c47f310"
          ]
        },
        "outputId": "74eff0ca-d2aa-4ac2-eed2-eeeb15e842eb"
      },
      "source": [
        "#ALLENO UN MODELLO DA ZERO CON DATI+EXEMPLAR SINTETICI\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=4, drop_last=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=4, drop_last=True)\n",
        "fake_model = resnet32(num_classes=100).to('cuda')\n",
        "fake_model.train()\n",
        "\n",
        "optimizer = optim.SGD(fake_model.parameters(), lr=2, momentum=0.9, weight_decay= 1e-5)\n",
        "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[49,63], gamma=0.2)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "for epoch in tqdm(range(70)):\n",
        "  tot_loss = 0.0 \n",
        "  for _, inputs, labels in tt:\n",
        "    inputs = inputs.float().to('cuda')\n",
        "    labels = torch.tensor([diz[c.item()] for c in labels])\n",
        "\n",
        "    labels=labels.to('cuda')\n",
        "    optimizer.zero_grad()\n",
        "    outputs=fake_model(inputs)\n",
        "\n",
        "    labels_encoded = F.one_hot(labels, 100).float().to('cuda') #CAMBIARE ONE_HOT\n",
        "    loss = criterion(outputs, labels_encoded) \n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    tot_loss+=loss.item()\n",
        "  scheduler.step()\n",
        "  print(f'loss at epoch {epoch}', tot_loss)\n",
        "fake_model.eval()\n",
        "\n",
        "total = 0.0\n",
        "correct = 0.0\n",
        "for  _, images, labels in test_loader:\n",
        "  labels = torch.tensor([torch.tensor(diz[c.item()]) for c in labels])\n",
        "  labels = labels.to('cuda')\n",
        "  images = images.to('cuda')\n",
        "  outputs = fake_model(images)\n",
        "  _, preds = torch.max(outputs, dim=1)\n",
        "  total += len(labels)\n",
        "  correct += torch.sum(preds == labels).item()\n",
        "\n",
        "acc = correct / total\n",
        "print('test accuracy with syntetic exemplars', acc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "46793bcc78c54ee287aadd37ae35d6a9",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=70.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "loss at epoch 0 2.5802983846515417\n",
            "loss at epoch 1 1.0101930405944586\n",
            "loss at epoch 2 0.9449390359222889\n",
            "loss at epoch 3 0.8827617298811674\n",
            "loss at epoch 4 0.8318252824246883\n",
            "loss at epoch 5 0.8027949687093496\n",
            "loss at epoch 6 0.7641696948558092\n",
            "loss at epoch 7 0.7337770871818066\n",
            "loss at epoch 8 0.6849148841574788\n",
            "loss at epoch 9 0.6638820888474584\n",
            "loss at epoch 10 0.6440098490566015\n",
            "loss at epoch 11 0.6198452329263091\n",
            "loss at epoch 12 0.5859145103022456\n",
            "loss at epoch 13 0.5727171348407865\n",
            "loss at epoch 14 0.5553180174902081\n",
            "loss at epoch 15 0.5281490152701735\n",
            "loss at epoch 16 0.507981772068888\n",
            "loss at epoch 17 0.492285686545074\n",
            "loss at epoch 18 0.47756931837648153\n",
            "loss at epoch 19 0.4554994157515466\n",
            "loss at epoch 20 0.4444514620117843\n",
            "loss at epoch 21 0.4434776599518955\n",
            "loss at epoch 22 0.41753871412947774\n",
            "loss at epoch 23 0.40018820902332664\n",
            "loss at epoch 24 0.4093581195920706\n",
            "loss at epoch 25 0.37781394994817674\n",
            "loss at epoch 26 0.3682264487724751\n",
            "loss at epoch 27 0.35050928429700434\n",
            "loss at epoch 28 0.34922860912047327\n",
            "loss at epoch 29 0.33773291180841625\n",
            "loss at epoch 30 0.3539655697531998\n",
            "loss at epoch 31 0.33665915625169873\n",
            "loss at epoch 32 0.32713000499643385\n",
            "loss at epoch 33 0.3107083775103092\n",
            "loss at epoch 34 0.3202135612955317\n",
            "loss at epoch 35 0.3007053965702653\n",
            "loss at epoch 36 0.2873319030040875\n",
            "loss at epoch 37 0.2838056094478816\n",
            "loss at epoch 38 0.29142809892073274\n",
            "loss at epoch 39 0.2747353131417185\n",
            "loss at epoch 40 0.2713928893208504\n",
            "loss at epoch 41 0.2678395169787109\n",
            "loss at epoch 42 0.2627664516912773\n",
            "loss at epoch 43 0.23724426154512912\n",
            "loss at epoch 44 0.2398262384813279\n",
            "loss at epoch 45 0.22887001000344753\n",
            "loss at epoch 46 0.2322451442014426\n",
            "loss at epoch 47 0.2371225401875563\n",
            "loss at epoch 48 0.24387746676802635\n",
            "loss at epoch 49 0.18481053708819672\n",
            "loss at epoch 50 0.13808603514917195\n",
            "loss at epoch 51 0.12126739177620038\n",
            "loss at epoch 52 0.11194270063424483\n",
            "loss at epoch 53 0.1030979330826085\n",
            "loss at epoch 54 0.09589124246849678\n",
            "loss at epoch 55 0.09134707448538393\n",
            "loss at epoch 56 0.08247602259507403\n",
            "loss at epoch 57 0.08724874121253379\n",
            "loss at epoch 58 0.07810271115158685\n",
            "loss at epoch 59 0.07616556627908722\n",
            "loss at epoch 60 0.07045453114551492\n",
            "loss at epoch 61 0.07007620512740687\n",
            "loss at epoch 62 0.06721597004798241\n",
            "loss at epoch 63 0.060428396973293275\n",
            "loss at epoch 64 0.054925138741964474\n",
            "loss at epoch 65 0.05443641243618913\n",
            "loss at epoch 66 0.05473326685023494\n",
            "loss at epoch 67 0.05210005758272018\n",
            "loss at epoch 68 0.0519548839511117\n",
            "loss at epoch 69 0.05186211624823045\n",
            "\n",
            "test accuracy with syntetic exemplars 0.8147321428571429\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "c465e2f3bb3c4aabbe1808aaf2dc10da",
            "279a75ced88f4f27911f40002732b82e",
            "ab2b81b93afa41de8bae496bfb9e2f91",
            "6805c3102397405897214cfe6cb0e8b8",
            "92c62f85026e41dd8e3574e5a7250077",
            "a238202280264cdb899e73456400f836",
            "9034c74b912f45758c2d20f2055e4a8e",
            "2eced2ecd43449dabd20be7e1289e73d"
          ]
        },
        "id": "hDD941kelDM6",
        "outputId": "ca8531db-599b-4253-c10a-bcc3b1ec3efa"
      },
      "source": [
        "#ALLENO UN MODELLO DA ZERO CON SOLO I DATI E CONFRONTO I RISULTATI CON QUELLI PRECEDENTI (PORCODIO)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=4, drop_last=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=4, drop_last=True)\n",
        "fake_model = resnet32(num_classes=100).to('cuda')\n",
        "fake_model.train()\n",
        "\n",
        "optimizer = optim.SGD(fake_model.parameters(), lr=2, momentum=0.9, weight_decay= 1e-5)\n",
        "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[49,63], gamma=0.2)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "for epoch in tqdm(range(70)):\n",
        "  tot_loss = 0.0 \n",
        "  for _, inputs, labels in train_loader:\n",
        "    inputs = inputs.float().to('cuda')\n",
        "    labels = torch.tensor([diz[c.item()] for c in labels])\n",
        "\n",
        "    labels=labels.to('cuda')\n",
        "    optimizer.zero_grad()\n",
        "    outputs=fake_model(inputs)\n",
        "\n",
        "    labels_encoded = F.one_hot(labels, 100).float().to('cuda') #CAMBIARE ONE_HOT\n",
        "    loss = criterion(outputs, labels_encoded) \n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    tot_loss+=loss.item()\n",
        "  scheduler.step()\n",
        "  print(f'loss at epoch {epoch}', tot_loss)\n",
        "fake_model.eval()\n",
        "\n",
        "total = 0.0\n",
        "correct = 0.0\n",
        "for  _, images, labels in test_loader:\n",
        "  labels = torch.tensor([torch.tensor(diz[c.item()]) for c in labels])\n",
        "  labels = labels.to('cuda')\n",
        "  images = images.to('cuda')\n",
        "  outputs = fake_model(images)\n",
        "  _, preds = torch.max(outputs, dim=1)\n",
        "  total += len(labels)\n",
        "  correct += torch.sum(preds == labels).item()\n",
        "\n",
        "acc = correct / total\n",
        "print('test accuracy without syntetic exemplars', acc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c465e2f3bb3c4aabbe1808aaf2dc10da",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=70.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "loss at epoch 0 2.4703085776418447\n",
            "loss at epoch 1 0.9339430555701256\n",
            "loss at epoch 2 0.8338259309530258\n",
            "loss at epoch 3 0.78937110863626\n",
            "loss at epoch 4 0.7423957977443933\n",
            "loss at epoch 5 0.7198673170059919\n",
            "loss at epoch 6 0.6831999151036143\n",
            "loss at epoch 7 0.67094671074301\n",
            "loss at epoch 8 0.6377775687724352\n",
            "loss at epoch 9 0.6111948760226369\n",
            "loss at epoch 10 0.5824559880420566\n",
            "loss at epoch 11 0.5568671561777592\n",
            "loss at epoch 12 0.5324082942679524\n",
            "loss at epoch 13 0.5201006978750229\n",
            "loss at epoch 14 0.5228700172156096\n",
            "loss at epoch 15 0.489839824847877\n",
            "loss at epoch 16 0.4739494966343045\n",
            "loss at epoch 17 0.4669453129172325\n",
            "loss at epoch 18 0.4445440610870719\n",
            "loss at epoch 19 0.43292839266359806\n",
            "loss at epoch 20 0.40585058368742466\n",
            "loss at epoch 21 0.4028914086520672\n",
            "loss at epoch 22 0.3990298006683588\n",
            "loss at epoch 23 0.37390231247991323\n",
            "loss at epoch 24 0.3628548560664058\n",
            "loss at epoch 25 0.36734587978571653\n",
            "loss at epoch 26 0.3553555691614747\n",
            "loss at epoch 27 0.3461624034680426\n",
            "loss at epoch 28 0.34944326011464\n",
            "loss at epoch 29 0.35434877779334784\n",
            "loss at epoch 30 0.31598217971622944\n",
            "loss at epoch 31 0.32446893211454153\n",
            "loss at epoch 32 0.30008572386577725\n",
            "loss at epoch 33 0.3113550180569291\n",
            "loss at epoch 34 0.2864303016103804\n",
            "loss at epoch 35 0.2791980607435107\n",
            "loss at epoch 36 0.29127933317795396\n",
            "loss at epoch 37 0.2663182783871889\n",
            "loss at epoch 38 0.28133521042764187\n",
            "loss at epoch 39 0.27009989507496357\n",
            "loss at epoch 40 0.24596875300630927\n",
            "loss at epoch 41 0.2575107207521796\n",
            "loss at epoch 42 0.2335980711504817\n",
            "loss at epoch 43 0.22617949591949582\n",
            "loss at epoch 44 0.22330635786056519\n",
            "loss at epoch 45 0.25822074385359883\n",
            "loss at epoch 46 0.23385180765762925\n",
            "loss at epoch 47 0.21599727380089462\n",
            "loss at epoch 48 0.2186085965950042\n",
            "loss at epoch 49 0.1620518914423883\n",
            "loss at epoch 50 0.13150424847844988\n",
            "loss at epoch 51 0.12317949323914945\n",
            "loss at epoch 52 0.11455937172286212\n",
            "loss at epoch 53 0.10940443875733763\n",
            "loss at epoch 54 0.10752291756216437\n",
            "loss at epoch 55 0.1006922012893483\n",
            "loss at epoch 56 0.1003602835116908\n",
            "loss at epoch 57 0.09940986742731184\n",
            "loss at epoch 58 0.09121914918068796\n",
            "loss at epoch 59 0.09516349725890905\n",
            "loss at epoch 60 0.08522347640246153\n",
            "loss at epoch 61 0.08331030758563429\n",
            "loss at epoch 62 0.08228137867990881\n",
            "loss at epoch 63 0.07878952124156058\n",
            "loss at epoch 64 0.07612868933938444\n",
            "loss at epoch 65 0.06840585067402571\n",
            "loss at epoch 66 0.06926961144199595\n",
            "loss at epoch 67 0.06618171220179647\n",
            "loss at epoch 68 0.0681896316818893\n",
            "loss at epoch 69 0.06855158100370318\n",
            "\n",
            "test accuracy with syntetic exemplars 0.8392857142857143\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zMVWq87mcCqu",
        "outputId": "d927d9d9-2c1d-4a33-c51f-9ee56b46af2f"
      },
      "source": [
        "fake_model.eval()\n",
        "total = 0.0\n",
        "correct = 0.0\n",
        "for  _, images, labels in test_loader:\n",
        "  labels = torch.tensor([torch.tensor(diz[c.item()]) for c in labels])\n",
        "  labels = labels.to('cuda')\n",
        "  images = images.to('cuda')\n",
        "  outputs = fake_model(images)\n",
        "  _, preds = torch.max(outputs, dim=1)\n",
        "  total += len(labels)\n",
        "  correct += torch.sum(preds == labels).item()\n",
        "\n",
        "acc = correct / total\n",
        "print('test accuracy with syntetic exemplars', acc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test accuracy with syntetic exemplars 0.8392857142857143\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "umrCzlLBgXYq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}