{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Immagini sintetiche integrate nell'allenamento.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "8cf040765ab74dbaadad053a354daf90": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_2d54b65c0d66461eaa32669b22d60a92",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_d7bbbea14a4642ccb0fae794fe27452b",
              "IPY_MODEL_cfcb505a079d45109b8809cb39663d9e"
            ]
          }
        },
        "2d54b65c0d66461eaa32669b22d60a92": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d7bbbea14a4642ccb0fae794fe27452b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_68849cae536d4e989e533ad376b910cd",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 70,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 70,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3fe118bf3d6b447580d04e3d768b978a"
          }
        },
        "cfcb505a079d45109b8809cb39663d9e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_6d57853ebd9f40cfa4820a95f0f4990e",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 70/70 [09:09&lt;00:00,  7.85s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_dde161228c3842548624717186633725"
          }
        },
        "68849cae536d4e989e533ad376b910cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3fe118bf3d6b447580d04e3d768b978a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6d57853ebd9f40cfa4820a95f0f4990e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "dde161228c3842548624717186633725": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "34fc9558c5c844f8bbbd057d938a13de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_6b2dd5c6fe0949b8825b7df113dee6e1",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_40a58b073e924c459db2c10cced3a3e2",
              "IPY_MODEL_7469612806524cc285c1be1e11da6b83"
            ]
          }
        },
        "6b2dd5c6fe0949b8825b7df113dee6e1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "40a58b073e924c459db2c10cced3a3e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_ad4ffa5060b041649e20b07c54196ecf",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 30,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 30,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a0205e1a3dcb4bee8f8807601a19dfdb"
          }
        },
        "7469612806524cc285c1be1e11da6b83": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_ea81207411964d6f97ad1c3e65d3494f",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 30/30 [10:29&lt;00:00, 20.98s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_950639fdad4649d9ab674fb968c180ed"
          }
        },
        "ad4ffa5060b041649e20b07c54196ecf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a0205e1a3dcb4bee8f8807601a19dfdb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ea81207411964d6f97ad1c3e65d3494f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "950639fdad4649d9ab674fb968c180ed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8ab9014f6da940dcbca287ae87403955": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_9e1312a916f548cc82647afb4bc1f93e",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_7f041afe43e14af79fd2f6597cc43b75",
              "IPY_MODEL_80948880c9864340a539797dcfb95562"
            ]
          }
        },
        "9e1312a916f548cc82647afb4bc1f93e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7f041afe43e14af79fd2f6597cc43b75": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_79cdaacab4f2456e9d70b6eebf42e594",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1500,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1500,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8679353713f544a38be7e308738302c8"
          }
        },
        "80948880c9864340a539797dcfb95562": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_154763e3f13d4164acf06ac0f3a8a3d3",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1500/1500 [13:54&lt;00:00,  1.80it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_971fba7bbe9642a4b7ceeb872429bcec"
          }
        },
        "79cdaacab4f2456e9d70b6eebf42e594": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8679353713f544a38be7e308738302c8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "154763e3f13d4164acf06ac0f3a8a3d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "971fba7bbe9642a4b7ceeb872429bcec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2a31ea6f07bf47d3b943744ebb4dc2da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_979432db69974b7f983e64b9556ae519",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_e0c734b418f24d02af2c89cb85818e61",
              "IPY_MODEL_1b873a72e9eb4f50a22f743a0c683077"
            ]
          }
        },
        "979432db69974b7f983e64b9556ae519": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e0c734b418f24d02af2c89cb85818e61": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_a53c583c6ca440948a71a37875694ccb",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 70,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 70,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b6ddd90b25544dc1a9478d0546c9d537"
          }
        },
        "1b873a72e9eb4f50a22f743a0c683077": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_40e4b81f48f94a379a06f520765652f9",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 70/70 [03:40&lt;00:00,  3.16s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_296fe8ad9d23414ab1cd80d06b00960c"
          }
        },
        "a53c583c6ca440948a71a37875694ccb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b6ddd90b25544dc1a9478d0546c9d537": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "40e4b81f48f94a379a06f520765652f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "296fe8ad9d23414ab1cd80d06b00960c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "cb8fc012d780484cb6fc58526c2dd82c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_3ca0fbcc154f45d9b9d904ae38fbf1de",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_9476ee6f351d4894b3a2f5f818ab451f",
              "IPY_MODEL_7baef604cf784f0f98f17bd234037c53"
            ]
          }
        },
        "3ca0fbcc154f45d9b9d904ae38fbf1de": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9476ee6f351d4894b3a2f5f818ab451f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_6bef149195cc4be7844189cce7f71a78",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 70,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 70,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0c3f1e3e59d9409f9286cad49defa66e"
          }
        },
        "7baef604cf784f0f98f17bd234037c53": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_99b4f1610a7e4532921bcbeed599aa56",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 70/70 [04:30&lt;00:00,  3.86s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6b3b8a0ba37345b8baac5af94a91a7af"
          }
        },
        "6bef149195cc4be7844189cce7f71a78": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0c3f1e3e59d9409f9286cad49defa66e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "99b4f1610a7e4532921bcbeed599aa56": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6b3b8a0ba37345b8baac5af94a91a7af": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alessandronicolini/IncrementalLearning/blob/main/Immagini_sintetiche_integrate_nell'allenamento.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WH7YHXeh0hFj",
        "outputId": "e7c186fa-b0c3-42bf-dc1f-a93b98d7c6cb"
      },
      "source": [
        "!pip3 install 'import_ipynb'\n",
        "!pip3 install 'tqdm'\n",
        " \n",
        "!rm -r IncrementalLearning\n",
        "# upload work files from your git hub repository\n",
        "import sys\n",
        " \n",
        "!git clone https://github.com/alessandronicolini/IncrementalLearning.git # clone proj repository\n",
        "!rm -rf IncrementalLearning/README.md \n",
        "!rm -rf IncrementalLearning/baselines.ipynb\n",
        " \n",
        "path = 'IncrementalLearning/'\n",
        "if path not in sys.path:\n",
        "    sys.path.append('IncrementalLearning/')\n",
        " \n",
        "!pip3 install import_ipynb"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting import_ipynb\n",
            "  Downloading https://files.pythonhosted.org/packages/63/35/495e0021bfdcc924c7cdec4e9fbb87c88dd03b9b9b22419444dc370c8a45/import-ipynb-0.1.3.tar.gz\n",
            "Building wheels for collected packages: import-ipynb\n",
            "  Building wheel for import-ipynb (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for import-ipynb: filename=import_ipynb-0.1.3-cp36-none-any.whl size=2976 sha256=c91094f716a901e92f47eb37d28e678c5a1d8f549814a99627ee9cf7dac0d9d8\n",
            "  Stored in directory: /root/.cache/pip/wheels/b4/7b/e9/a3a6e496115dffdb4e3085d0ae39ffe8a814eacc44bbf494b5\n",
            "Successfully built import-ipynb\n",
            "Installing collected packages: import-ipynb\n",
            "Successfully installed import-ipynb-0.1.3\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.41.1)\n",
            "rm: cannot remove 'IncrementalLearning': No such file or directory\n",
            "Cloning into 'IncrementalLearning'...\n",
            "remote: Enumerating objects: 172, done.\u001b[K\n",
            "remote: Counting objects: 100% (172/172), done.\u001b[K\n",
            "remote: Compressing objects: 100% (171/171), done.\u001b[K\n",
            "remote: Total 646 (delta 107), reused 0 (delta 0), pack-reused 474\u001b[K\n",
            "Receiving objects: 100% (646/646), 882.27 KiB | 3.01 MiB/s, done.\n",
            "Resolving deltas: 100% (382/382), done.\n",
            "Requirement already satisfied: import_ipynb in /usr/local/lib/python3.6/dist-packages (0.1.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UEF9KBox0cAd"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import VisionDataset\n",
        "from PIL import Image\n",
        "import random\n",
        "import torchvision\n",
        "ROOT = './data'\n",
        "class ilCIFAR100(VisionDataset):\n",
        "    \"\"\"\n",
        "    Extends CIFAR100 class. Split the dataset into 10 batches, each one containing 10 classes.\n",
        "    You can retrieve the batches from the attribute \"batches\", it has different structure according to\n",
        "    test and train CIFAR100 splits:\n",
        "        - train -> batches is a dictionary {0:{'train':indexes, 'val':indexes}...} \n",
        "        - test -> batches is a dictionary {0:indexes...}\n",
        "    where the keys are the batch number.\n",
        "\n",
        "    Args:\n",
        "        root (string): Root directory of dataset where directory\n",
        "            `cifar-10-batches-py` exists or will be saved to if download is set to True.\n",
        "        seed(int): used to ensure reproducibility in shuffling operations.\n",
        "        val_size(float, optional): between 0 and 1, fraction of data used for validation.\n",
        "        train (bool, optional): If True, creates dataset from training set, otherwise\n",
        "            creates from test set.\n",
        "        transform (callable, optional): A function/transform that takes in an PIL image\n",
        "            and returns a transformed version. E.g, `transforms.RandomCrop`\n",
        "        target_transform (callable, optional): A function/transform that takes in the\n",
        "            target and transforms it.\n",
        "        download (bool, optional): If true, downloads the dataset from the internet and\n",
        "            puts it in root directory. If dataset is already downloaded, it is not\n",
        "            downloaded again.\n",
        "    \"\"\"\n",
        "    def __init__(self,classes_per_batch, seed, val_size=0.1, train=True, transform=None, target_transform=False, \n",
        "    download=True):\n",
        "        \n",
        "        super(ilCIFAR100, self).__init__(root=0)\n",
        "        self.classes_per_batch=classes_per_batch\n",
        "\n",
        "        \n",
        "        self.__rs = seed # set random seed \n",
        "        self.train=train\n",
        "        self.__transform_train = transforms.Compose([\n",
        "            transforms.RandomCrop(32, padding=4),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))\n",
        "        ])\n",
        "\n",
        "        self.__transform_test = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))\n",
        "        ])\n",
        "        # if train:\n",
        "        #     self.batches = self.__make_train_batches(val_size)\n",
        "        # else:\n",
        "        #     self.batches = self.__make_test_batches()\n",
        "        if self.train == 'train':\n",
        "          self.dataset = torchvision.datasets.CIFAR100(root=ROOT, train=True,\n",
        "                                            download=True, transform=self.__transform_train)\n",
        "        elif self.train == 'exemplars':\n",
        "          self.dataset = torchvision.datasets.CIFAR100(root=ROOT, train=True,\n",
        "                                            download=True, transform=self.__transform_test)\n",
        "        else:\n",
        "          self.dataset = torchvision.datasets.CIFAR100(root=ROOT, train=False,\n",
        "                                        download=True, transform=self.__transform_test)\n",
        "          \n",
        "        self.targets = np.array(self.dataset.targets) # make targets an array to exploit masking\n",
        "        random.seed(seed)\n",
        "        self.classes = random.sample(range(0, 100), 100)\n",
        "        #self.classes = self.classes.reshape((10, -1)) # each row contains the classes for the corrisponding batch\n",
        "        #print(self.classes)\n",
        "        self.__dictionary = {}\n",
        "        for i, c in enumerate(self.classes):\n",
        "          self.__dictionary[c] = i\n",
        "\n",
        "\n",
        "    def get_dict(self):\n",
        "      return self.__dictionary\n",
        "    def __getitem__(self, index):\n",
        "        return index,self.dataset.__getitem__(index)[0],self.dataset.__getitem__(index)[1]\n",
        "    def __len__(self):\n",
        "        return self.dataset.__len__()\n",
        "    def getbatches(self):\n",
        "      classlist=self.classes\n",
        "      batches=[]\n",
        "      for i in range(0,int(100/self.classes_per_batch)):\n",
        "        #print(i)\n",
        "        batch=classlist[int(i*self.classes_per_batch):int(i*10+self.classes_per_batch)]\n",
        "        batches.append(batch)\n",
        "      return batches\n",
        "    def get_batch_indexes(self):\n",
        "      classlist=self.classes\n",
        "      numclass=self.classes_per_batch\n",
        "      batch_indexes=[]\n",
        "      for i in range(0,int(100/self.classes_per_batch)):\n",
        "        batch=classlist[int(i*numclass):int(i*numclass+numclass)]\n",
        "        mask=np.isin(self.targets,batch)\n",
        "        indexes=np.array(np.arange(len(self.dataset.targets)))\n",
        "        indexes=indexes[mask]\n",
        "        batch_indexes.append(indexes)\n",
        "      return batch_indexes\n",
        "    def get_class_indexes(self,label):\n",
        "      indexes = np.array(np.arange(len(self.dataset.targets)))\n",
        "      labels = self.dataset.targets\n",
        "      mask = np.isin(labels, label)\n",
        "      indexes = indexes[mask]\n",
        "\n",
        "      return indexes\n",
        "    def get_train_val(self,valid):\n",
        "      batches=self.get_batch_indexes()\n",
        "      train=[]\n",
        "      val=[]\n",
        "      for batch in batches:\n",
        "        #print(type(batch))\n",
        "        random.shuffle(batch)\n",
        "        valbatch=batch[0:int(valid*len(batch))]\n",
        "        trainbatch=batch[int(valid*len(batch)):]\n",
        "        train.append(trainbatch)\n",
        "        val.append(valbatch)\n",
        "      return train,val\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import time\n",
        "import os\n",
        "import argparse\n",
        "import numpy as np\n",
        "\n",
        "def tensor2im(input_image, imtype=np.uint8):\n",
        "    #CIFAR100\n",
        "    mean = [0.5071, 0.4867, 0.4408] \n",
        "    std = [0.2675, 0.2565, 0.2761]\n",
        "    #CIFAR10\n",
        "    #mean = [0.4914, 0.4822, 0.4465]\n",
        "    #std = [0.2023, 0.1994, 0.2010]\n",
        "    if not isinstance(input_image, np.ndarray):\n",
        "        if isinstance(input_image, torch.Tensor):\n",
        "            image_tensor = input_image.data\n",
        "        else:\n",
        "            return input_image\n",
        "        image_numpy = image_tensor.cpu().detach().float().numpy()\n",
        "        if image_numpy.shape[0] == 1:\n",
        "            image_numpy = np.tile(image_numpy, (3, 1, 1))\n",
        "        for i in range(len(mean)): \n",
        "            image_numpy[i] = image_numpy[i] * std[i] + mean[i]\n",
        "        image_numpy = image_numpy * 255\n",
        "        image_numpy = np.transpose(image_numpy, (1, 2, 0))\n",
        "    else:\n",
        "        image_numpy = input_image\n",
        "    return image_numpy.astype(imtype)"
      ],
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5O4jUchQ1EAI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6aea21e5-55ab-4406-dbc3-b474a9d5f7c0"
      },
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import math\n",
        "from sklearn.preprocessing import normalize\n",
        "import copy\n",
        "import torchvision.datasets as dsets\n",
        "import torchvision.models as models\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.gridspec as gridspec\n",
        "from torch.utils.data import Subset, DataLoader, Dataset\n",
        "import random\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pickle\n",
        "import pandas as pd\n",
        "\n",
        "import import_ipynb\n",
        "#from IncrementalLearning.cifar100 import ilCIFAR100\n",
        "\n",
        "from IncrementalLearning.resnet_cifar import resnet32\n",
        "from tqdm.notebook import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "importing Jupyter notebook from /content/IncrementalLearning/resnet_cifar.ipynb\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "id": "RAoV7Gq_aBW3",
        "outputId": "56828663-f8d3-46eb-b559-cc6aeeb319d4"
      },
      "source": [
        "class mnemonics():\n",
        "  def __init__(self, randomseed):\n",
        "    self.device = 'cuda'\n",
        "    self.model = resnet32(num_classes=100).to(self.device)\n",
        "    self.feature_extractor = self.model.features\n",
        "    self.temp_model = None\n",
        "    self.lr = 2\n",
        "    self.gamma = 0.2\n",
        "    self.weight_decay = 1e-5 \n",
        "    self.milestones = [49,63]\n",
        "    self.batch_size = 128\n",
        "    self.numepochs = 70\n",
        "    self.n_classes = 0\n",
        "    self.n_known = 0\n",
        "    self.feature_size=64\n",
        "    self.momentum=0.9\n",
        "    self.criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    self.NUM_BATCHES=10\n",
        "    self.randomseed=randomseed\n",
        "    self.trainloader=None\n",
        "    self.testloader=None\n",
        "    self.CLASSES_PER_BATCH=10\n",
        "\n",
        "    self.original_training_set = ilCIFAR100(self.CLASSES_PER_BATCH, self.randomseed, train = 'train')\n",
        "    self.original_exemplar_set = ilCIFAR100(self.CLASSES_PER_BATCH, self.randomseed, train = 'exemplars')\n",
        "    self.original_test_set = ilCIFAR100(self.CLASSES_PER_BATCH,self.randomseed, train= 'test')\n",
        "\n",
        "    self.last_test = None\n",
        "    self.y_pred = []\n",
        "    self.y_test = []\n",
        "\n",
        "    self.cumulative_class_mean = []\n",
        "\n",
        "    self.classes_seen=0\n",
        "    self.diz = self.original_training_set.get_dict()\n",
        "\n",
        "    self.exemplar_features_mean = None\n",
        "    # lista di liste, ogni lista contiene gli exemplars di una classe\n",
        "    self.exemplar_sets_idxs = [] # mn_exemplat_sets\n",
        "    # lista unica, tutti gli indici degli exemplar\n",
        "    self.exemplar_idxs = []\n",
        "\n",
        "  '''\n",
        "  def update_params(self, \n",
        "                    m,\n",
        "                    finetuning_idxs, \n",
        "                    training_idxs, \n",
        "                    mnemonics_to_optimize, \n",
        "                    batch_size,\n",
        "                    new=True,\n",
        "                    lr=10, \n",
        "                    momentum=0.9, \n",
        "                    weight_decay=1e-5, \n",
        "                    milestones=[10, 20, 30, 40],\n",
        "                    gamma=0.5, \n",
        "                    tuning_epochs=4,\n",
        "                    updating_epochs=50):\n",
        "    \n",
        "    \"\"\"\n",
        "    finetuning_idxs = indexes of current task elements\n",
        "    mnemonics_idxs = indexes of exemplar elements\n",
        "    mnemonics_to_optimize = the optimized parameters in the update phase\n",
        "    \"\"\"\n",
        "\n",
        "    # make a copy of the model\n",
        "    model_copy = copy.deepcopy(self.model)\n",
        "    model_copy.train()\n",
        "    model_copy.to(self.device)\n",
        "\n",
        "    # define the loss\n",
        "    # criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    # FINE TUNING FOR 1 EPOCH eq. 8 --------------------------------------------\n",
        "    \n",
        "    # define optimizer and scheduler for fine tuning phase\n",
        "    optimizer = optim.SGD(model_copy.parameters(), lr=2, momentum=momentum, weight_decay=weight_decay)\n",
        "    \n",
        "    # create the subset dataset to load the data you want, and the loader\n",
        "    finetuning_labels = np.array([self.original_training_set.__getitem__(idx)[2] for idx in finetuning_idxs], dtype=int)\n",
        "    meta_idxs = [i for i in range(len(finetuning_idxs))]\n",
        "    random.shuffle(meta_idxs)\n",
        "\n",
        "    # split the meta idxs in batches\n",
        "    n_batches = int(np.floor(len(finetuning_idxs)/batch_size))\n",
        "    meta_idxs_batches = []\n",
        "    for i in range(n_batches):\n",
        "      meta_idxs_batches.append(np.array(meta_idxs[batch_size*i:batch_size*(i+1)]))\n",
        "    meta_idxs_batches.append(np.array(meta_idxs[batch_size*n_batches:]))\n",
        "\n",
        "    # now fine tune the copied model\n",
        "    for epoch in range(tuning_epochs):\n",
        "      for meta_idxs_batch in meta_idxs_batches:\n",
        "        inputs = mnemonics_to_optimize[0][meta_idxs_batch] # are already in cuda\n",
        "        labels = finetuning_labels[meta_idxs_batch]\n",
        "        labels = torch.tensor([self.diz[c] for c in labels])\n",
        "        labels = labels.to(self.device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model_copy(inputs)\n",
        "        labels_encoded = F.one_hot(labels,100).float().to(self.device)\n",
        "        loss = self.criterion(outputs, labels_encoded)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    \n",
        "\n",
        "    # UPDATE THE MNEMONICS eq.9/10 ---------------------------------------------\n",
        "    \n",
        "    model_copy.eval()\n",
        "    \n",
        "    optimizer = optim.SGD(mnemonics_to_optimize, lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
        "    scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=milestones, gamma=gamma)\n",
        "    \n",
        "\n",
        "    if new:\n",
        "      exlvl_training = Subset(self.original_training_set, training_idxs)\n",
        "      exlvl_loader = DataLoader(exlvl_training, batch_size=batch_size, shuffle=True, num_workers=4)\n",
        "\n",
        "      current_task_labels = set([self.original_training_set.__getitem__(idx)[2] for idx in training_idxs])\n",
        "      new_dict = {label:new_label for label, new_label in zip(current_task_labels, range(10))}\n",
        "\n",
        "      new_class_mean = {new_dict[key] : value for key, value in self.cumulative_class_mean.items()}\n",
        "      means_ready = torch.Tensor(list(new_class_mean.values())).to(self.device)\n",
        "\n",
        "    \n",
        "    else:\n",
        "      exlvl_training = Subset(self.original_exemplar_set, training_idxs)\n",
        "      exlvl_loader = DataLoader(exlvl_training, batch_size=batch_size, shuffle=True, num_workers=4)\n",
        "\n",
        "    print('lunghezza del exemplar update:', len(training_idxs))\n",
        "    for epoch in tqdm(range(updating_epochs)):\n",
        "\n",
        "      for _, inputs, labels in exlvl_loader:\n",
        "\n",
        "        if new:\n",
        "          labels = torch.tensor([new_dict[c.item()] for c in labels])\n",
        "        else:\n",
        "          labels = torch.tensor([self.diz[c.item()] for c in labels])\n",
        "\n",
        "        labels = labels.to(self.device)\n",
        "        inputs = inputs.to(self.device)\n",
        "        optimizer.zero_grad()\n",
        "        out_features = model_copy.features(inputs)\n",
        "        # compute features mean of mnemonics for each class\n",
        "        if new:\n",
        "          the_logits = F.linear(F.normalize(out_features, p=2, dim=1), F.normalize(means_ready, p=2, dim=1))\n",
        "        else:\n",
        "          ##Da capire questa cosa!!!!!!! se provassi a mettere self?\n",
        "          #OPPURE uso un altro tensore copiando all_class_means_calcolato_fuori?  \n",
        "          #dov'Ã¨ il gradiente?\n",
        "          n_classes = int(len(finetuning_labels)/m)\n",
        "          all_class_means = torch.zeros((0, 64))\n",
        "          all_class_means = all_class_means.to(self.device)\n",
        "          for i in range(n_classes): # how many classes\n",
        "            mnemonics_features = model_copy.features(mnemonics_to_optimize[0][i*m:(i+1)*m])\n",
        "            this_class_means = torch.mean(mnemonics_features, dim=0) # size 64\n",
        "            this_class_means = torch.unsqueeze(this_class_means, dim=0) # add the second dimension\n",
        "            all_class_means = torch.cat((all_class_means, this_class_means), dim=0)\n",
        "          the_logits = F.linear(F.normalize(out_features, p=2, dim=1), F.normalize(all_class_means, p=2, dim=1))\n",
        "\n",
        "        #labels_encoded = F.one_hot(labels,100).float().cuda()\n",
        "\n",
        "        loss = F.cross_entropy(the_logits, labels) # al secondo batch di classi per i new mnemonics le uscite sono sempre 10 ma le label vanno da 10 a 19\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "  def exemplar_level_optimization(self, m, task_num, current_task_indices):  \n",
        "    \n",
        "    # UPDATING NEW EXEMPLAR-----------------------------------------------------\n",
        "\n",
        "    # isola gli indici dei nuovi exemplars\n",
        "    new_exemplar_idxs = []\n",
        "    for idxs in self.exemplar_sets_idxs[-10:]:\n",
        "      new_exemplar_idxs += idxs\n",
        "\n",
        "    # ora ottieni gli mnemonics che poi sono da ottimizzare\n",
        "    new_mnemonics_data = torch.zeros((10*m, 3, 32, 32))\n",
        "\n",
        "    for i, idx in enumerate(new_exemplar_idxs):\n",
        "      new_mnemonics_data[i, :, :, :] = self.original_training_set.__getitem__(int(idx))[1]\n",
        "\n",
        "    new_mnemonics = nn.ParameterList()\n",
        "    new_mnemonics.append(nn.Parameter(new_mnemonics_data))\n",
        "    new_mnemonics.to(self.device)\n",
        "    \n",
        "    #print(new_mnemonics[0][0])\n",
        "\n",
        "    options_new ={'finetuning_idxs': new_exemplar_idxs, \n",
        "                  'training_idxs': current_task_indices, \n",
        "                  'mnemonics_to_optimize':  new_mnemonics,  \n",
        "                  'batch_size':128,\n",
        "                  'm':m}\n",
        "\n",
        "    print('---start mnemonics updating---')\n",
        "\n",
        "    self.update_params(**options_new)    \n",
        "\n",
        "    for i, idx in enumerate(new_exemplar_idxs):\n",
        "      self.original_exemplar_set.dataset.data[idx] = tensor2im(new_mnemonics[0][i])\n",
        "\n",
        "    \n",
        "    # UPDATING OLD EXEMPLARS ---------------------------------------------------\n",
        "\n",
        "    if task_num:\n",
        "      # decidi quanti elementi ha ogni exemlar set in a e in b a seconda se m Ã¨ \n",
        "      # pari o dispari\n",
        "      if m%2:\n",
        "        l_a = int((m+1)/2)\n",
        "      else:\n",
        "        l_a = int(m/2)\n",
        "      l_b = int(m-l_a)\n",
        "\n",
        "      # isola gli indici dei vecchi exemplars, dividendoli in due parti\n",
        "      # ogni classe deve avere circa la metÃ  degli exemplar originali\n",
        "      old_exemplar_idxs_a = []\n",
        "      old_exemplar_idxs_b = []\n",
        "      \n",
        "      for idxs in self.exemplar_sets_idxs[:-10]:\n",
        "        old_exemplar_idxs_a += idxs[:l_a]\n",
        "        old_exemplar_idxs_b += idxs[l_a:]\n",
        "\n",
        "      old_mnemonics_data_a = torch.zeros((task_num*10*l_a, 3, 32, 32))\n",
        "      old_mnemonics_data_b = torch.zeros((task_num*10*l_b, 3, 32, 32))\n",
        "\n",
        "      for i, idx in enumerate(old_exemplar_idxs_a):\n",
        "        old_mnemonics_data_a[i, :, :, :] = self.original_training_set.__getitem__(int(idx))[1]\n",
        "      \n",
        "      for i, idx in enumerate(old_exemplar_idxs_b):\n",
        "          old_mnemonics_data_b[i, :, :, :] = self.original_training_set.__getitem__(int(idx))[1]\n",
        "      \n",
        "      old_mnemonics_a = nn.ParameterList()\n",
        "      old_mnemonics_a.append(nn.Parameter(old_mnemonics_data_a))\n",
        "      old_mnemonics_a.to(self.device)\n",
        "      old_mnemonics_b = nn.ParameterList()\n",
        "      old_mnemonics_b.append(nn.Parameter(old_mnemonics_data_b))\n",
        "      old_mnemonics_b.to(self.device)\n",
        "\n",
        "      options_old_a = {'finetuning_idxs':old_exemplar_idxs_a, \n",
        "                       'training_idxs':old_exemplar_idxs_b, \n",
        "                       'mnemonics_to_optimize':old_mnemonics_a, \n",
        "                       'batch_size':128,\n",
        "                       'm': l_a,\n",
        "                       'new':False}\n",
        "\n",
        "      options_old_b = {'finetuning_idxs':old_exemplar_idxs_b, \n",
        "                       'training_idxs':old_exemplar_idxs_a, \n",
        "                       'mnemonics_to_optimize':old_mnemonics_b, \n",
        "                       'batch_size':128,\n",
        "                       'm':l_b,\n",
        "                       'new':False}\n",
        "\n",
        "      self.update_params(**options_old_a) \n",
        "      self.update_params(**options_old_b)\n",
        "\n",
        "      # CONVERT AND STORE UPDATED EXEMPLAR as numpy array\n",
        "\n",
        "      for i, idx in enumerate(old_exemplar_idxs_a):\n",
        "        self.original_exemplar_set.dataset.data[idx] = tensor2im(old_mnemonics_a[0][i])\n",
        "\n",
        "      for i, idx in enumerate(old_exemplar_idxs_b):\n",
        "        self.original_exemplar_set.dataset.data[idx] = tensor2im(old_mnemonics_b[0][i])\n",
        "    \n",
        "\n",
        "\n",
        "    # FINE TUNE THE CURRENT NET ON ALL THE EXEMPLARS COLLECTED 'TILL NOW\n",
        "   \n",
        "  '''\n",
        "\n",
        "  def model_level_optimization(self):\n",
        "    \n",
        "    old_model = copy.deepcopy(self.model)\n",
        "    old_model.eval()\n",
        "    old_model.to(self.device)\n",
        "    n_classes = self.classes_seen+self.CLASSES_PER_BATCH\n",
        "    print(n_classes)\n",
        "    optimizer = optim.SGD(self.model.parameters(), lr=self.lr, momentum=self.momentum, weight_decay=self.weight_decay)\n",
        "    scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=self.milestones, gamma=self.gamma)\n",
        "    for epoch in tqdm(range(self.numepochs)):\n",
        "        \n",
        "      for _, inputs, labels in self.trainloader:\n",
        "        inputs = inputs.float().to(self.device)\n",
        "        labels = torch.tensor([self.diz[c.item()] for c in labels])\n",
        "\n",
        "        labels=labels.to(self.device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs=self.model(inputs)\n",
        "\n",
        "        labels_encoded = F.one_hot(labels,100).float().to(self.device) #CAMBIARE ONE_HOT\n",
        "        \n",
        "        if self.classes_seen:\n",
        "          old_target = old_model(inputs).to(self.device)\n",
        "          old_target = torch.sigmoid(old_target).to(self.device)\n",
        "          \n",
        "          target = torch.cat((old_target[:,:self.classes_seen], labels_encoded[:, self.classes_seen:]), dim=1)\n",
        "          loss = self.criterion(outputs, target)\n",
        "        else:\n",
        "          loss = self.criterion(outputs, labels_encoded) \n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "      \n",
        "      scheduler.step()\n",
        "\n",
        "\n",
        "  def classify_nme(self, input_batch):\n",
        "    min_distances = float('inf')*torch.ones(len(input_batch)).to(self.device) # shape: batch_size --> 128\n",
        "    y_pred = torch.zeros(len(input_batch), dtype=torch.int8).to(self.device) # shape: batch_size --> 128\n",
        "    input_features = self.model.features(input_batch) # shape: (batch_size, feature_size) --> (128, 64)\n",
        "\n",
        "    for i in range(len(self.exemplar_sets_idxs)):\n",
        "      ex_mean = self.exemplar_means[i,:]\n",
        "\n",
        "      # compute distances between inputs features and exemplar set means\n",
        "      pdist = nn.PairwiseDistance(p=2)\n",
        "      distances = pdist(input_features, ex_mean) # shape: batch_size --> 128\n",
        "\n",
        "      # update min distancies and predicted labels\n",
        "      mask = distances < min_distances\n",
        "      min_distances[mask] = distances[mask]\n",
        "      y_pred[mask] = self.exemplar_labels[i]\n",
        "\n",
        "    return y_pred\n",
        "    \n",
        "\n",
        "\n",
        "  def get_new_exemplars(self, batch, m):\n",
        "    loader = torch.utils.data.DataLoader(batch, batch_size=self.batch_size,shuffle=False, num_workers=4)\n",
        "    features = np.zeros((0,self.feature_size))\n",
        "    indices = np.zeros((0), dtype=int)\n",
        "    with torch.no_grad():\n",
        "      for indexes, images, labels in loader:\n",
        "        images = images.cuda()\n",
        "        feature = self.feature_extractor(images).data.cpu().numpy()\n",
        "        feature = normalize(feature, axis=1, norm='l2')\n",
        "        features = np.concatenate((features,feature), axis=0)\n",
        "        indices = np.concatenate((indices,indexes), axis=0)\n",
        "\n",
        "    class_mean = np.mean(features, axis=0)\n",
        "    class_mean = class_mean / np.linalg.norm(class_mean)  # Normalize\n",
        "\n",
        "    #self.cumulative_class_mean.append(class_mean)\n",
        "\n",
        "    exemplar_set = []\n",
        "    exemplar_features = np.zeros((0,self.feature_size))\n",
        "\n",
        "    for k in range(1, int(m)+1):\n",
        "        S = np.sum(exemplar_features, axis=0)\n",
        "        phi = features\n",
        "        mu = class_mean\n",
        "        mu_p = 1.0 / k * (phi + S)\n",
        "        mu_p = normalize(mu_p, axis=1, norm='l2')\n",
        "        i = np.argmin(np.sqrt(np.sum((mu - mu_p) ** 2, axis=1)))\n",
        "        exemplar_set.append(int(indices[i]))\n",
        "        addfeature =  np.expand_dims(features[i], axis=0)\n",
        "        exemplar_features = np.concatenate((exemplar_features,addfeature), axis=0)\n",
        "\n",
        "        #remove duplicates\n",
        "        features = np.delete(features, i, 0)\n",
        "        indices = np.delete(indices, i, 0)\n",
        "        \n",
        "    self.exemplar_sets_idxs.append(exemplar_set)\n",
        "    #self.exemplar_sets_idxs.append(random.sample(list(batch), m))\n",
        "\n",
        "\n",
        "  def reduce_old_exemplars(self, m):\n",
        "    for i, set_i in enumerate(self.exemplar_sets_idxs):\n",
        "      self.exemplar_sets_idxs[i] = random.sample(set_i, m)\n",
        "\n",
        "  def __accuracy_fc(self, dl, mapper):\n",
        "    total = 0.0\n",
        "    correct = 0.0\n",
        "    for  _, images, labels in dl:\n",
        "      labels = torch.tensor([torch.tensor(mapper[c.item()]) for c in labels])\n",
        "      labels = labels.to(self.device)\n",
        "      images = images.to(self.device)\n",
        "      outputs = self.model(images)\n",
        "      _, preds = torch.max(outputs, dim=1)\n",
        "      total += len(labels)\n",
        "      correct += torch.sum(preds == labels).item()\n",
        "\n",
        "    acc = correct / total\n",
        "    return acc\n",
        "\n",
        "\n",
        "  def __accuracy_nme(self, dl):\n",
        "    \n",
        "    total = 0.0\n",
        "    correct = 0.0\n",
        "    \n",
        "    for  _, images, labels in dl:\n",
        "      labels = labels.to(self.device)\n",
        "      images = images.to(self.device)\n",
        "      preds = self.classify_nme(images)\n",
        "      total += len(labels)\n",
        "      correct += torch.sum(preds == labels).item()\n",
        "\n",
        "      if self.last_test:\n",
        "        self.y_pred += preds.tolist()\n",
        "        self.y_test += labels.tolist()\n",
        "\n",
        "    acc = correct / total\n",
        "    return acc\n",
        "\n",
        "\n",
        "  def plot_confusion_matrix(self):\n",
        " \n",
        "    cm = confusion_matrix(self.y_test, self.y_pred)\n",
        "    cm = np.log(cm+1)\n",
        "    fig, ax = plt.subplots(figsize=(7,7))\n",
        "    sns.heatmap(cm, square=True, cbar=False, ax=ax, cmap=plt.get_cmap('seismic'))\n",
        "    ax.set_xticks(np.linspace(19,99,5))\n",
        "    ax.set_yticks(np.linspace(19,99,5))\n",
        "    ax.set_xticklabels([20,40,60,80,100], rotation=0)\n",
        "    ax.set_yticklabels([20,40,60,80,100], rotation=0)\n",
        "    ax.set_title(\"iCaRL\")\n",
        "    ax.set_xlabel(\"Predicted class\")\n",
        "    ax.set_ylabel(\"True class\")\n",
        "    plt.savefig(\"iCaRL_\"+str(self.randomseed)+\"_cm.png\")\n",
        "    plt.show()\n",
        "    return cm\n",
        "\n",
        "  def plot_data(self, train_dl):\n",
        "\n",
        "    from sklearn.manifold import TSNE\n",
        "    print('------plot data------')\n",
        "\n",
        "    #Data points\n",
        "    train_labels_array = torch.zeros(0).to('cuda')\n",
        "    train_dataset_to_reduce = np.zeros((0,64), dtype = float)\n",
        "\n",
        "    for  _, images, labels in train_dl:\n",
        "      labels = labels.to(self.device)\n",
        "      images = images.to(self.device)\n",
        "      train_dataset_to_reduce = np.concatenate((train_dataset_to_reduce, self.feature_extractor(images).cpu().detach().numpy()))\n",
        "      train_labels_array = torch.cat((train_labels_array, labels))\n",
        "\n",
        "    \n",
        "    #EX e MN loaders \n",
        "    current_exemplar_indices = np.array([], dtype=int)\n",
        "\n",
        "    for exemplar_set in self.exemplar_sets_idxs:\n",
        "      current_exemplar_indices = np.concatenate([current_exemplar_indices, np.array(exemplar_set)])\n",
        "\n",
        "    exemplar_dataset = Subset(self.original_training_set, current_exemplar_indices)\n",
        "    ex_dl = DataLoader(exemplar_dataset, batch_size=self.batch_size, shuffle=False, num_workers=4, drop_last=True) #usato solo per plottare\n",
        "\n",
        "    mn_dataset = Subset(self.original_exemplar_set, current_exemplar_indices)\n",
        "    mn_dl = DataLoader(mn_dataset, batch_size=self.batch_size, shuffle=False, num_workers=4, drop_last=True) #usato solo per plottare\n",
        "\n",
        "\n",
        "    #Exemplars\n",
        "\n",
        "    ex_labels_array = torch.zeros(0).to('cuda')\n",
        "    ex_dataset_to_reduce = np.zeros((0,64), dtype = float)\n",
        "\n",
        "    for  _, images, labels in ex_dl:\n",
        "      labels = labels.to(self.device)\n",
        "      images = images.to(self.device)\n",
        "      ex_dataset_to_reduce = np.concatenate((ex_dataset_to_reduce, self.feature_extractor(images).cpu().detach().numpy()))\n",
        "      ex_labels_array = torch.cat((ex_labels_array, labels), dim = 0)\n",
        "\n",
        "\n",
        "    #Mnemonics\n",
        "\n",
        "\n",
        "    mn_labels_array = torch.zeros(0).to('cuda')\n",
        "    mn_dataset_to_reduce = np.zeros((0,64), dtype = float)\n",
        "\n",
        "    for  _, images, labels in mn_dl:\n",
        "      labels = labels.to(self.device)\n",
        "      images = images.to(self.device)\n",
        "      mn_dataset_to_reduce = np.concatenate((mn_dataset_to_reduce, self.feature_extractor(images).cpu().detach().numpy()))\n",
        "      mn_labels_array = torch.cat((mn_labels_array, labels), dim = 0)\n",
        "\n",
        "    #PLOT'''\n",
        "    total_data_w_exemplars = np.concatenate((train_dataset_to_reduce, ex_dataset_to_reduce))\n",
        "    total_data_w_mn =  np.concatenate((train_dataset_to_reduce, mn_dataset_to_reduce))\n",
        "\n",
        "    total_transformed_ex = TSNE(n_components=2).fit_transform(total_data_w_exemplars)\n",
        "    X_transformed_w_ex = total_transformed_ex[:train_dataset_to_reduce.shape[0]]\n",
        "    ex_transformed = total_transformed_ex[train_dataset_to_reduce.shape[0]:]\n",
        "\n",
        "    total_transformed_mn = TSNE(n_components=2).fit_transform(total_data_w_mn)\n",
        "    X_transformed_w_mn = total_transformed_mn[:train_dataset_to_reduce.shape[0]]\n",
        "    mn_transformed = total_transformed_mn[train_dataset_to_reduce.shape[0]:]\n",
        "\n",
        "    fig, (ax1, ax2) = plt.subplots(1,2, figsize=(24,12))\n",
        "    ax1.scatter(X_transformed_w_ex[:,0], X_transformed_w_ex[:,1], c = train_labels_array.cpu(), alpha = 0.1)\n",
        "    ax1.scatter(ex_transformed[:,0], ex_transformed[:,1], c = ex_labels_array.cpu(), alpha = 1)\n",
        "    #ax1.title('EXEMPLARS')\n",
        "\n",
        "    ax2.scatter(X_transformed_w_mn[:,0], X_transformed_w_mn[:,1], c = train_labels_array.cpu(), alpha = 0.1)\n",
        "    ax2.scatter(mn_transformed[:,0], mn_transformed[:,1], c = mn_labels_array.cpu(), alpha = 1)\n",
        "    #ax2.title('MNEMONICS')\n",
        "    plt.show()\n",
        "\n",
        "  def trainer(self):\n",
        "    \n",
        "    train_indices = self.original_training_set.get_batch_indexes()\n",
        "    test_indices = self.original_test_set.get_batch_indexes()\n",
        "    batches=self.original_training_set.getbatches()\n",
        "    current_test_indexes=[]\n",
        "    test_acc = []\n",
        "    self.last_test = False\n",
        "\n",
        "    for i in range(self.NUM_BATCHES):\n",
        "      print('current batches', batches[i])\n",
        "      if i == self.NUM_BATCHES-1:\n",
        "        self.last_test = True\n",
        "\n",
        "      current_exemplar_indices = np.array([], dtype=int)\n",
        "    \n",
        "      for exemplar_set in self.exemplar_sets_idxs:\n",
        "        current_exemplar_indices = np.concatenate([current_exemplar_indices, np.array(exemplar_set)])\n",
        "\n",
        "      exemplar_dataset = Subset(self.original_training_set, current_exemplar_indices) \n",
        "      #DA CAMBIARE CON SELF.ORIGINAL EXEMPLAR SET\n",
        "      if i > 1: #FINETUNING\n",
        "        print('----inizio finetuning----')\n",
        "        print('numbero of classes in the exemplar sets', len(self.exemplar_sets_idxs))\n",
        "        self.numepochs = 10\n",
        "        self.lr = 0.2\n",
        "        temporary_classes_seen = self.classes_seen\n",
        "        self.classes_seen = 0\n",
        "        self.trainloader = DataLoader(exemplar_dataset, batch_size=self.batch_size, shuffle=False, num_workers=4, drop_last=True) \n",
        "        print('accuracy on exemplar set before finetuining:', 100*self.__accuracy_fc(self.trainloader,self.diz))\n",
        "        self.model.train()\n",
        "        self.model_level_optimization()\n",
        "        #BACK TO THE NORMAL PARAMETERS\n",
        "        self.model.eval()\n",
        "        self.numepochs = 70\n",
        "        self.lr = 2\n",
        "        self.classes_seen = temporary_classes_seen\n",
        "        print('accuracy on exemplar set after finetuining:', 100*self.__accuracy_fc(self.trainloader,self.diz))\n",
        "        current_test_acc = self.__accuracy_nme(self.testloader)\n",
        "        print('accuracy on test set after finetuning:', 100*current_test_acc)\n",
        "        print('-----fine finetuning------')\n",
        "        print('-'*80)\n",
        "\n",
        "      train_dataset = Subset(self.original_training_set, train_indices[i])\n",
        "      current_test_indexes += test_indices[i].tolist()\n",
        "      test_dataset = Subset(self.original_test_set,current_test_indexes)\n",
        "      self.train_loader = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True, num_workers=4, drop_last=True)\n",
        "      self.testloader = DataLoader(test_dataset, batch_size=self.batch_size, shuffle=False, num_workers=4, drop_last=True)        \n",
        "      \n",
        "\n",
        "      if i == 0:\n",
        "        self.trainloader = self.train_loader\n",
        "      else:\n",
        "        self.trainloader = DataLoader(torch.utils.data.ConcatDataset([train_dataset, exemplar_dataset]), batch_size=self.batch_size, shuffle=True,\n",
        "          num_workers=4, pin_memory=True)\n",
        "\n",
        "        \n",
        "      self.model.train()\n",
        "      self.model_level_optimization()    \n",
        "      self.classes_seen += 10\n",
        "      self.model.eval() # Set Network to evaluation mode\n",
        "      m=int(2000/(int(i*10+10)))\n",
        "\n",
        "      \n",
        "      current_test_acc = self.__accuracy_fc(self.testloader, self.diz)\n",
        "      print('accuracy on test set:', 100*current_test_acc)\n",
        "\n",
        "      \n",
        "      return self.model, batches[i]\n",
        "\n",
        "      break\n",
        "'''\n",
        "      #NUOVO PAPER DEL PORCODDIO\n",
        "      labels_of_modified = torch.zeros(0, dtype = int).to('cuda')\n",
        "      for label in batches[i]:\n",
        "        labels = torch.LongTensor([self.diz[label]]*m).to('cuda')\n",
        "        labels_of_modified = torch.cat((labels_of_modified, labels), dim=0)\n",
        "      print('labels to be created', labels_of_modified)\n",
        "      print('len to be created', len(labels_of_modified))\n",
        "      number_of_images_created = m*10\n",
        "      net_student = resnet32(num_classes=100).to(self.device)\n",
        "      data_type = torch.float\n",
        "      inputs = torch.randn((number_of_images_created, 3, 32, 32), requires_grad=True, device='cuda', dtype=data_type)\n",
        "\n",
        "      net_student = copy.deepcopy(self.model)\n",
        "      net_student.eval() #important, otherwise generated images will be non natural\n",
        "      \n",
        "      train_writer = None  # tensorboard writter\n",
        "      global_iteration = 0\n",
        "      di_lr = 0.05\n",
        "      optimizer_di = optim.Adam([inputs], lr=di_lr)\n",
        "\n",
        "      print(\"Starting model inversion\")\n",
        "      batch_idx = 0\n",
        "      inputs = get_images(net=self.model, bs=200, epochs=2000, idx=batch_idx,\n",
        "                        net_student=net_student,\n",
        "                        train_writer=train_writer, use_amp=False,\n",
        "                        optimizer=optimizer_di, inputs=inputs, \n",
        "                        var_scale=0.00005, labels=labels)\n",
        "\n",
        "\n",
        "      plt.imshow(tensor2im(inputs[0]))\n",
        "      plt.show()\n",
        "      plt.imshow(tensor2im(inputs[55]))\n",
        "      plt.show()\n",
        "      print('deepinversion finshed')\n",
        "      # update exemplars number\n",
        "      \n",
        "\n",
        "      # reduce the number of each exemplars set\n",
        "      self.reduce_old_exemplars(m) \n",
        "\n",
        "      self.cumulative_class_mean = {}\n",
        "\n",
        "      for classlabel in batches[i]:\n",
        "        indexes_class = self.original_training_set.get_class_indexes(classlabel)\n",
        "        current_class = Subset(self.original_training_set, indexes_class)\n",
        "        #self.get_new_exemplars(indexes_class, m)\n",
        "        self.get_new_exemplars(current_class, m)\n",
        "        indexes_class = self.original_training_set.get_class_indexes(classlabel)\n",
        "        current_class = Subset(self.original_training_set, indexes_class)\n",
        "\n",
        "        loader = torch.utils.data.DataLoader(current_class, batch_size=self.batch_size,shuffle=False, num_workers=4)\n",
        "        features = np.zeros((0, 64))\n",
        "        with torch.no_grad():\n",
        "          for indexes, images, labels in loader:\n",
        "            images = images.cuda()\n",
        "            feature = self.feature_extractor(images).data.cpu().numpy()\n",
        "            feature = normalize(feature, axis=1, norm='l2')\n",
        "            features = np.concatenate((features,feature), axis=0)\n",
        "\n",
        "        class_mean = np.mean(features, axis=0)\n",
        "        class_mean = class_mean / np.linalg.norm(class_mean)  # Normalize\n",
        "\n",
        "        self.cumulative_class_mean[classlabel] = class_mean\n",
        "        \n",
        "      #plt.imshow(self.original_exemplar_set.dataset.data[self.exemplar_sets_idxs[0][0]])\n",
        "      #plt.show()\n",
        "      \n",
        "  \n",
        "      #self.exemplar_level_optimization(m, i, self.original_training_set.get_batch_indexes()[i])\n",
        "\n",
        "      # compute means of exemplar set\n",
        "      # cycle for each exemplar set\n",
        "      self.exemplar_means = torch.zeros((0, self.feature_size), dtype=torch.float).to(self.device)\n",
        "      self.exemplar_labels = []\n",
        "      for j in range(len(self.exemplar_sets_idxs)):\n",
        "        exemplars_dataset = Subset(self.original_training_set, self.exemplar_sets_idxs[j])\n",
        "        exemplars_loader = torch.utils.data.DataLoader(exemplars_dataset, batch_size=self.batch_size, shuffle=False, num_workers=4)\n",
        "        ex_features = torch.zeros((0, self.feature_size), dtype=torch.float).to(self.device) # alla fine shape: (len(exemplar_set), feature_size) --> (m, 64)\n",
        "      \n",
        "        with torch.no_grad():\n",
        "          _, _, exemplar_label = self.original_training_set.__getitem__(self.exemplar_sets_idxs[j][0]) \n",
        "          self.exemplar_labels.append(exemplar_label)\n",
        "          # cycle for each batch in the current exemplar set\n",
        "          for _,  exemplars, _ in exemplars_loader:\n",
        "          \n",
        "            # get exemplars features\n",
        "            exemplars = exemplars.to(self.device)\n",
        "            features = self.model.features(exemplars) # shape: (len(exemplars), feature_size)\n",
        "          \n",
        "            # normalize \n",
        "            feature_norms = torch.norm(features, p=2, dim=1) # shape: len(exemplars)\n",
        "            feature_norms.unsqueeze_(1) # shape: (len(exemplars), 1)\n",
        "            features = features/feature_norms\n",
        "          \n",
        "            # concatenate over columns\n",
        "            ex_features = torch.cat((ex_features, features), dim=0)\n",
        "          \n",
        "        # compute current exemplar set mean and normalize it\n",
        "        ex_mean = torch.mean(ex_features, dim=0) # shape: feature_size --> 64\n",
        "        ex_mean = ex_mean/torch.norm(ex_mean)\n",
        "        ex_mean.unsqueeze_(0) # shape: (1, feature_size) --> (1, 64)\n",
        "        self.exemplar_means = torch.cat((self.exemplar_means, ex_mean), dim=0) # shape: (n_examplar sets, feature size)\n",
        "      \n",
        "      #if i == 0:\n",
        "       # self.plot_data(self.trainloader)\n",
        "      print('accuracy on training set:', 100*self.__accuracy_fc(self.trainloader,self.diz))\n",
        "      # print('accuracy on test set:', self.__accuracy_on(self.testloader,self,self.diz))\n",
        "      current_test_acc = self.__accuracy_nme(self.testloader)\n",
        "      print('accuracy on test set:', 100*current_test_acc)\n",
        "      print('-' * 80)\n",
        "      test_acc.append(current_test_acc)\n",
        "\n",
        "    # compute comfusion matrix and save results\n",
        "    cm = self.plot_confusion_matrix()\n",
        "    with open('iCaRL_'+str(self.randomseed)+\"_cm\", 'wb') as file:\n",
        "      pickle.dump(cm, file, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "    with open('iCaRL_'+str(self.randomseed)+\"_testacc\", 'wb') as file:\n",
        "      pickle.dump(test_acc, file, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "      print('PRINT IMAGES')\n",
        "      print('with data augmentation')\n",
        "      plt.imshow(self.original_training_set.dataset.data[self.exemplar_sets_idxs[0][0]])\n",
        "      plt.show()\n",
        "      print('without data augmentation')\n",
        "      plt.imshow(self.original_exemplar_set.dataset.data[self.exemplar_sets_idxs[0][0]])\n",
        "      plt.show()\n",
        "'''"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n      #NUOVO PAPER DEL PORCODDIO\\n      labels_of_modified = torch.zeros(0, dtype = int).to(\\'cuda\\')\\n      for label in batches[i]:\\n        labels = torch.LongTensor([self.diz[label]]*m).to(\\'cuda\\')\\n        labels_of_modified = torch.cat((labels_of_modified, labels), dim=0)\\n      print(\\'labels to be created\\', labels_of_modified)\\n      print(\\'len to be created\\', len(labels_of_modified))\\n      number_of_images_created = m*10\\n      net_student = resnet32(num_classes=100).to(self.device)\\n      data_type = torch.float\\n      inputs = torch.randn((number_of_images_created, 3, 32, 32), requires_grad=True, device=\\'cuda\\', dtype=data_type)\\n\\n      net_student = copy.deepcopy(self.model)\\n      net_student.eval() #important, otherwise generated images will be non natural\\n      \\n      train_writer = None  # tensorboard writter\\n      global_iteration = 0\\n      di_lr = 0.05\\n      optimizer_di = optim.Adam([inputs], lr=di_lr)\\n\\n      print(\"Starting model inversion\")\\n      batch_idx = 0\\n      inputs = get_images(net=self.model, bs=200, epochs=2000, idx=batch_idx,\\n                        net_student=net_student,\\n                        train_writer=train_writer, use_amp=False,\\n                        optimizer=optimizer_di, inputs=inputs, \\n                        var_scale=0.00005, labels=labels)\\n\\n\\n      plt.imshow(tensor2im(inputs[0]))\\n      plt.show()\\n      plt.imshow(tensor2im(inputs[55]))\\n      plt.show()\\n      print(\\'deepinversion finshed\\')\\n      # update exemplars number\\n      \\n\\n      # reduce the number of each exemplars set\\n      self.reduce_old_exemplars(m) \\n\\n      self.cumulative_class_mean = {}\\n\\n      for classlabel in batches[i]:\\n        indexes_class = self.original_training_set.get_class_indexes(classlabel)\\n        current_class = Subset(self.original_training_set, indexes_class)\\n        #self.get_new_exemplars(indexes_class, m)\\n        self.get_new_exemplars(current_class, m)\\n        indexes_class = self.original_training_set.get_class_indexes(classlabel)\\n        current_class = Subset(self.original_training_set, indexes_class)\\n\\n        loader = torch.utils.data.DataLoader(current_class, batch_size=self.batch_size,shuffle=False, num_workers=4)\\n        features = np.zeros((0, 64))\\n        with torch.no_grad():\\n          for indexes, images, labels in loader:\\n            images = images.cuda()\\n            feature = self.feature_extractor(images).data.cpu().numpy()\\n            feature = normalize(feature, axis=1, norm=\\'l2\\')\\n            features = np.concatenate((features,feature), axis=0)\\n\\n        class_mean = np.mean(features, axis=0)\\n        class_mean = class_mean / np.linalg.norm(class_mean)  # Normalize\\n\\n        self.cumulative_class_mean[classlabel] = class_mean\\n        \\n      #plt.imshow(self.original_exemplar_set.dataset.data[self.exemplar_sets_idxs[0][0]])\\n      #plt.show()\\n      \\n  \\n      #self.exemplar_level_optimization(m, i, self.original_training_set.get_batch_indexes()[i])\\n\\n      # compute means of exemplar set\\n      # cycle for each exemplar set\\n      self.exemplar_means = torch.zeros((0, self.feature_size), dtype=torch.float).to(self.device)\\n      self.exemplar_labels = []\\n      for j in range(len(self.exemplar_sets_idxs)):\\n        exemplars_dataset = Subset(self.original_training_set, self.exemplar_sets_idxs[j])\\n        exemplars_loader = torch.utils.data.DataLoader(exemplars_dataset, batch_size=self.batch_size, shuffle=False, num_workers=4)\\n        ex_features = torch.zeros((0, self.feature_size), dtype=torch.float).to(self.device) # alla fine shape: (len(exemplar_set), feature_size) --> (m, 64)\\n      \\n        with torch.no_grad():\\n          _, _, exemplar_label = self.original_training_set.__getitem__(self.exemplar_sets_idxs[j][0]) \\n          self.exemplar_labels.append(exemplar_label)\\n          # cycle for each batch in the current exemplar set\\n          for _,  exemplars, _ in exemplars_loader:\\n          \\n            # get exemplars features\\n            exemplars = exemplars.to(self.device)\\n            features = self.model.features(exemplars) # shape: (len(exemplars), feature_size)\\n          \\n            # normalize \\n            feature_norms = torch.norm(features, p=2, dim=1) # shape: len(exemplars)\\n            feature_norms.unsqueeze_(1) # shape: (len(exemplars), 1)\\n            features = features/feature_norms\\n          \\n            # concatenate over columns\\n            ex_features = torch.cat((ex_features, features), dim=0)\\n          \\n        # compute current exemplar set mean and normalize it\\n        ex_mean = torch.mean(ex_features, dim=0) # shape: feature_size --> 64\\n        ex_mean = ex_mean/torch.norm(ex_mean)\\n        ex_mean.unsqueeze_(0) # shape: (1, feature_size) --> (1, 64)\\n        self.exemplar_means = torch.cat((self.exemplar_means, ex_mean), dim=0) # shape: (n_examplar sets, feature size)\\n      \\n      #if i == 0:\\n       # self.plot_data(self.trainloader)\\n      print(\\'accuracy on training set:\\', 100*self.__accuracy_fc(self.trainloader,self.diz))\\n      # print(\\'accuracy on test set:\\', self.__accuracy_on(self.testloader,self,self.diz))\\n      current_test_acc = self.__accuracy_nme(self.testloader)\\n      print(\\'accuracy on test set:\\', 100*current_test_acc)\\n      print(\\'-\\' * 80)\\n      test_acc.append(current_test_acc)\\n\\n    # compute comfusion matrix and save results\\n    cm = self.plot_confusion_matrix()\\n    with open(\\'iCaRL_\\'+str(self.randomseed)+\"_cm\", \\'wb\\') as file:\\n      pickle.dump(cm, file, protocol=pickle.HIGHEST_PROTOCOL)\\n    with open(\\'iCaRL_\\'+str(self.randomseed)+\"_testacc\", \\'wb\\') as file:\\n      pickle.dump(test_acc, file, protocol=pickle.HIGHEST_PROTOCOL)\\n\\n      print(\\'PRINT IMAGES\\')\\n      print(\\'with data augmentation\\')\\n      plt.imshow(self.original_training_set.dataset.data[self.exemplar_sets_idxs[0][0]])\\n      plt.show()\\n      print(\\'without data augmentation\\')\\n      plt.imshow(self.original_exemplar_set.dataset.data[self.exemplar_sets_idxs[0][0]])\\n      plt.show()\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HzqwQeHB1Tg-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12f829e9-a9fd-4f5d-ad8a-e14bbf46cb1f"
      },
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "from __future__ import unicode_literals\n",
        "\n",
        "import argparse\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "# import torch.nn.parallel\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.optim as optim\n",
        "# import torch.utils.data\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.utils as vutils\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import glob\n",
        "import collections\n",
        "\n",
        "#from resnet_cifar import ResNet34, ResNet18\n",
        "\n",
        "try:\n",
        "    from apex.parallel import DistributedDataParallel as DDP\n",
        "    from apex import amp, optimizers\n",
        "    USE_APEX = True\n",
        "except ImportError:\n",
        "    print(\"Please install apex from https://www.github.com/nvidia/apex to run this example.\")\n",
        "    print(\"will attempt to run without it\")\n",
        "    USE_APEX = False\n",
        "\n",
        "#provide intermeiate information\n",
        "debug_output = False\n",
        "debug_output = True\n",
        "\n",
        "\n",
        "class DeepInversionFeatureHook():\n",
        "    '''\n",
        "    Implementation of the forward hook to track feature statistics and compute a loss on them.\n",
        "    Will compute mean and variance, and will use l2 as a loss\n",
        "    '''\n",
        "\n",
        "    def __init__(self, module):\n",
        "        self.hook = module.register_forward_hook(self.hook_fn)\n",
        "\n",
        "    def hook_fn(self, module, input, output):\n",
        "        # hook co compute deepinversion's feature distribution regularization\n",
        "        nch = input[0].shape[1]\n",
        "\n",
        "        mean = input[0].mean([0, 2, 3])\n",
        "        var = input[0].permute(1, 0, 2, 3).contiguous().view([nch, -1]).var(1, unbiased=False)\n",
        "\n",
        "        # forcing mean and variance to match between two distributions\n",
        "        # other ways might work better, e.g. KL divergence\n",
        "        r_feature = torch.norm(module.running_var.data.type(var.type()) - var, 2) + torch.norm(\n",
        "            module.running_mean.data.type(var.type()) - mean, 2)\n",
        "\n",
        "        self.r_feature = r_feature\n",
        "        # must have no output\n",
        "\n",
        "    def close(self):\n",
        "        self.hook.remove()\n",
        "\n",
        "def get_images(net, bs=256, epochs=1000, idx=-1, var_scale=0.00005, competitive_scale=0.01,\n",
        "               net_student=None, prefix=None, train_writer = None, global_iteration=None,\n",
        "               use_amp=False, bn_reg_scale = 0.0,\n",
        "               optimizer = None, inputs = None, labels = False, l2_coeff=0.0):\n",
        "    '''\n",
        "    Function returns inverted images from the pretrained model, parameters are tight to CIFAR dataset\n",
        "    args in:\n",
        "        net: network to be inverted\n",
        "        bs: batch size\n",
        "        epochs: total number of iterations to generate inverted images, training longer helps a lot!\n",
        "        idx: an external flag for printing purposes: only print in the first round, set as -1 to disable\n",
        "        var_scale: the scaling factor for variance loss regularization. this may vary depending on bs\n",
        "            larger - more blurred but less noise\n",
        "        net_student: model to be used for Adaptive DeepInversion\n",
        "        prefix: defines the path to store images\n",
        "        competitive_scale: coefficient for Adaptive DeepInversion\n",
        "        train_writer: tensorboardX object to store intermediate losses\n",
        "        global_iteration: indexer to be used for tensorboard\n",
        "        use_amp: boolean to indicate usage of APEX AMP for FP16 calculations - twice faster and less memory on TensorCores\n",
        "        optimizer: potimizer to be used for model inversion\n",
        "        inputs: data place holder for optimization, will be reinitialized to noise\n",
        "        bn_reg_scale: weight for r_feature_regularization\n",
        "        random_labels: sample labels from random distribution or use columns of the same class\n",
        "        l2_coeff: coefficient for L2 loss on input\n",
        "    return:\n",
        "        A tensor on GPU with shape (bs, 3, 32, 32) for CIFAR\n",
        "    '''\n",
        "\n",
        "    kl_loss = nn.KLDivLoss(reduction='batchmean').cuda()\n",
        "\n",
        "    # preventing backpropagation through student for Adaptive DeepInversion\n",
        "    net_student.eval()\n",
        "\n",
        "    best_cost = 1e6\n",
        "\n",
        "    # initialize gaussian inputs\n",
        "    inputs.data = torch.randn((bs, 3, 32, 32), requires_grad=True, device='cuda')\n",
        "    # if use_amp:\n",
        "    #     inputs.data = inputs.data.half()\n",
        "\n",
        "    # set up criteria for optimization\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    optimizer.state = collections.defaultdict(dict)  # Reset state of optimizer\n",
        "\n",
        "    # target outputs to generate\n",
        "    #if labels:\n",
        "    targets = labels\n",
        "    #else:\n",
        "     #   targets = torch.LongTensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9] * 25 + [0, 1, 2, 3, 4, 5]).to('cuda')\n",
        "\n",
        "    outputs=net(inputs.data)\n",
        "    _, predicted_teach = outputs.max(1)\n",
        "\n",
        "    outputs_student=net_student(inputs.data)\n",
        "    _, predicted_std = outputs_student.max(1)\n",
        "\n",
        "    if idx == 0:\n",
        "        print('Teacher correct out of {}: {}, loss at {}'.format(bs, predicted_teach.eq(targets).sum().item(), criterion(outputs, targets).item()))\n",
        "        print('Student correct out of {}: {}, loss at {}'.format(bs, predicted_std.eq(targets).sum().item(), criterion(outputs_student, targets).item()))\n",
        "\n",
        "    ## Create hooks for feature statistics catching\n",
        "    loss_r_feature_layers = []\n",
        "    for module in net.modules():\n",
        "        if isinstance(module, nn.BatchNorm2d):\n",
        "            loss_r_feature_layers.append(DeepInversionFeatureHook(module))\n",
        "\n",
        "    # setting up the range for jitter\n",
        "    lim_0, lim_1 = 2, 2\n",
        "\n",
        "    for epoch in tqdm(range(epochs)):\n",
        "        # apply random jitter offsets\n",
        "        off1 = random.randint(-lim_0, lim_0)\n",
        "        off2 = random.randint(-lim_1, lim_1)\n",
        "        inputs_jit = torch.roll(inputs, shifts=(off1,off2), dims=(2,3))\n",
        "\n",
        "        # foward with jit images\n",
        "        optimizer.zero_grad()\n",
        "        net.zero_grad()\n",
        "        outputs = net(inputs_jit)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss_target = loss.item()\n",
        "\n",
        "        # competition loss, Adaptive DeepInvesrion\n",
        "        if competitive_scale != 0.0:\n",
        "            net_student.zero_grad()\n",
        "            outputs_student = net_student(inputs_jit)\n",
        "            T = 3.0\n",
        "\n",
        "            if 1:\n",
        "                # jensen shanon divergence:\n",
        "                # another way to force KL between negative probabilities\n",
        "                P = F.softmax(outputs_student / T, dim=1)\n",
        "                Q = F.softmax(outputs / T, dim=1)\n",
        "                M = 0.5 * (P + Q)\n",
        "\n",
        "                P = torch.clamp(P, 0.01, 0.99)\n",
        "                Q = torch.clamp(Q, 0.01, 0.99)\n",
        "                M = torch.clamp(M, 0.01, 0.99)\n",
        "                eps = 0.0\n",
        "                # loss_verifier_cig = 0.5 * kl_loss(F.log_softmax(outputs_verifier / T, dim=1), M) +  0.5 * kl_loss(F.log_softmax(outputs/T, dim=1), M)\n",
        "                loss_verifier_cig = 0.5 * kl_loss(torch.log(P + eps), M) + 0.5 * kl_loss(torch.log(Q + eps), M)\n",
        "                # JS criteria - 0 means full correlation, 1 - means completely different\n",
        "                loss_verifier_cig = 1.0 - torch.clamp(loss_verifier_cig, 0.0, 1.0)\n",
        "\n",
        "                loss = loss + competitive_scale * loss_verifier_cig\n",
        "\n",
        "        # apply total variation regularization\n",
        "        diff1 = inputs_jit[:,:,:,:-1] - inputs_jit[:,:,:,1:]\n",
        "        diff2 = inputs_jit[:,:,:-1,:] - inputs_jit[:,:,1:,:]\n",
        "        diff3 = inputs_jit[:,:,1:,:-1] - inputs_jit[:,:,:-1,1:]\n",
        "        diff4 = inputs_jit[:,:,:-1,:-1] - inputs_jit[:,:,1:,1:]\n",
        "        loss_var = torch.norm(diff1) + torch.norm(diff2) + torch.norm(diff3) + torch.norm(diff4)\n",
        "        loss = loss + var_scale*loss_var\n",
        "\n",
        "        # R_feature loss\n",
        "        loss_distr = sum([mod.r_feature for mod in loss_r_feature_layers])\n",
        "        loss = loss + bn_reg_scale*loss_distr # best for noise before BN\n",
        "\n",
        "        # l2 loss\n",
        "        if 1:\n",
        "            loss = loss + l2_coeff * torch.norm(inputs_jit, 2)\n",
        "\n",
        "        if debug_output and epoch % 200==0:\n",
        "            print(f\"It {epoch}\\t Losses: total: {loss.item():3.3f},\\ttarget: {loss_target:3.3f} \\tR_feature_loss unscaled:\\t {loss_distr.item():3.3f}\")\n",
        "            #vutils.save_image(inputs.data.clone(),\n",
        "             #                 './{}/output_{}.png'.format(prefix, epoch//200),\n",
        "              #                normalize=True, scale_each=True, nrow=10)\n",
        "\n",
        "        if best_cost > loss.item():\n",
        "            best_cost = loss.item()\n",
        "            best_inputs = inputs.data\n",
        "\n",
        "        # backward pass\n",
        "        if use_amp:\n",
        "            with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
        "                scaled_loss.backward()\n",
        "        else:\n",
        "            loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "    outputs=net(best_inputs)\n",
        "    _, predicted_teach = outputs.max(1)\n",
        "\n",
        "    outputs_student=net_student(best_inputs)\n",
        "    _, predicted_std = outputs_student.max(1)\n",
        "\n",
        "    if idx == 0:\n",
        "        print('Teacher correct out of {}: {}, loss at {}'.format(bs, predicted_teach.eq(targets).sum().item(), criterion(outputs, targets).item()))\n",
        "        print('Student correct out of {}: {}, loss at {}'.format(bs, predicted_std.eq(targets).sum().item(), criterion(outputs_student, targets).item()))\n",
        "\n",
        "    name_use = \"best_images\"\n",
        "    if prefix is not None:\n",
        "        name_use = prefix + name_use\n",
        "    next_batch = len(glob.glob(\"./%s/*.png\" % name_use)) // 1\n",
        "\n",
        "    #vutils.save_image(best_inputs[:20].clone(),\n",
        "     #                 './{}/output_{}.png'.format(name_use, next_batch),\n",
        "      #                normalize=True, scale_each = True, nrow=10)\n",
        "\n",
        "    #if train_writer is not None:\n",
        "     #   train_writer.add_scalar('gener_teacher_criteria', criterion(outputs, targets), global_iteration)\n",
        "      #  train_writer.add_scalar('gener_student_criteria', criterion(outputs_student, targets), global_iteration)\n",
        "\n",
        "       # train_writer.add_scalar('gener_teacher_acc', predicted_teach.eq(targets).sum().item() / bs, global_iteration)\n",
        "       # train_writer.add_scalar('gener_student_acc', predicted_std.eq(targets).sum().item() / bs, global_iteration)\n",
        "\n",
        "        #train_writer.add_scalar('gener_loss_total', loss.item(), global_iteration)\n",
        "        #train_writer.add_scalar('gener_loss_var', (var_scale*loss_var).item(), global_iteration)\n",
        "\n",
        "    net_student.train()\n",
        "\n",
        "    return best_inputs"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Please install apex from https://www.github.com/nvidia/apex to run this example.\n",
            "will attempt to run without it\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170,
          "referenced_widgets": [
            "8cf040765ab74dbaadad053a354daf90",
            "2d54b65c0d66461eaa32669b22d60a92",
            "d7bbbea14a4642ccb0fae794fe27452b",
            "cfcb505a079d45109b8809cb39663d9e",
            "68849cae536d4e989e533ad376b910cd",
            "3fe118bf3d6b447580d04e3d768b978a",
            "6d57853ebd9f40cfa4820a95f0f4990e",
            "dde161228c3842548624717186633725"
          ]
        },
        "id": "OYzLuYGDLr15",
        "outputId": "c8d9f4c6-5207-4d8c-f79b-b612701d055d"
      },
      "source": [
        "method = mnemonics(randomseed=203)\n",
        "model, batch = method.trainer()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "current batches [11, 5, 62, 76, 27, 3, 96, 33, 78, 30]\n",
            "10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8cf040765ab74dbaadad053a354daf90",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=70.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "accuracy on test set: 83.92857142857143\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AhzQjA4HT2wH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce4dc78a-8bec-405d-8423-ffb95693b9e7"
      },
      "source": [
        "!git clone https://github.com/huyvnphan/PyTorch_CIFAR10.git\n",
        "\n",
        "! cp -r /content/PyTorch_CIFAR10/cifar10_models/resnet.py /content\n",
        "\n",
        "!pip install pytorch_lightning\n",
        "!python /content/PyTorch_CIFAR10/train.py --download_weights 1\n",
        "\n",
        "! cp -r /content/cifar10_models/state_dicts /content"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'PyTorch_CIFAR10'...\n",
            "remote: Enumerating objects: 85, done.\u001b[K\n",
            "remote: Counting objects: 100% (85/85), done.\u001b[K\n",
            "remote: Compressing objects: 100% (63/63), done.\u001b[K\n",
            "remote: Total 637 (delta 39), reused 50 (delta 22), pack-reused 552\u001b[K\n",
            "Receiving objects: 100% (637/637), 6.59 MiB | 5.19 MiB/s, done.\n",
            "Resolving deltas: 100% (222/222), done.\n",
            "cp: cannot stat '/content/cifar10_models/state_dicts': No such file or directory\n",
            "Collecting pytorch_lightning\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/12/98/86a89dcd54f84582bbf24cb29cd104b966fcf934d92d5dfc626f225015d2/pytorch_lightning-1.1.4-py3-none-any.whl (684kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 686kB 13.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.6/dist-packages (from pytorch_lightning) (4.41.1)\n",
            "Collecting fsspec[http]>=0.8.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ec/80/72ac0982cc833945fada4b76c52f0f65435ba4d53bc9317d1c70b5f7e7d5/fsspec-0.8.5-py3-none-any.whl (98kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 102kB 13.9MB/s \n",
            "\u001b[?25hCollecting PyYAML>=5.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/64/c2/b80047c7ac2478f9501676c988a5411ed5572f35d1beff9cae07d321512c/PyYAML-5.3.1.tar.gz (269kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 276kB 27.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.6/dist-packages (from pytorch_lightning) (2.4.0)\n",
            "Collecting future>=0.17.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/45/0b/38b06fd9b92dc2b68d58b75f900e97884c45bedd2ff83203d933cf5851c9/future-0.18.2.tar.gz (829kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 829kB 40.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.6/dist-packages (from pytorch_lightning) (1.19.5)\n",
            "Requirement already satisfied: torch>=1.3 in /usr/local/lib/python3.6/dist-packages (from pytorch_lightning) (1.7.0+cu101)\n",
            "Collecting aiohttp; extra == \"http\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ad/e6/d4b6235d776c9b33f853e603efede5aac5a34f71ca9d3877adb30492eb4e/aiohttp-3.7.3-cp36-cp36m-manylinux2014_x86_64.whl (1.3MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.3MB 50.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests; extra == \"http\" in /usr/local/lib/python3.6/dist-packages (from fsspec[http]>=0.8.1->pytorch_lightning) (2.23.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (0.4.2)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.32.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.0.1)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (0.36.2)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.17.2)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.15.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (51.1.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (3.3.3)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (3.12.4)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.7.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (0.10.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch>=1.3->pytorch_lightning) (3.7.4.3)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch>=1.3->pytorch_lightning) (0.8)\n",
            "Collecting async-timeout<4.0,>=3.0\n",
            "  Downloading https://files.pythonhosted.org/packages/e1/1e/5a4441be21b0726c4464f3f23c8b19628372f606755a9d2e46c187e65ec4/async_timeout-3.0.1-py3-none-any.whl\n",
            "Collecting yarl<2.0,>=1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/da/08/52b26b44bce7b818b410aee37c5e424c9ea420c557bca97dc2adac29b151/yarl-1.6.3-cp36-cp36m-manylinux2014_x86_64.whl (293kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 296kB 60.2MB/s \n",
            "\u001b[?25hCollecting multidict<7.0,>=4.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a1/35/b22524d6b9cacfb4c5eff413a069bbc17c6ea628e54da5c6c989998ced5f/multidict-5.1.0-cp36-cp36m-manylinux2014_x86_64.whl (141kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 143kB 57.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.6/dist-packages (from aiohttp; extra == \"http\"->fsspec[http]>=0.8.1->pytorch_lightning) (20.3.0)\n",
            "Requirement already satisfied: chardet<4.0,>=2.0 in /usr/local/lib/python3.6/dist-packages (from aiohttp; extra == \"http\"->fsspec[http]>=0.8.1->pytorch_lightning) (3.0.4)\n",
            "Collecting idna-ssl>=1.0; python_version < \"3.7\"\n",
            "  Downloading https://files.pythonhosted.org/packages/46/03/07c4894aae38b0de52b52586b24bf189bb83e4ddabfe2e2c8f2419eec6f4/idna-ssl-1.1.0.tar.gz\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests; extra == \"http\"->fsspec[http]>=0.8.1->pytorch_lightning) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests; extra == \"http\"->fsspec[http]>=0.8.1->pytorch_lightning) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests; extra == \"http\"->fsspec[http]>=0.8.1->pytorch_lightning) (2020.12.5)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch_lightning) (1.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (4.2.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (4.6)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard>=2.2.0->pytorch_lightning) (3.3.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch_lightning) (3.1.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (0.4.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard>=2.2.0->pytorch_lightning) (3.4.0)\n",
            "Building wheels for collected packages: PyYAML, future, idna-ssl\n",
            "  Building wheel for PyYAML (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for PyYAML: filename=PyYAML-5.3.1-cp36-cp36m-linux_x86_64.whl size=44621 sha256=70e515543280d8f1e10ca722fbfe26dafd7a521d825656ade642a06dfc3afed6\n",
            "  Stored in directory: /root/.cache/pip/wheels/a7/c1/ea/cf5bd31012e735dc1dfea3131a2d5eae7978b251083d6247bd\n",
            "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for future: filename=future-0.18.2-cp36-none-any.whl size=491057 sha256=ec6a14cf948f98beae44453fdf25abf989de209c5e2d49757847b33b68bb4426\n",
            "  Stored in directory: /root/.cache/pip/wheels/8b/99/a0/81daf51dcd359a9377b110a8a886b3895921802d2fc1b2397e\n",
            "  Building wheel for idna-ssl (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for idna-ssl: filename=idna_ssl-1.1.0-cp36-none-any.whl size=3163 sha256=1b4755e5ecd6016b1ebaf6253c3d9a69787cdd976049eaec99b1ed7868cb436e\n",
            "  Stored in directory: /root/.cache/pip/wheels/d3/00/b3/32d613e19e08a739751dd6bf998cfed277728f8b2127ad4eb7\n",
            "Successfully built PyYAML future idna-ssl\n",
            "Installing collected packages: async-timeout, multidict, yarl, idna-ssl, aiohttp, fsspec, PyYAML, future, pytorch-lightning\n",
            "  Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Found existing installation: future 0.16.0\n",
            "    Uninstalling future-0.16.0:\n",
            "      Successfully uninstalled future-0.16.0\n",
            "Successfully installed PyYAML-5.3.1 aiohttp-3.7.3 async-timeout-3.0.1 fsspec-0.8.5 future-0.18.2 idna-ssl-1.1.0 multidict-5.1.0 pytorch-lightning-1.1.4 yarl-1.6.3\n",
            "100% 979M/979M [01:29<00:00, 10.9MMiB/s]\n",
            "Download successful. Unzipping file...\n",
            "Unzip file successful!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UTxouRaITy8I"
      },
      "source": [
        "#DOPO AVER IMPORTATO IL CONTENUTO DEL TIPO, SCEGLOERE LA RETE CHE SI VUOLE\n",
        "from resnet import resnet50, resnet18, resnet34\n",
        "\n",
        "trials = resnet34(pretrained = True).to('cuda')"
      ],
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4PNt5gwYHm7Q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 708,
          "referenced_widgets": [
            "34fc9558c5c844f8bbbd057d938a13de",
            "6b2dd5c6fe0949b8825b7df113dee6e1",
            "40a58b073e924c459db2c10cced3a3e2",
            "7469612806524cc285c1be1e11da6b83",
            "ad4ffa5060b041649e20b07c54196ecf",
            "a0205e1a3dcb4bee8f8807601a19dfdb",
            "ea81207411964d6f97ad1c3e65d3494f",
            "950639fdad4649d9ab674fb968c180ed"
          ]
        },
        "outputId": "357a65fa-fd1f-454c-a305-47ed32d29d41"
      },
      "source": [
        "#QUI FACCIO FINETUINING SULLA NUOVA RETE, CON LE MIE CLASSI\n",
        "# ORA STO USANDO LE IMMAGINI PRESE DALL'EXEMPLARS SET PER VEDERE SE HO MIGLIORAMENTI\n",
        "train_dataset = Subset(ilCIFAR100(10, 203, train = 'exemplars'), ilCIFAR100(10, 203, train = 'train').get_batch_indexes()[0])\n",
        "test_dataset = Subset(ilCIFAR100(10, 203, train = 'test'), ilCIFAR100(10, 203, train = 'test').get_batch_indexes()[0])\n",
        "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=4, drop_last=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=True, num_workers=4, drop_last=True)\n",
        "diz = ilCIFAR100(10, 203, train = 'train').get_dict()\n",
        "\n",
        "\n",
        "# Prepare Training\n",
        "optimizer = optim.SGD(trials.parameters(), lr=0.008, momentum=0.9, weight_decay= 1e-5)\n",
        "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[14,24], gamma=0.2)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "trials.eval()\n",
        "total = 0.0\n",
        "correct = 0.0\n",
        "for  _, images, labels in test_loader:\n",
        "  labels = torch.tensor([torch.tensor(diz[c.item()]) for c in labels])\n",
        "  labels = labels.to('cuda')\n",
        "  images = images.to('cuda')\n",
        "  outputs = trials(images)\n",
        "  _, preds = torch.max(outputs, dim=1)\n",
        "  total += len(labels)\n",
        "  correct += torch.sum(preds == labels).item()\n",
        "\n",
        "acc = correct / total\n",
        "print('test accuracy before finetuning', acc)\n",
        "\n",
        "\n",
        "trials.train()\n",
        "for epoch in tqdm(range(30)):\n",
        "  tot_loss = 0.0\n",
        "  for _, inputs, labels in train_loader:\n",
        "    inputs = inputs.float().to('cuda')\n",
        "    labels = torch.tensor([diz[c.item()] for c in labels])\n",
        "\n",
        "    labels=labels.to('cuda')\n",
        "    optimizer.zero_grad()\n",
        "    outputs=trials(inputs)\n",
        "\n",
        "    labels_encoded = F.one_hot(labels,10).float().to('cuda') #CAMBIARE ONE_HOT\n",
        "    loss = criterion(outputs, labels_encoded) \n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    tot_loss+=loss.item()\n",
        "  \n",
        "  scheduler.step()\n",
        "  print(f'loss at epoch{epoch}', tot_loss)\n",
        "\n",
        "\n",
        "trials.eval()\n",
        "total = 0.0\n",
        "correct = 0.0\n",
        "for  _, images, labels in test_loader:\n",
        "  labels = torch.tensor([torch.tensor(diz[c.item()]) for c in labels])\n",
        "  labels = labels.to('cuda')\n",
        "  images = images.to('cuda')\n",
        "  outputs = trials(images)\n",
        "  _, preds = torch.max(outputs, dim=1)\n",
        "  total += len(labels)\n",
        "  correct += torch.sum(preds == labels).item()\n",
        "\n",
        "acc = correct / total\n",
        "print('test accuracy after finetuning', acc)\n"
      ],
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "test accuracy before finetuning 0.14174107142857142\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "34fc9558c5c844f8bbbd057d938a13de",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=30.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "loss at epoch0 21.199261128902435\n",
            "loss at epoch1 10.543528825044632\n",
            "loss at epoch2 6.134027734398842\n",
            "loss at epoch3 3.932923801243305\n",
            "loss at epoch4 2.899127058684826\n",
            "loss at epoch5 2.1529407761991024\n",
            "loss at epoch6 1.9162600487470627\n",
            "loss at epoch7 1.6985274329781532\n",
            "loss at epoch8 1.3835681024938822\n",
            "loss at epoch9 1.2618831489235163\n",
            "loss at epoch10 1.101167593151331\n",
            "loss at epoch11 0.9173290506005287\n",
            "loss at epoch12 0.86254090256989\n",
            "loss at epoch13 1.0169573174789548\n",
            "loss at epoch14 0.7516529085114598\n",
            "loss at epoch15 0.548499371856451\n",
            "loss at epoch16 0.531963087618351\n",
            "loss at epoch17 0.5109023815020919\n",
            "loss at epoch18 0.49481094535440207\n",
            "loss at epoch19 0.48337959591299295\n",
            "loss at epoch20 0.48277781903743744\n",
            "loss at epoch21 0.48180488031357527\n",
            "loss at epoch22 0.4690206050872803\n",
            "loss at epoch23 0.4594260845333338\n",
            "loss at epoch24 0.451158806681633\n",
            "loss at epoch25 0.4557098615914583\n",
            "loss at epoch26 0.4528517108410597\n",
            "loss at epoch27 0.44149372447282076\n",
            "loss at epoch28 0.43964744359254837\n",
            "loss at epoch29 0.4507970530539751\n",
            "\n",
            "test accuracy after finetuning 0.8816964285714286\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zJ4w-tWhY6XC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 824,
          "referenced_widgets": [
            "8ab9014f6da940dcbca287ae87403955",
            "9e1312a916f548cc82647afb4bc1f93e",
            "7f041afe43e14af79fd2f6597cc43b75",
            "80948880c9864340a539797dcfb95562",
            "79cdaacab4f2456e9d70b6eebf42e594",
            "8679353713f544a38be7e308738302c8",
            "154763e3f13d4164acf06ac0f3a8a3d3",
            "971fba7bbe9642a4b7ceeb872429bcec"
          ]
        },
        "outputId": "e47b63f0-8949-4737-84fa-2d56eddc0983"
      },
      "source": [
        "# CODICE PER CREARE LE IMMAGINI SINTETICHE\n",
        "\n",
        "labels_of_modified = torch.zeros(0, dtype = int).to('cuda')\n",
        "for label in batch:\n",
        "  labels = torch.LongTensor([diz[label]]*20).to('cuda')\n",
        "  labels_of_modified = torch.cat((labels_of_modified, labels), dim=0)\n",
        "print('len to be created', len(labels_of_modified))\n",
        "number_of_images_created = 200\n",
        "\n",
        "#SE VOGLIO USARE COME TEACHER LA NOSTRA RETE USARE IL CODICE QUI\n",
        "\n",
        "#teacher = copy.deepcopy(fake_model)\n",
        "#net_teacher = resnet32(num_classes=10).to('cuda')\n",
        "#net_teacher.load_state_dict(teacher.state_dict())\n",
        "#net_teacher.eval()\n",
        "\n",
        "#net_student = resnet32(num_classes=10).to('cuda')\n",
        "net_student = resnet18().to('cuda')\n",
        "\n",
        "trials.eval()\n",
        "\n",
        "inputs = torch.randn((number_of_images_created, 3, 32, 32), requires_grad=True, device='cuda', dtype=torch.float)\n",
        "train_writer = None  # tensorboard writter\n",
        "global_iteration = 0\n",
        "di_lr = 0.05\n",
        "optimizer_di = optim.Adam([inputs], lr=di_lr)\n",
        "plt.imshow(tensor2im(inputs[0]))\n",
        "plt.show()\n",
        "print(\"Starting model inversion\")\n",
        "batch_idx = 0\n",
        "inputs = get_images(net=trials, bs=len(labels_of_modified), epochs=1500, idx=batch_idx, \n",
        "                  net_student=net_student, competitive_scale=10.0, l2_coeff = 3e-8, bn_reg_scale = 5.0,\n",
        "                  train_writer=train_writer, use_amp=False,\n",
        "                  optimizer=optimizer_di, inputs=inputs, \n",
        "                  var_scale=0.001, labels=labels_of_modified) #2.5e-5\n",
        "trials.eval()\n",
        "plt.imshow(tensor2im(inputs[0]))\n",
        "plt.show()\n",
        "print('deepinversion finshed')"
      ],
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "len to be created 200\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Starting model inversion\n",
            "Teacher correct out of 200: 18, loss at 5.609484672546387\n",
            "Student correct out of 200: 20, loss at 2.774320602416992\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8ab9014f6da940dcbca287ae87403955",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=1500.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "It 0\t Losses: total: 41.473,\ttarget: 5.546 \tR_feature_loss unscaled:\t 4.576\n",
            "It 200\t Losses: total: 10.594,\ttarget: 0.021 \tR_feature_loss unscaled:\t 0.363\n",
            "It 400\t Losses: total: 9.516,\ttarget: 0.001 \tR_feature_loss unscaled:\t 0.241\n",
            "It 600\t Losses: total: 9.100,\ttarget: 0.001 \tR_feature_loss unscaled:\t 0.184\n",
            "It 800\t Losses: total: 9.551,\ttarget: 0.001 \tR_feature_loss unscaled:\t 0.227\n",
            "It 1000\t Losses: total: 8.989,\ttarget: 0.001 \tR_feature_loss unscaled:\t 0.142\n",
            "It 1200\t Losses: total: 8.837,\ttarget: 0.001 \tR_feature_loss unscaled:\t 0.150\n",
            "It 1400\t Losses: total: 8.723,\ttarget: 0.001 \tR_feature_loss unscaled:\t 0.130\n",
            "\n",
            "Teacher correct out of 200: 200, loss at 0.0007011687848716974\n",
            "Student correct out of 200: 4, loss at 2.871809959411621\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "deepinversion finshed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "id": "sokaKsjfV9_n",
        "outputId": "c62bec69-ff6a-48bf-907f-d16bde0d83cb"
      },
      "source": [
        "plt.imshow(tensor2im(inputs_data[0]))\n",
        "plt.show()"
      ],
      "execution_count": 169,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g00ncUTSWhBA",
        "outputId": "bfb3707a-a515-47fe-d23a-534341666603"
      },
      "source": [
        "len(inputs)"
      ],
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "128"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 153
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0fG-9jw4UAgZ"
      },
      "source": [
        "#IN MODO DA AVERE UN DIZIONARIO UNICO PER IL FINETUNING SINTETICI + IMMAGINI\n",
        "fake_diz = {0:11, 1:5, 2:62, 3:76, 4:27, 5:3, 6:96, 7:33, 8:78, 9:30}\n",
        "labels_of_modified = torch.tensor([fake_diz[c.item()] for c in labels_of_modified]).to('cuda')"
      ],
      "execution_count": 156,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3g__5sQg-uzp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3bc5be53-01f5-4605-dda4-85f10778cb83"
      },
      "source": [
        "# CREO UN DATALOADER UNICO CON DATI+IMMAGINI SINTETICHE\n",
        "inputs_data = torch.tensor(inputs, requires_grad=False).cpu()\n",
        "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=4, drop_last=True)\n",
        "\n",
        "porcoddio = torch.utils.data.TensorDataset(torch.randn(len(inputs.data), requires_grad=False).cpu(), inputs_data, labels_of_modified.cpu())\n",
        "porcoddiol = DataLoader(porcoddio, batch_size = 128, shuffle=False, drop_last=False)#, num_workers=4, drop_last=False) #A SECONDA DEL BATCH\n",
        "\n",
        "tt = DataLoader(torch.utils.data.ConcatDataset((train_dataset, porcoddio)), batch_size=128, shuffle=False)#, num_workers=4, pin_memory=True)\n",
        "\n",
        "\n",
        "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=4, drop_last=False)\n",
        "\n",
        "for param in model.parameters():\n",
        "  param.requires_grad = False\n",
        "\n",
        "model.train('False')\n",
        "correct = 0.0\n",
        "total = 0.0\n",
        "for _, image, label in tt:\n",
        "  labels = torch.tensor(torch.tensor([diz[c.item()] for c in label]))\n",
        "  labels = labels.to('cuda')\n",
        "  image = image.to('cuda')\n",
        "  outputs = model(image)\n",
        "  _, preds = torch.max(outputs, dim=1)\n",
        "  total += len(labels)\n",
        "  correct += torch.sum(preds == labels).item()\n",
        "\n",
        "acc = correct / total\n",
        "model.train('False')\n",
        "print('test accuracy data + syntetic exemplars', acc, total)\n"
      ],
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "test accuracy data + syntetic exemplars 0.9467307692307693 5200.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NalwamQY5jQ8",
        "outputId": "500a8141-6010-493e-edd5-5930ef1c5aad"
      },
      "source": [
        "#PER VERIFICARE COME LA NOSTRA RETE CLASSIFICA LE IMMAGINI SINTETICHE\n",
        "trials.eval()\n",
        "\n",
        "total = 200.0\n",
        "correct = 0.0\n",
        "\n",
        "label = torch.tensor([torch.tensor(diz[c.item()]) for c in labels_of_modified]).to('cuda')\n",
        "outputs = model(inputs.data)\n",
        "_, preds = torch.max(outputs, dim=1)\n",
        "correct += torch.sum(preds == label).item()\n",
        "#total += len(label)\n",
        "acc = correct / total\n",
        "print('test accuracy with syntetic exemplars', acc)"
      ],
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test accuracy with syntetic exemplars 0.845\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FKLFf0sLiKFe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "2a31ea6f07bf47d3b943744ebb4dc2da",
            "979432db69974b7f983e64b9556ae519",
            "e0c734b418f24d02af2c89cb85818e61",
            "1b873a72e9eb4f50a22f743a0c683077",
            "a53c583c6ca440948a71a37875694ccb",
            "b6ddd90b25544dc1a9478d0546c9d537",
            "40e4b81f48f94a379a06f520765652f9",
            "296fe8ad9d23414ab1cd80d06b00960c"
          ]
        },
        "outputId": "fb7d4344-956a-4b9e-cb94-810596d7d4a5"
      },
      "source": [
        "#ALLENO UN MODELLO DA ZERO CON DATI+EXEMPLAR SINTETICI\n",
        "#tt = DataLoader(torch.utils.data.ConcatDataset((train_dataset, porcoddio)), batch_size=128, shuffle=True)#, num_workers=4, pin_memory=True)\n",
        "train_dataset = Subset(ilCIFAR100(10, 203, train = 'train'), ilCIFAR100(10, 203, train = 'train').get_batch_indexes()[0])\n",
        "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=4, drop_last=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=4, drop_last=True)\n",
        "fake_model = resnet32(num_classes=100).to('cuda')\n",
        "fake_model.train()\n",
        "\n",
        "optimizer = optim.SGD(fake_model.parameters(), lr=2, momentum=0.9, weight_decay= 1e-5)\n",
        "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[49,63], gamma=0.2)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "for epoch in tqdm(range(70)):\n",
        "  tot_loss = 0.0 \n",
        "  for _, inputs, labels in tt:\n",
        "    inputs = inputs.float().to('cuda')\n",
        "    labels = torch.tensor([diz[c.item()] for c in labels])\n",
        "\n",
        "    labels=labels.to('cuda')\n",
        "    optimizer.zero_grad()\n",
        "    outputs=fake_model(inputs)\n",
        "\n",
        "    labels_encoded = F.one_hot(labels, 100).float().to('cuda') #CAMBIARE ONE_HOT\n",
        "    loss = criterion(outputs, labels_encoded) \n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    tot_loss+=loss.item()\n",
        "  scheduler.step()\n",
        "  print(f'loss at epoch {epoch}', tot_loss)\n",
        "fake_model.eval()\n",
        "\n",
        "total = 0.0\n",
        "correct = 0.0\n",
        "for  _, images, labels in test_loader:\n",
        "  labels = torch.tensor([torch.tensor(diz[c.item()]) for c in labels])\n",
        "  labels = labels.to('cuda')\n",
        "  images = images.to('cuda')\n",
        "  outputs = fake_model(images)\n",
        "  _, preds = torch.max(outputs, dim=1)\n",
        "  total += len(labels)\n",
        "  correct += torch.sum(preds == labels).item()\n",
        "\n",
        "acc = correct / total\n",
        "print('test accuracy with syntetic exemplars', acc)"
      ],
      "execution_count": 171,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2a31ea6f07bf47d3b943744ebb4dc2da",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=70.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "loss at epoch 0 2.4540810100734234\n",
            "loss at epoch 1 0.9700034447014332\n",
            "loss at epoch 2 0.8656038641929626\n",
            "loss at epoch 3 0.785343898460269\n",
            "loss at epoch 4 0.7194453729316592\n",
            "loss at epoch 5 0.6722258748486638\n",
            "loss at epoch 6 0.6303174179047346\n",
            "loss at epoch 7 0.5516120810061693\n",
            "loss at epoch 8 0.4870914202183485\n",
            "loss at epoch 9 0.43468355666846037\n",
            "loss at epoch 10 0.42519158590584993\n",
            "loss at epoch 11 0.4045130517333746\n",
            "loss at epoch 12 0.39306805515661836\n",
            "loss at epoch 13 0.3057033601216972\n",
            "loss at epoch 14 0.24645023792982101\n",
            "loss at epoch 15 0.25532248336821795\n",
            "loss at epoch 16 0.2748253480531275\n",
            "loss at epoch 17 0.2523432490415871\n",
            "loss at epoch 18 0.19916161010041833\n",
            "loss at epoch 19 0.17305954755283892\n",
            "loss at epoch 20 0.1533484470564872\n",
            "loss at epoch 21 0.13120939279906452\n",
            "loss at epoch 22 0.10854474175721407\n",
            "loss at epoch 23 0.07234471826814115\n",
            "loss at epoch 24 0.053358428238425404\n",
            "loss at epoch 25 0.03999096667394042\n",
            "loss at epoch 26 0.029273440944962204\n",
            "loss at epoch 27 0.021654711657902226\n",
            "loss at epoch 28 0.026881582962232642\n",
            "loss at epoch 29 0.01737167085229885\n",
            "loss at epoch 30 0.017715373272949364\n",
            "loss at epoch 31 0.009480462504143361\n",
            "loss at epoch 32 0.005044861238275189\n",
            "loss at epoch 33 0.0026598133372317534\n",
            "loss at epoch 34 0.0021308240538928658\n",
            "loss at epoch 35 0.002044381129962858\n",
            "loss at epoch 36 0.0020118399697821587\n",
            "loss at epoch 37 0.0020053692715009674\n",
            "loss at epoch 38 0.0020139500848017633\n",
            "loss at epoch 39 0.002032604257692583\n",
            "loss at epoch 40 0.002058189296803903\n",
            "loss at epoch 41 0.0020886187885480467\n",
            "loss at epoch 42 0.0021223844669293612\n",
            "loss at epoch 43 0.002158326740755001\n",
            "loss at epoch 44 0.002196126995841041\n",
            "loss at epoch 45 0.0022349313912854996\n",
            "loss at epoch 46 0.0022744570633221883\n",
            "loss at epoch 47 0.0023141863239288796\n",
            "loss at epoch 48 0.0023538180757896043\n",
            "loss at epoch 49 0.002360003021749435\n",
            "loss at epoch 50 0.0023678755169385113\n",
            "loss at epoch 51 0.002375762433075579\n",
            "loss at epoch 52 0.0023836740765545983\n",
            "loss at epoch 53 0.0023916044483485166\n",
            "loss at epoch 54 0.0023995298870431725\n",
            "loss at epoch 55 0.0024074393368209712\n",
            "loss at epoch 56 0.0024153122431016527\n",
            "loss at epoch 57 0.0024231407878687605\n",
            "loss at epoch 58 0.002430960928904824\n",
            "loss at epoch 59 0.0024387382727582008\n",
            "loss at epoch 60 0.0024464762755087577\n",
            "loss at epoch 61 0.002454187473631464\n",
            "loss at epoch 62 0.0024618320203444455\n",
            "loss at epoch 63 0.002463058703142451\n",
            "loss at epoch 64 0.0024645832199894357\n",
            "loss at epoch 65 0.0024661042443767656\n",
            "loss at epoch 66 0.0024676241300767288\n",
            "loss at epoch 67 0.002469141541951103\n",
            "loss at epoch 68 0.0024706604053790215\n",
            "loss at epoch 69 0.002472177060553804\n",
            "\n",
            "test accuracy with syntetic exemplars 0.7299107142857143\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "cb8fc012d780484cb6fc58526c2dd82c",
            "3ca0fbcc154f45d9b9d904ae38fbf1de",
            "9476ee6f351d4894b3a2f5f818ab451f",
            "7baef604cf784f0f98f17bd234037c53",
            "6bef149195cc4be7844189cce7f71a78",
            "0c3f1e3e59d9409f9286cad49defa66e",
            "99b4f1610a7e4532921bcbeed599aa56",
            "6b3b8a0ba37345b8baac5af94a91a7af"
          ]
        },
        "id": "hDD941kelDM6",
        "outputId": "1a1714f1-a1c2-49e1-d03e-3d43df7b1977"
      },
      "source": [
        "#ALLENO UN MODELLO DA ZERO CON SOLO I DATI E CONFRONTO I RISULTATI CON QUELLI PRECEDENTI (PORCODIO)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=4, drop_last=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=4, drop_last=True)\n",
        "fake_model = resnet32(num_classes=100).to('cuda')\n",
        "fake_model.train()\n",
        "\n",
        "optimizer = optim.SGD(fake_model.parameters(), lr=2, momentum=0.9, weight_decay= 1e-5)\n",
        "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[49,63], gamma=0.2)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "for epoch in tqdm(range(70)):\n",
        "  tot_loss = 0.0 \n",
        "  for _, inputs, labels in train_loader:\n",
        "    inputs = inputs.float().to('cuda')\n",
        "    labels = torch.tensor([diz[c.item()] for c in labels])\n",
        "\n",
        "    labels=labels.to('cuda')\n",
        "    optimizer.zero_grad()\n",
        "    outputs=fake_model(inputs)\n",
        "\n",
        "    labels_encoded = F.one_hot(labels, 100).float().to('cuda') #CAMBIARE ONE_HOT\n",
        "    loss = criterion(outputs, labels_encoded) \n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    tot_loss+=loss.item()\n",
        "  scheduler.step()\n",
        "  print(f'loss at epoch {epoch}', tot_loss)\n",
        "fake_model.eval()\n",
        "\n",
        "total = 0.0\n",
        "correct = 0.0\n",
        "for  _, images, labels in test_loader:\n",
        "  labels = torch.tensor([torch.tensor(diz[c.item()]) for c in labels])\n",
        "  labels = labels.to('cuda')\n",
        "  images = images.to('cuda')\n",
        "  outputs = fake_model(images)\n",
        "  _, preds = torch.max(outputs, dim=1)\n",
        "  total += len(labels)\n",
        "  correct += torch.sum(preds == labels).item()\n",
        "\n",
        "acc = correct / total\n",
        "print('test accuracy without syntetic exemplars', acc)"
      ],
      "execution_count": 162,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cb8fc012d780484cb6fc58526c2dd82c",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=70.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "loss at epoch 0 2.5166656356304884\n",
            "loss at epoch 1 0.9523913506418467\n",
            "loss at epoch 2 0.8591399174183607\n",
            "loss at epoch 3 0.803829051554203\n",
            "loss at epoch 4 0.7612009737640619\n",
            "loss at epoch 5 0.7314405580982566\n",
            "loss at epoch 6 0.6943546757102013\n",
            "loss at epoch 7 0.6900990819558501\n",
            "loss at epoch 8 0.6516273748129606\n",
            "loss at epoch 9 0.6197463888674974\n",
            "loss at epoch 10 0.5956556769087911\n",
            "loss at epoch 11 0.5654664691537619\n",
            "loss at epoch 12 0.5755842570215464\n",
            "loss at epoch 13 0.5547781558707356\n",
            "loss at epoch 14 0.5169421285390854\n",
            "loss at epoch 15 0.4973963415250182\n",
            "loss at epoch 16 0.483091508038342\n",
            "loss at epoch 17 0.46920445561408997\n",
            "loss at epoch 18 0.4651121199131012\n",
            "loss at epoch 19 0.45408163126558065\n",
            "loss at epoch 20 0.4264025557786226\n",
            "loss at epoch 21 0.4077454339712858\n",
            "loss at epoch 22 0.3871168722398579\n",
            "loss at epoch 23 0.3955209795385599\n",
            "loss at epoch 24 0.3999937744811177\n",
            "loss at epoch 25 0.36884749960154295\n",
            "loss at epoch 26 0.35026110569015145\n",
            "loss at epoch 27 0.3412933452054858\n",
            "loss at epoch 28 0.35256827995181084\n",
            "loss at epoch 29 0.3570840200409293\n",
            "loss at epoch 30 0.3359459824860096\n",
            "loss at epoch 31 0.30266787810251117\n",
            "loss at epoch 32 0.31790626095607877\n",
            "loss at epoch 33 0.3019117056392133\n",
            "loss at epoch 34 0.28794259345158935\n",
            "loss at epoch 35 0.2924129790626466\n",
            "loss at epoch 36 0.287518123164773\n",
            "loss at epoch 37 0.2850620290264487\n",
            "loss at epoch 38 0.2687176610343158\n",
            "loss at epoch 39 0.27072661253623664\n",
            "loss at epoch 40 0.2618739474564791\n",
            "loss at epoch 41 0.23273056792095304\n",
            "loss at epoch 42 0.26535919215530157\n",
            "loss at epoch 43 0.26131547754630446\n",
            "loss at epoch 44 0.2297619367018342\n",
            "loss at epoch 45 0.23307961178943515\n",
            "loss at epoch 46 0.24387547746300697\n",
            "loss at epoch 47 0.23208424961194396\n",
            "loss at epoch 48 0.23589274589903653\n",
            "loss at epoch 49 0.17036690399982035\n",
            "loss at epoch 50 0.13472769735381007\n",
            "loss at epoch 51 0.11681262962520123\n",
            "loss at epoch 52 0.1190395433222875\n",
            "loss at epoch 53 0.10967305640224367\n",
            "loss at epoch 54 0.10966377228032798\n",
            "loss at epoch 55 0.09855420584790409\n",
            "loss at epoch 56 0.09736131294630468\n",
            "loss at epoch 57 0.094041129341349\n",
            "loss at epoch 58 0.08644184144213796\n",
            "loss at epoch 59 0.09320031385868788\n",
            "loss at epoch 60 0.08520278229843825\n",
            "loss at epoch 61 0.08052237378433347\n",
            "loss at epoch 62 0.07820162933785468\n",
            "loss at epoch 63 0.06842975824838504\n",
            "loss at epoch 64 0.06414391577709466\n",
            "loss at epoch 65 0.06807276047766209\n",
            "loss at epoch 66 0.06435037055052817\n",
            "loss at epoch 67 0.06559596775332466\n",
            "loss at epoch 68 0.05981505324598402\n",
            "loss at epoch 69 0.06289416423533112\n",
            "\n",
            "test accuracy without syntetic exemplars 0.8337053571428571\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zMVWq87mcCqu",
        "outputId": "d927d9d9-2c1d-4a33-c51f-9ee56b46af2f"
      },
      "source": [
        "fake_model.eval()\n",
        "total = 0.0\n",
        "correct = 0.0\n",
        "for  _, images, labels in test_loader:\n",
        "  labels = torch.tensor([torch.tensor(diz[c.item()]) for c in labels])\n",
        "  labels = labels.to('cuda')\n",
        "  images = images.to('cuda')\n",
        "  outputs = fake_model(images)\n",
        "  _, preds = torch.max(outputs, dim=1)\n",
        "  total += len(labels)\n",
        "  correct += torch.sum(preds == labels).item()\n",
        "\n",
        "acc = correct / total\n",
        "print('test accuracy with syntetic exemplars', acc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test accuracy with syntetic exemplars 0.8392857142857143\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "umrCzlLBgXYq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}