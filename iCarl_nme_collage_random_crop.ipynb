{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "iCarl_nme_collage.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "aafe00c5af80418c98eff425415931fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_33da1aac471645f991f776491d0e1979",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_4ad469eaf6c948ecb3165e775b1d78df",
              "IPY_MODEL_4c6acf59ea4b40649894fcfcd40de769"
            ]
          }
        },
        "33da1aac471645f991f776491d0e1979": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4ad469eaf6c948ecb3165e775b1d78df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_89c854557f30456aa08d5934fdc935a2",
            "_dom_classes": [],
            "description": "  6%",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 70,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 4,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_98466a50ec39450fae2eb51197c9b7f1"
          }
        },
        "4c6acf59ea4b40649894fcfcd40de769": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_a16dc5847f554f6ca50abaf11bc3e637",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 4/70 [00:17&lt;04:47,  4.36s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b10de0b7ff4a4773bbc49dd4b2e2e692"
          }
        },
        "89c854557f30456aa08d5934fdc935a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "98466a50ec39450fae2eb51197c9b7f1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a16dc5847f554f6ca50abaf11bc3e637": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b10de0b7ff4a4773bbc49dd4b2e2e692": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alessandronicolini/IncrementalLearning/blob/main/iCarl_nme_collage_random_crop.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WH7YHXeh0hFj",
        "outputId": "859178f7-5a28-4378-9f4a-a4bb0769d327"
      },
      "source": [
        "!pip3 install 'import_ipynb'\n",
        "!pip3 install 'tqdm'\n",
        "\n",
        "!rm -r IncrementalLearning\n",
        "# upload work files from your git hub repository\n",
        "import sys\n",
        "\n",
        "!git clone https://github.com/alessandronicolini/IncrementalLearning.git # clone proj repository\n",
        "!rm -rf IncrementalLearning/README.md \n",
        "!rm -rf IncrementalLearning/baselines.ipynb\n",
        "\n",
        "path = 'IncrementalLearning/'\n",
        "if path not in sys.path:\n",
        "    sys.path.append('IncrementalLearning/')\n",
        "\n",
        "!pip3 install import_ipynb"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting import_ipynb\n",
            "  Downloading https://files.pythonhosted.org/packages/63/35/495e0021bfdcc924c7cdec4e9fbb87c88dd03b9b9b22419444dc370c8a45/import-ipynb-0.1.3.tar.gz\n",
            "Building wheels for collected packages: import-ipynb\n",
            "  Building wheel for import-ipynb (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for import-ipynb: filename=import_ipynb-0.1.3-cp36-none-any.whl size=2976 sha256=05f0fa720cc7be20793d14505e89ea8cddbf6947a8a4aaf9af1d272c3d0e05e0\n",
            "  Stored in directory: /root/.cache/pip/wheels/b4/7b/e9/a3a6e496115dffdb4e3085d0ae39ffe8a814eacc44bbf494b5\n",
            "Successfully built import-ipynb\n",
            "Installing collected packages: import-ipynb\n",
            "Successfully installed import-ipynb-0.1.3\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.41.1)\n",
            "rm: cannot remove 'IncrementalLearning': No such file or directory\n",
            "Cloning into 'IncrementalLearning'...\n",
            "remote: Enumerating objects: 119, done.\u001b[K\n",
            "remote: Counting objects: 100% (119/119), done.\u001b[K\n",
            "remote: Compressing objects: 100% (119/119), done.\u001b[K\n",
            "remote: Total 786 (delta 61), reused 0 (delta 0), pack-reused 667\u001b[K\n",
            "Receiving objects: 100% (786/786), 5.62 MiB | 14.08 MiB/s, done.\n",
            "Resolving deltas: 100% (462/462), done.\n",
            "Requirement already satisfied: import_ipynb in /usr/local/lib/python3.6/dist-packages (0.1.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5O4jUchQ1EAI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f213991-d2c7-4f5a-a4ec-fcadf2613276"
      },
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import math\n",
        "from sklearn.preprocessing import normalize\n",
        "import copy\n",
        "from torchvision.datasets import VisionDataset\n",
        "from torchvision.utils import make_grid\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.gridspec as gridspec\n",
        "from torch.utils.data import Subset, DataLoader\n",
        "import random\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pickle\n",
        "import pandas as pd\n",
        "from sklearn.manifold import TSNE\n",
        "import import_ipynb\n",
        "# from IncrementalLearning.cifar100 import ilCIFAR100\n",
        "\n",
        "from IncrementalLearning.resnet_cifar import resnet32\n",
        "from tqdm.notebook import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "importing Jupyter notebook from /content/IncrementalLearning/resnet_cifar.ipynb\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TpHzYPBweCe-"
      },
      "source": [
        "# SOME UTILS FUNCTIONS\n",
        "\n",
        "def tensor2im(input_image, imtype=np.uint8):\n",
        "    mean = [0.5071, 0.4867, 0.4408]\n",
        "    std = [0.2675, 0.2565, 0.2761]\n",
        "    if not isinstance(input_image, np.ndarray):\n",
        "        if isinstance(input_image, torch.Tensor):\n",
        "            image_tensor = input_image.data\n",
        "        else:\n",
        "            return input_image\n",
        "        image_numpy = image_tensor.cpu().detach().float().numpy()\n",
        "        if image_numpy.shape[0] == 1:\n",
        "            image_numpy = np.tile(image_numpy, (3, 1, 1))\n",
        "        for i in range(len(mean)): \n",
        "            image_numpy[i] = image_numpy[i] * std[i] + mean[i]\n",
        "        image_numpy = image_numpy * 255\n",
        "        image_numpy = np.transpose(image_numpy, (1, 2, 0))\n",
        "    else:\n",
        "        image_numpy = input_image\n",
        "    return image_numpy.astype(imtype)\n",
        "\n",
        "\n",
        "ROOT = './data'\n",
        "class ilCIFAR100(VisionDataset):\n",
        "    \"\"\"\n",
        "    Extends CIFAR100 class. Split the dataset into 10 batches, each one containing 10 classes.\n",
        "    You can retrieve the batches from the attribute \"batches\", it has different structure according to\n",
        "    test and train CIFAR100 splits:\n",
        "        - train -> batches is a dictionary {0:{'train':indexes, 'val':indexes}...} \n",
        "        - test -> batches is a dictionary {0:indexes...}\n",
        "    where the keys are the batch number.\n",
        " \n",
        "    Args:\n",
        "        root (string): Root directory of dataset where directory\n",
        "            `cifar-10-batches-py` exists or will be saved to if download is set to True.\n",
        "        seed(int): used to ensure reproducibility in shuffling operations.\n",
        "        val_size(float, optional): between 0 and 1, fraction of data used for validation.\n",
        "        train (bool, optional): If True, creates dataset from training set, otherwise\n",
        "            creates from test set.\n",
        "        transform (callable, optional): A function/transform that takes in an PIL image\n",
        "            and returns a transformed version. E.g, `transforms.RandomCrop`\n",
        "        target_transform (callable, optional): A function/transform that takes in the\n",
        "            target and transforms it.\n",
        "        download (bool, optional): If true, downloads the dataset from the internet and\n",
        "            puts it in root directory. If dataset is already downloaded, it is not\n",
        "            downloaded again.\n",
        "    \"\"\"\n",
        "    def __init__(self,classes_per_batch, seed, val_size=0.1, train='train', transform=None, target_transform=False, \n",
        "    download=True):\n",
        "        \n",
        "        super(ilCIFAR100, self).__init__(root=0)\n",
        "        self.classes_per_batch=classes_per_batch\n",
        "        self.coarse_labels = ['apple', 'aquarium_fish', 'baby', 'bear', 'beaver','bed','bee','beetle','bicycle','bottle',\n",
        "                              'bowl','boy','bridge','bus','butterfly','camel','can','castle','caterpillar','cattle',\n",
        "                              'chair','chimpanzee','clock','cloud','cockroach','couch','crab','crocodile','cup', 'dinosaur',\n",
        "                              'dolphin','elephant','flatfish','forest','fox','girl','hamster','house','kangaroo','computer_keyboard',\n",
        "                              'lamp','lawn_mower','leopard','lion','lizard','lobster','man','maple_tree','motorcycle','mountain',\n",
        "                              'mouse','mushroom','oak_tree','orange','orchid','otter','palm_tree','pear','pickup_truck','pine_tree',\n",
        "                              'plain','plate','poppy','porcupine','possum','rabbit','raccoon','ray','road','rocket',\n",
        "                              'rose','sea','seal','shark','shrew','skunk','skyscraper','snail','snake','spider',\n",
        "                              'squirrel','streetcar','sunflower','sweet_pepper','table','tank','telephone','television','tiger','tractor',\n",
        "                              'train','trout','tulip','turtle','wardrobe','whale','willow_tree','wolf','woman','worm',]\n",
        "\n",
        "\n",
        "        \n",
        "        self.__rs = seed # set random seed \n",
        "        self.train = train\n",
        "        self._transform_train = transforms.Compose([\n",
        "            transforms.RandomCrop(32, padding=4),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))\n",
        "        ])\n",
        " \n",
        "        self._transform_test = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))\n",
        "        ])\n",
        "        # if train:\n",
        "        #     self.batches = self.__make_train_batches(val_size)\n",
        "        # else:\n",
        "        #     self.batches = self.__make_test_batches()\n",
        "        if self.train == 'train':\n",
        "          self.dataset = torchvision.datasets.CIFAR100(root=ROOT, train=True,\n",
        "                                            download=True, transform=self._transform_train)\n",
        "        elif self.train == 'exemplars':\n",
        "          self.dataset = torchvision.datasets.CIFAR100(root=ROOT, train=True,\n",
        "                                            download=True, transform=self._transform_test)\n",
        "        else:\n",
        "          self.dataset = torchvision.datasets.CIFAR100(root=ROOT, train=False,\n",
        "                                        download=True, transform=self._transform_test)\n",
        "          \n",
        "        self.targets = np.array(self.dataset.targets) # make targets an array to exploit masking\n",
        "        random.seed(seed)\n",
        "        self.classes = random.sample(range(0, 100), 100)\n",
        "        #self.classes = self.classes.reshape((10, -1)) # each row contains the classes for the corrisponding batch\n",
        "        #print(self.classes)\n",
        "        self.__dictionary = {}\n",
        "        for i, c in enumerate(self.classes):\n",
        "          self.__dictionary[c] = i\n",
        " \n",
        " \n",
        "    def get_dict(self):\n",
        "      return self.__dictionary\n",
        "      \n",
        "    def __getitem__(self, index):\n",
        "        return index,self.dataset.__getitem__(index)[0],self.dataset.__getitem__(index)[1]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.dataset.__len__()\n",
        "\n",
        "    def getbatches(self):\n",
        "      classlist=self.classes\n",
        "      batches=[]\n",
        "      for i in range(0,int(100/self.classes_per_batch)):\n",
        "        #print(i)\n",
        "        batch=classlist[int(i*self.classes_per_batch):int(i*10+self.classes_per_batch)]\n",
        "        batches.append(batch)\n",
        "      return batches\n",
        "\n",
        "    def get_batch_indexes(self):\n",
        "      classlist=self.classes\n",
        "      numclass=self.classes_per_batch\n",
        "      batch_indexes=[]\n",
        "      for i in range(0,int(100/self.classes_per_batch)):\n",
        "        batch=classlist[int(i*numclass):int(i*numclass+numclass)]\n",
        "        mask=np.isin(self.targets,batch)\n",
        "        indexes=np.array(np.arange(len(self.dataset.targets)))\n",
        "        indexes=indexes[mask]\n",
        "        batch_indexes.append(indexes)\n",
        "      return batch_indexes\n",
        "\n",
        "    def get_class_indexes(self,label):\n",
        "      indexes = np.array(np.arange(len(self.dataset.targets)))\n",
        "      labels = self.dataset.targets\n",
        "      mask = np.isin(labels, label)\n",
        "      indexes = indexes[mask]\n",
        " \n",
        "      return indexes\n",
        "    def get_train_val(self,valid):\n",
        "      batches=self.get_batch_indexes()\n",
        "      train=[]\n",
        "      val=[]\n",
        "      for batch in batches:\n",
        "        #print(type(batch))\n",
        "        random.shuffle(batch)\n",
        "        valbatch=batch[0:int(valid*len(batch))]\n",
        "        trainbatch=batch[int(valid*len(batch)):]\n",
        "        train.append(trainbatch)\n",
        "        val.append(valbatch)\n",
        "      return train,val"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HzqwQeHB1Tg-"
      },
      "source": [
        "class icarl(nn.Module):\n",
        "  def __init__(self, randomseed, name):\n",
        "    super(icarl, self).__init__()\n",
        "    self.model = resnet32(num_classes=100)\n",
        "    self.feature_extractor = self.model.features\n",
        "    self.lr = 2\n",
        "    self.gamma = 0.2\n",
        "    self.weight_decay =1e-5 \n",
        "    self.milestones = [49,63]\n",
        "    self.batch_size = 128\n",
        "    self.numepochs = 70\n",
        "    self.n_classes = 0\n",
        "    self.n_known = 0\n",
        "    self.feature_size = 64\n",
        "    self.momentum = 0.9\n",
        "    self.criterion = nn.BCEWithLogitsLoss()\n",
        "    self.compute_means = True\n",
        "    self.exemplar_means = None\n",
        "    self.exemplar_sets = []\n",
        "    self.exemplar_labels = []\n",
        "    self.NUM_BATCHES = 10\n",
        "    self.randomseed = randomseed\n",
        "    self.trainloader = None\n",
        "    self.testloader = None\n",
        "    self.CLASSES_PER_BATCH = 10\n",
        "    self.original_training_set = ilCIFAR100(self.CLASSES_PER_BATCH,self.randomseed, train='train')\n",
        "    self.original_exemplar_set = ilCIFAR100(self.CLASSES_PER_BATCH,self.randomseed, train='exemplars')\n",
        "    self.original_test_set = ilCIFAR100(self.CLASSES_PER_BATCH,self.randomseed, train='test')\n",
        "    self.name = name+'_'+str(randomseed)\n",
        "\n",
        "    self.y_pred = []\n",
        "    self.y_test = []\n",
        "\n",
        "    self.classes_seen = 0\n",
        "    self.diz = self.original_training_set.get_dict()\n",
        "\n",
        "    # tsne attributes\n",
        "    self.tsne_labels = self.original_training_set.getbatches()[0][:5]\n",
        "    tsne_train_idxs = []\n",
        "    hexcolors = ['#0b649e', '#d85c20','#c43509', '#21130d','#741a00']\n",
        "    for label in self.tsne_labels:\n",
        "      tsne_train_idxs.append(self.original_training_set.get_class_indexes(label).tolist())\n",
        "\n",
        "    \n",
        "    self.tsne_train_idxs = tsne_train_idxs\n",
        "    self.label2hexcolor = {label:hexcolor for label, hexcolor in zip(self.tsne_labels, hexcolors)}\n",
        "    self.label2color ={label:color for label, color in zip(self.tsne_labels, ['blue','orange','red','black','brown'])}\n",
        "\n",
        "\n",
        "  def update_parameters(self):\n",
        "    old_model = copy.deepcopy(self)\n",
        "    old_model.eval()\n",
        "    old_model.to('cuda')\n",
        "    n_classes = self.classes_seen+self.CLASSES_PER_BATCH\n",
        "    print(n_classes)\n",
        "    optimizer = optim.SGD(self.model.parameters(), lr=self.lr, momentum=self.momentum, weight_decay=self.weight_decay)\n",
        "    scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=self.milestones, gamma=self.gamma)\n",
        "    for epoch in tqdm(range(self.numepochs)):\n",
        "        \n",
        "      for _, inputs, labels in self.trainloader:\n",
        "        inputs = inputs.float().cuda()\n",
        "        labels = torch.tensor([self.diz[c.item()] for c in labels])\n",
        "\n",
        "        labels=labels.to('cuda')\n",
        "        optimizer.zero_grad()\n",
        "        outputs=self.forward(inputs)\n",
        "\n",
        "        labels_encoded = F.one_hot(labels,100).float().cuda() #CAMBIARE ONE_HOT\n",
        "        \n",
        "        if self.classes_seen:\n",
        "          old_target = old_model.forward(inputs).cuda()\n",
        "          old_target = torch.sigmoid(old_target).cuda()\n",
        "          \n",
        "          target = torch.cat((old_target[:,:self.classes_seen], labels_encoded[:, self.classes_seen:]), dim=1)\n",
        "          loss = self.criterion(outputs, target)\n",
        "        else:\n",
        "          loss = self.criterion(outputs,labels_encoded) \n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "      \n",
        "      scheduler.step()\n",
        "\n",
        "  def classify_nme(self, input_batch):\n",
        "    min_distances = float('inf')*torch.ones(len(input_batch)).cuda() # shape: batch_size --> 128\n",
        "    y_pred = torch.zeros(len(input_batch), dtype=torch.int8).cuda() # shape: batch_size --> 128\n",
        "    input_features = self.feature_extractor(input_batch) # shape: (batch_size, feature_size) --> (128, 64)\n",
        "\n",
        "    for i in range(len(self.exemplar_sets)):\n",
        "      ex_mean = self.exemplar_means[i,:]\n",
        "\n",
        "      # compute distances between inputs features and exemplar set means\n",
        "      pdist = nn.PairwiseDistance(p=2)\n",
        "      distances = pdist(input_features, ex_mean) # shape: batch_size --> 128\n",
        "\n",
        "      # update min distancies and predicted labels\n",
        "      mask = distances < min_distances\n",
        "      min_distances[mask] = distances[mask]\n",
        "      y_pred[mask] = self.exemplar_labels[i]\n",
        "\n",
        "    return y_pred\n",
        "    \n",
        "\n",
        "  def get_new_exemplars(self, batch, m):\n",
        "    new_ex_idxs = random.sample(list(batch), m)\n",
        "    self.exemplar_sets.append(new_ex_idxs) \n",
        "\n",
        "    # memorizza tutti i tensori degli exemplars della singola classe\n",
        "    ex_tensors = torch.zeros((m, 3, 32, 32), dtype=torch.float)\n",
        "    for i, ex_idx in enumerate(new_ex_idxs):\n",
        "      _, ex, _ = self.original_exemplar_set.__getitem__(ex_idx)\n",
        "      ex_tensors[i] = ex\n",
        "\n",
        "    # create m collage images\n",
        "    for i, idx in enumerate(new_ex_idxs):\n",
        "      #scegli randomicamente 4 immagini appartenenti agli exemplars della singola classe\n",
        "      images = ex_tensors[np.random.choice(range(m), size=4, replace=False)]\n",
        "      upper_left = torchvision.transforms.RandomCrop(16)(images[0])\n",
        "      upper_right = torchvision.transforms.RandomCrop(16)(images[1])\n",
        "      bottom_left = torchvision.transforms.RandomCrop(16)(images[2])\n",
        "      bottom_right = torchvision.transforms.RandomCrop(16)(images[3])\n",
        "      # crea il collage \n",
        "      collage = make_grid([upper_left, upper_right, bottom_left, bottom_right], nrow=2, padding=0)\n",
        "      # salva il nuovo collge in original_exemplat_set\n",
        "      self.original_exemplar_set.dataset.data[idx] = tensor2im(collage)\n",
        "      '''\n",
        "      if not i:\n",
        "        plt.figure()\n",
        "        plt.imshow(tensor2im(images[0]))\n",
        "        plt.figure()\n",
        "        plt.imshow(tensor2im(images[1]))\n",
        "        plt.figure()\n",
        "        plt.imshow(tensor2im(images[2]))\n",
        "        plt.figure()\n",
        "        plt.imshow(tensor2im(images[3]))\n",
        "        plt.figure()\n",
        "        plt.imshow(tensor2im(collage))\n",
        "        #plt.imshow(self.original_exemplar_set.__getitem__(idx)[1].permute(1,2,0).numpy())'''\n",
        "\n",
        "        \n",
        "  def reduce_old_exemplars(self,m):\n",
        "    for y, P_y in enumerate(self.exemplar_sets):\n",
        "      self.exemplar_sets[y] = P_y[:int(m)]\n",
        "\n",
        "  def forward(self, x):\n",
        "    self.model = self.model.cuda()\n",
        "    return self.model.forward(x)\n",
        "\n",
        "  def __accuracy_fc(self, dl, model, mapper):\n",
        "    total = 0.0\n",
        "    correct = 0.0\n",
        "    for  _, images, labels in dl:\n",
        "      labels = torch.tensor([torch.tensor(mapper[c.item()]) for c in labels])\n",
        "      labels = labels.cuda()\n",
        "      images = images.cuda()\n",
        "      outputs = self.forward(images)\n",
        "      _, preds = torch.max(outputs, dim=1)\n",
        "      total += len(labels)\n",
        "      correct += torch.sum(preds == labels).item()\n",
        "\n",
        "    acc = correct / total\n",
        "    return acc\n",
        "\n",
        "\n",
        "  def __accuracy_nme(self, dl, model, last_test=False):\n",
        "    \n",
        "    total = 0.0\n",
        "    correct = 0.0\n",
        "    \n",
        "    for  _, images, labels in dl:\n",
        "      labels = labels.cuda()\n",
        "      images = images.cuda()\n",
        "      outputs = model(images)\n",
        "      preds = self.classify_nme(images)\n",
        "      total += len(labels)\n",
        "      correct += torch.sum(preds == labels).item()\n",
        "\n",
        "      if last_test:\n",
        "        self.y_pred += preds.tolist()\n",
        "        self.y_test += labels.tolist()\n",
        "\n",
        "    acc = correct / total\n",
        "    return acc\n",
        "\n",
        "\n",
        "  def plot_confusion_matrix(self):\n",
        " \n",
        "    cm = confusion_matrix(self.y_test, self.y_pred)\n",
        "    cm = np.log(cm+1)\n",
        "    fig, ax = plt.subplots(figsize=(7,7))\n",
        "    sns.heatmap(cm, square=True, cbar=False, ax=ax, cmap=plt.get_cmap('seismic'))\n",
        "    ax.set_xticks(np.linspace(19,99,5))\n",
        "    ax.set_yticks(np.linspace(19,99,5))\n",
        "    ax.set_xticklabels([20,40,60,80,100], rotation=0)\n",
        "    ax.set_yticklabels([20,40,60,80,100], rotation=0)\n",
        "    ax.set_title(\"iCaRL\")\n",
        "    ax.set_xlabel(\"Predicted class\")\n",
        "    ax.set_ylabel(\"True class\")\n",
        "    plt.savefig(\"iCaRL_\"+str(self.randomseed)+\"_cm.png\")\n",
        "    plt.show()\n",
        "    return cm\n",
        "\n",
        "  def add_tsne_plot(self, task_num):\n",
        "    \n",
        "    # stampa in nome delle classi con i rispettivi colori, e printane alcune\n",
        "    for label, idxs in zip(self.tsne_labels, self.exemplar_sets[:5]):\n",
        "      print('label %d,    coarse label: %s,    color: %s'%(label, self.original_training_set.coarse_labels[label], self.label2color[label]))\n",
        "      # printa 5 exemplars random per ognuna delle classi\n",
        "      rnd_ex_idxs = random.sample(idxs, 5)\n",
        "      plt.figure()\n",
        "      fig, axes = plt.subplots(nrows=1, ncols=5, figsize=(4*5, 4))\n",
        "      for i, ex_idx in enumerate(rnd_ex_idxs):\n",
        "        axes[i].imshow(tensor2im(self.original_exemplar_set.__getitem__(ex_idx)[1]))\n",
        "        axes[i].tick_params(bottom=False, left=False, labelleft=False, labelbottom=False)\n",
        "      plt.tight_layout()\n",
        "      plt.savefig('tn_'+str(task_num)+'_'+str(label)+'_'+self.original_training_set.coarse_labels[label]+'.png')\n",
        "      plt.show()\n",
        "\n",
        "\n",
        "    # salva l'attuale transform e inseriscine uno nuovo che non faccia data augmentation\n",
        "    transform = self.original_training_set.dataset.transform \n",
        "    self.original_training_set.dataset.transform = transforms.Compose(\n",
        "        [transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))]\n",
        "    )\n",
        "\n",
        "    # crea il dataset da scorrere\n",
        "    # dataset con tutti gli elemnti delle 5 classi prese in considerazione, per la distribuzione della classe\n",
        "    class_idxs = []\n",
        "    for idxs in self.tsne_train_idxs:\n",
        "      class_idxs += idxs\n",
        "    dataset_tasks = Subset(self.original_training_set, class_idxs)\n",
        "    \n",
        "    # dataset con gli exemplars delle 5 classi prese in considerazione\n",
        "    ex_idxs = []\n",
        "    for idxs in self.exemplar_sets[:5]:\n",
        "      ex_idxs += idxs\n",
        "    dataset_ex = Subset(self.original_exemplar_set, ex_idxs)\n",
        "    n_ex = dataset_ex.__len__()\n",
        "\n",
        "    # concatenate the two datasets into a unique one\n",
        "    dataset = torch.utils.data.ConcatDataset([dataset_tasks, dataset_ex])\n",
        "    loader = DataLoader(dataset, batch_size=self.batch_size, num_workers=4, shuffle=False)\n",
        "\n",
        "    train_dataset_to_reduce = np.zeros((0,64), dtype=float)\n",
        "    train_labels = np.zeros((0,), dtype=int)\n",
        "\n",
        "    # il modello è già in evaluation, non serve specificarlo\n",
        "    for _, inputs, labels in loader:\n",
        "      inputs = inputs.cuda()\n",
        "      features = self.feature_extractor(inputs).cpu().detach().numpy()\n",
        "      train_dataset_to_reduce = np.concatenate((train_dataset_to_reduce, features), axis=0)\n",
        "      train_labels = np.concatenate((train_labels, labels.detach().numpy()))\n",
        "\n",
        "    # ristabilisci il vecchio transform\n",
        "    self.original_training_set.dataset.transform = transform\n",
        "\n",
        "    # colori di ogni label\n",
        "    color_labels = np.array([self.label2hexcolor[label] for label in train_labels])\n",
        "    \n",
        "    # trasformazione\n",
        "    X_transformed = TSNE(n_components=2).fit_transform(train_dataset_to_reduce)\n",
        "\n",
        "    # figura\n",
        "    fig, ax = plt.subplots(figsize=(1.618*6, 6))\n",
        "    X_not_exemplars = X_transformed[:-n_ex]\n",
        "    not_ex_colors = color_labels[:-n_ex]\n",
        "    ax.scatter(X_not_exemplars[:,0], X_not_exemplars[:,1], color=not_ex_colors, alpha=0.2)\n",
        "    \n",
        "    # setta dimensione dei markers\n",
        "    if task_num==0:\n",
        "      s = 15\n",
        "    elif task_num==4:\n",
        "      s = 20\n",
        "    elif task_num==9:\n",
        "      s = 25\n",
        "\n",
        "    X_exemplars = X_transformed[-n_ex:]\n",
        "    ex_colors = color_labels[-n_ex:]\n",
        "    ax.scatter(X_exemplars[:,0], X_exemplars[:,1], color=ex_colors, s=s)\n",
        "    \n",
        "    # elimina le etichette dagli assi\n",
        "    ax.tick_params(bottom=False, left=False, labelbottom=False, labelleft=False)\n",
        "\n",
        "    # salva la figura\n",
        "    plt.savefig(self.name+\"_tsne_\"+str(task_num)+\".png\") \n",
        "    plt.show()\n",
        "\n",
        "  def training_model(self):\n",
        "    \n",
        "    train_indices = self.original_training_set.get_batch_indexes()\n",
        "    test_indices = self.original_test_set.get_batch_indexes()\n",
        "    batches=self.original_training_set.getbatches()\n",
        "    current_test_indexes=[]\n",
        "    test_acc = []\n",
        "    last_test = False\n",
        "\n",
        "    for i in range(self.NUM_BATCHES):\n",
        "\n",
        "      if i == self.NUM_BATCHES-1:\n",
        "        last_test = True\n",
        "\n",
        "      # crea il dataset fatto solo da exemplars\n",
        "      current_exemplar_indices = np.array([], dtype=int)\n",
        "      for exemplar_set in self.exemplar_sets:\n",
        "        current_exemplar_indices = np.concatenate([current_exemplar_indices, np.array(exemplar_set)])\n",
        "      exemplar_dataset = Subset(self.original_exemplar_set, current_exemplar_indices) \n",
        "\n",
        "      # crea il dataset con le immagini per questa task\n",
        "      train_dataset = Subset(self.original_training_set, train_indices[i])\n",
        "\n",
        "      # setta il loader su cui trainare la rete durante l'update parameters\n",
        "      if  i:\n",
        "        self.trainloader = DataLoader(torch.utils.data.ConcatDataset([train_dataset, exemplar_dataset]), \\\n",
        "                                      batch_size=self.batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
        "      else:\n",
        "        self.trainloader = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True, num_workers=4)       \n",
        "\n",
        "      # test dataset e test dataloader\n",
        "      current_test_indexes += test_indices[i].tolist()\n",
        "      test_dataset = Subset(self.original_test_set,current_test_indexes)\n",
        "      self.testloader = DataLoader(test_dataset, batch_size=self.batch_size, shuffle=True, num_workers=4, drop_last=True) \n",
        "\n",
        "      # metti il modello in train\n",
        "      self.train()\n",
        "      self.update_parameters()    \n",
        "      self.classes_seen += 10\n",
        "      self.eval() # Set Network to evaluation mode\n",
        "\n",
        "      # reduce exemplars\n",
        "      m=int(2000/(int(i*10+10)))\n",
        "      self.reduce_old_exemplars(m) \n",
        "      # add new exemplar sets\n",
        "      for classlabel in batches[i]:\n",
        "        indexes_class = self.original_training_set.get_class_indexes(classlabel)\n",
        "        # current_class = Subset(self.original_training_set, indexes_class)\n",
        "        #self.get_new_exemplars(current_class, m) #HERDING\n",
        "        self.get_new_exemplars(indexes_class, m) # RANDOM\n",
        "      \n",
        "      # tsne evaluation\n",
        "      if i==0 or i==4 or i==9:\n",
        "        self.add_tsne_plot(i)\n",
        "\n",
        "      # compute means of exemplar set\n",
        "      # cycle for each exemplar set\n",
        "      self.exemplar_means = torch.zeros((0, self.feature_size), dtype=torch.float).cuda()\n",
        "      self.exemplar_labels = []\n",
        "      for i in range(len(self.exemplar_sets)):\n",
        "        exemplars_dataset = Subset(self.original_training_set, self.exemplar_sets[i])\n",
        "        exemplars_loader = torch.utils.data.DataLoader(exemplars_dataset, batch_size=self.batch_size, shuffle=False, num_workers=4)\n",
        "        ex_features = torch.zeros((0, self.feature_size), dtype=torch.float).cuda() # alla fine shape: (len(exemplar_set), feature_size) --> (m, 64)\n",
        "      \n",
        "        with torch.no_grad():\n",
        "          _, _, exemplar_label = self.original_training_set.__getitem__(self.exemplar_sets[i][0]) \n",
        "          self.exemplar_labels.append(exemplar_label)\n",
        "          # cycle for each batch in the current exemplar set\n",
        "          for _,  exemplars, _ in exemplars_loader:\n",
        "          \n",
        "            # get exemplars features\n",
        "            exemplars = exemplars.cuda()\n",
        "            features = self.feature_extractor(exemplars) # shape: (len(exemplars), feature_size)\n",
        "          \n",
        "            # normalize \n",
        "            feature_norms = torch.norm(features, p=2, dim=1) # shape: len(exemplars)\n",
        "            feature_norms.unsqueeze_(1) # shape: (len(exemplars), 1)\n",
        "            features = features/feature_norms\n",
        "          \n",
        "            # concatenate over columns\n",
        "            ex_features = torch.cat((ex_features, features), dim=0)\n",
        "          \n",
        "        # compute current exemplar set mean and normalize it\n",
        "        ex_mean = torch.mean(ex_features, dim=0) # shape: feature_size --> 64\n",
        "        ex_mean = ex_mean/torch.norm(ex_mean)\n",
        "        ex_mean.unsqueeze_(0) # shape: (1, feature_size) --> (1, 64)\n",
        "        self.exemplar_means = torch.cat((self.exemplar_means, ex_mean), dim=0) # shape: (n_examplar set, feature size)\n",
        "      \n",
        "      print('accuracy on training set:', 100*self.__accuracy_fc(self.trainloader,self,self.diz))\n",
        "      # print('accuracy on test set:', self.__accuracy_on(self.testloader,self,self.diz))\n",
        "      current_test_acc = self.__accuracy_nme(self.testloader, self, last_test)\n",
        "      print('accuracy on test set:', 100*current_test_acc)\n",
        "      print('-' * 80)\n",
        "      test_acc.append(current_test_acc)\n",
        "\n",
        "    # compute comfusion matrix and save results\n",
        "    cm = self.plot_confusion_matrix()\n",
        "    with open(self.name+\"_cm\", 'wb') as file:\n",
        "      pickle.dump(cm, file, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "    with open(self.name+\"_testacc\", 'wb') as file:\n",
        "      pickle.dump(test_acc, file, protocol=pickle.HIGHEST_PROTOCOL)"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iNrgT-rf7uNJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118,
          "referenced_widgets": [
            "aafe00c5af80418c98eff425415931fc",
            "33da1aac471645f991f776491d0e1979",
            "4ad469eaf6c948ecb3165e775b1d78df",
            "4c6acf59ea4b40649894fcfcd40de769",
            "89c854557f30456aa08d5934fdc935a2",
            "98466a50ec39450fae2eb51197c9b7f1",
            "a16dc5847f554f6ca50abaf11bc3e637",
            "b10de0b7ff4a4773bbc49dd4b2e2e692"
          ]
        },
        "outputId": "9f7b1e9b-db6f-4da7-8844-c339ef76bd88"
      },
      "source": [
        "model = icarl(randomseed=203, name='iCarlCollage').cuda()\n",
        "model.training_model()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "aafe00c5af80418c98eff425415931fc",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=70.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_8lLULrquwY",
        "outputId": "71fede3d-e240-40bc-fffb-058c5f3132e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        }
      },
      "source": [
        "plt.imshow(tensor2im(xxx.__getitem__(0)[1]))"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f64035017f0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7jouboTrR9U"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}