{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Immagini sintetiche integrate nell'allenamento.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d624392a703f49408db7b1f032e342c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_82050864a2b94a7a8241ea2b2bf1d308",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_f53fd5a6a6b54eae8f933e2fda3d09bc",
              "IPY_MODEL_c2a985c6e58e4cc1bf2db2bf154ef322"
            ]
          }
        },
        "82050864a2b94a7a8241ea2b2bf1d308": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f53fd5a6a6b54eae8f933e2fda3d09bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_ee33d75a3e37436b8bf618c293b25094",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 70,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 70,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_033c4ad73ab745d19d85474eb07de1cc"
          }
        },
        "c2a985c6e58e4cc1bf2db2bf154ef322": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_5081e31785f24b0e962d0a6545e1ce83",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 70/70 [04:35&lt;00:00,  3.93s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f0af6af94bb845a1b6e66c2355bd71f4"
          }
        },
        "ee33d75a3e37436b8bf618c293b25094": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "033c4ad73ab745d19d85474eb07de1cc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5081e31785f24b0e962d0a6545e1ce83": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f0af6af94bb845a1b6e66c2355bd71f4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "44719370c7ea450095f24bebfa326f8e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_03be1f1817f74f6784c98050ccf40c68",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_65c8e2cc95874f02a5a1a4b8beab441f",
              "IPY_MODEL_0ddb4beee7274d52b132a64f2f437aef"
            ]
          }
        },
        "03be1f1817f74f6784c98050ccf40c68": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "65c8e2cc95874f02a5a1a4b8beab441f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_652527b14b7e4ca8843f38a9308172a7",
            "_dom_classes": [],
            "description": " 87%",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 30,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 26,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_15d0ad20c1ed4d8ca34af3c2c928a62d"
          }
        },
        "0ddb4beee7274d52b132a64f2f437aef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_9c35b00182684d708a476d2b7dc8e05a",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 26/30 [01:24&lt;00:13,  3.25s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_790d22d6b23940bcbaad14117c025cd8"
          }
        },
        "652527b14b7e4ca8843f38a9308172a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "15d0ad20c1ed4d8ca34af3c2c928a62d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9c35b00182684d708a476d2b7dc8e05a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "790d22d6b23940bcbaad14117c025cd8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2b3795afe1134cb0ad45755c3bb7ed0a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_31630c0adb3c491b840a0746924a5a01",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_9de3ce5233bc4a82846cbcfded29f8b4",
              "IPY_MODEL_f834166f2b4d4349b43f336aa0ebb4ad"
            ]
          }
        },
        "31630c0adb3c491b840a0746924a5a01": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9de3ce5233bc4a82846cbcfded29f8b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_78b454e44ef34d2b9f62c3b372955d41",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 2000,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 2000,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_357a278744b84bbaa75f157495caa3f4"
          }
        },
        "f834166f2b4d4349b43f336aa0ebb4ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_0b14aae1281541d6aa32de6fd39908c2",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 2000/2000 [25:35&lt;00:00,  1.30it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_43399d35dff94120b6399a471595bda9"
          }
        },
        "78b454e44ef34d2b9f62c3b372955d41": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "357a278744b84bbaa75f157495caa3f4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0b14aae1281541d6aa32de6fd39908c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "43399d35dff94120b6399a471595bda9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "fcdbd4e864714267b71068777e36fba8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_1de959783e4346eaa898c0fc046d033a",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_825ce2f2713c4e4a8f2df986e1b020a8",
              "IPY_MODEL_a387fbb509c644ffa2667320297a7fc1"
            ]
          }
        },
        "1de959783e4346eaa898c0fc046d033a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "825ce2f2713c4e4a8f2df986e1b020a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_c78f7dbd2c0d47e8b810ee9171d37809",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 70,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 70,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ef67c09006fa4d13aa45af4ae514807f"
          }
        },
        "a387fbb509c644ffa2667320297a7fc1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_46eb4e799199490198d9b051f6a9558c",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 70/70 [28:41&lt;00:00, 24.59s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f9510ed97f424e4abf6e3ad82780c73b"
          }
        },
        "c78f7dbd2c0d47e8b810ee9171d37809": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ef67c09006fa4d13aa45af4ae514807f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "46eb4e799199490198d9b051f6a9558c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f9510ed97f424e4abf6e3ad82780c73b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5b6e922aba81479fa8f3a01aef71eb5c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_681f49e1c68a45d589dd407747be64c8",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_7158724f1c5a4f8894d2daafdd4b751e",
              "IPY_MODEL_07e9a6340b0f4c47b99a88142de4c1cc"
            ]
          }
        },
        "681f49e1c68a45d589dd407747be64c8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7158724f1c5a4f8894d2daafdd4b751e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_9719812fbf0d4a29bc690a129f00a83e",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 70,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 70,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_920abf2d81174771898ad0a35429dbff"
          }
        },
        "07e9a6340b0f4c47b99a88142de4c1cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_aa37213d5f9d4b55910280aead830a5d",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 70/70 [00:48&lt;00:00,  1.46it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_365456adc01b4304aa32562e82c0f0e9"
          }
        },
        "9719812fbf0d4a29bc690a129f00a83e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "920abf2d81174771898ad0a35429dbff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "aa37213d5f9d4b55910280aead830a5d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "365456adc01b4304aa32562e82c0f0e9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alessandronicolini/IncrementalLearning/blob/main/exemplar_sintetici_con_pretreinata.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WH7YHXeh0hFj",
        "outputId": "b886b775-9762-49cc-e682-99ded638ffa4"
      },
      "source": [
        "!pip3 install 'import_ipynb'\n",
        "!pip3 install 'tqdm'\n",
        " \n",
        "!rm -r IncrementalLearning\n",
        "# upload work files from your git hub repository\n",
        "import sys\n",
        " \n",
        "!git clone https://github.com/alessandronicolini/IncrementalLearning.git # clone proj repository\n",
        "!rm -rf IncrementalLearning/README.md \n",
        "!rm -rf IncrementalLearning/baselines.ipynb\n",
        " \n",
        "path = 'IncrementalLearning/'\n",
        "if path not in sys.path:\n",
        "    sys.path.append('IncrementalLearning/')\n",
        " \n",
        "!pip3 install import_ipynb"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: import_ipynb in /usr/local/lib/python3.6/dist-packages (0.1.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.41.1)\n",
            "Cloning into 'IncrementalLearning'...\n",
            "remote: Enumerating objects: 187, done.\u001b[K\n",
            "remote: Counting objects: 100% (187/187), done.\u001b[K\n",
            "remote: Compressing objects: 100% (186/186), done.\u001b[K\n",
            "remote: Total 661 (delta 117), reused 0 (delta 0), pack-reused 474\u001b[K\n",
            "Receiving objects: 100% (661/661), 906.71 KiB | 8.09 MiB/s, done.\n",
            "Resolving deltas: 100% (392/392), done.\n",
            "Requirement already satisfied: import_ipynb in /usr/local/lib/python3.6/dist-packages (0.1.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UEF9KBox0cAd"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import VisionDataset\n",
        "from PIL import Image\n",
        "import random\n",
        "import torchvision\n",
        "ROOT = './data'\n",
        "class ilCIFAR100(VisionDataset):\n",
        "    \"\"\"\n",
        "    Extends CIFAR100 class. Split the dataset into 10 batches, each one containing 10 classes.\n",
        "    You can retrieve the batches from the attribute \"batches\", it has different structure according to\n",
        "    test and train CIFAR100 splits:\n",
        "        - train -> batches is a dictionary {0:{'train':indexes, 'val':indexes}...} \n",
        "        - test -> batches is a dictionary {0:indexes...}\n",
        "    where the keys are the batch number.\n",
        "\n",
        "    Args:\n",
        "        root (string): Root directory of dataset where directory\n",
        "            `cifar-10-batches-py` exists or will be saved to if download is set to True.\n",
        "        seed(int): used to ensure reproducibility in shuffling operations.\n",
        "        val_size(float, optional): between 0 and 1, fraction of data used for validation.\n",
        "        train (bool, optional): If True, creates dataset from training set, otherwise\n",
        "            creates from test set.\n",
        "        transform (callable, optional): A function/transform that takes in an PIL image\n",
        "            and returns a transformed version. E.g, `transforms.RandomCrop`\n",
        "        target_transform (callable, optional): A function/transform that takes in the\n",
        "            target and transforms it.\n",
        "        download (bool, optional): If true, downloads the dataset from the internet and\n",
        "            puts it in root directory. If dataset is already downloaded, it is not\n",
        "            downloaded again.\n",
        "    \"\"\"\n",
        "    def __init__(self,classes_per_batch, seed, val_size=0.1, train=True, transform=None, target_transform=False, \n",
        "    download=True):\n",
        "        \n",
        "        super(ilCIFAR100, self).__init__(root=0)\n",
        "        self.classes_per_batch=classes_per_batch\n",
        "\n",
        "        \n",
        "        self.__rs = seed # set random seed \n",
        "        self.train=train\n",
        "        self.__transform_train = transforms.Compose([\n",
        "            transforms.RandomCrop(32, padding=4),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))\n",
        "        ])\n",
        "\n",
        "        self.__transform_test = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))\n",
        "        ])\n",
        "        # if train:\n",
        "        #     self.batches = self.__make_train_batches(val_size)\n",
        "        # else:\n",
        "        #     self.batches = self.__make_test_batches()\n",
        "        if self.train == 'train':\n",
        "          self.dataset = torchvision.datasets.CIFAR100(root=ROOT, train=True,\n",
        "                                            download=True, transform=self.__transform_train)\n",
        "        elif self.train == 'exemplars':\n",
        "          self.dataset = torchvision.datasets.CIFAR100(root=ROOT, train=True,\n",
        "                                            download=True, transform=self.__transform_test)\n",
        "        else:\n",
        "          self.dataset = torchvision.datasets.CIFAR100(root=ROOT, train=False,\n",
        "                                        download=True, transform=self.__transform_test)\n",
        "          \n",
        "        self.targets = np.array(self.dataset.targets) # make targets an array to exploit masking\n",
        "        random.seed(seed)\n",
        "        self.classes = random.sample(range(0, 100), 100)\n",
        "        #self.classes = self.classes.reshape((10, -1)) # each row contains the classes for the corrisponding batch\n",
        "        #print(self.classes)\n",
        "        self.__dictionary = {}\n",
        "        for i, c in enumerate(self.classes):\n",
        "          self.__dictionary[c] = i\n",
        "\n",
        "\n",
        "    def get_dict(self):\n",
        "      return self.__dictionary\n",
        "    def __getitem__(self, index):\n",
        "        return index,self.dataset.__getitem__(index)[0],self.dataset.__getitem__(index)[1]\n",
        "    def __len__(self):\n",
        "        return self.dataset.__len__()\n",
        "    def getbatches(self):\n",
        "      classlist=self.classes\n",
        "      batches=[]\n",
        "      for i in range(0,int(100/self.classes_per_batch)):\n",
        "        #print(i)\n",
        "        batch=classlist[int(i*self.classes_per_batch):int(i*10+self.classes_per_batch)]\n",
        "        batches.append(batch)\n",
        "      return batches\n",
        "    def get_batch_indexes(self):\n",
        "      classlist=self.classes\n",
        "      numclass=self.classes_per_batch\n",
        "      batch_indexes=[]\n",
        "      for i in range(0,int(100/self.classes_per_batch)):\n",
        "        batch=classlist[int(i*numclass):int(i*numclass+numclass)]\n",
        "        mask=np.isin(self.targets,batch)\n",
        "        indexes=np.array(np.arange(len(self.dataset.targets)))\n",
        "        indexes=indexes[mask]\n",
        "        batch_indexes.append(indexes)\n",
        "      return batch_indexes\n",
        "    def get_class_indexes(self,label):\n",
        "      indexes = np.array(np.arange(len(self.dataset.targets)))\n",
        "      labels = self.dataset.targets\n",
        "      mask = np.isin(labels, label)\n",
        "      indexes = indexes[mask]\n",
        "\n",
        "      return indexes\n",
        "    def get_train_val(self,valid):\n",
        "      batches=self.get_batch_indexes()\n",
        "      train=[]\n",
        "      val=[]\n",
        "      for batch in batches:\n",
        "        #print(type(batch))\n",
        "        random.shuffle(batch)\n",
        "        valbatch=batch[0:int(valid*len(batch))]\n",
        "        trainbatch=batch[int(valid*len(batch)):]\n",
        "        train.append(trainbatch)\n",
        "        val.append(valbatch)\n",
        "      return train,val\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import time\n",
        "import os\n",
        "import argparse\n",
        "import numpy as np\n",
        "\n",
        "def tensor2im(input_image, imtype=np.uint8):\n",
        "    #CIFAR100\n",
        "    mean = [0.5071, 0.4867, 0.4408] \n",
        "    std = [0.2675, 0.2565, 0.2761]\n",
        "    #CIFAR10\n",
        "    #mean = [0.4914, 0.4822, 0.4465]\n",
        "    #std = [0.2023, 0.1994, 0.2010]\n",
        "    if not isinstance(input_image, np.ndarray):\n",
        "        if isinstance(input_image, torch.Tensor):\n",
        "            image_tensor = input_image.data\n",
        "        else:\n",
        "            return input_image\n",
        "        image_numpy = image_tensor.cpu().detach().float().numpy()\n",
        "        if image_numpy.shape[0] == 1:\n",
        "            image_numpy = np.tile(image_numpy, (3, 1, 1))\n",
        "        for i in range(len(mean)): \n",
        "            image_numpy[i] = image_numpy[i] * std[i] + mean[i]\n",
        "        image_numpy = image_numpy * 255\n",
        "        image_numpy = np.transpose(image_numpy, (1, 2, 0))\n",
        "    else:\n",
        "        image_numpy = input_image\n",
        "    return image_numpy.astype(imtype)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5O4jUchQ1EAI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ac2b362-43a1-49e9-d2a6-d4cadaa03a9e"
      },
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import math\n",
        "from sklearn.preprocessing import normalize\n",
        "import copy\n",
        "import torchvision.datasets as dsets\n",
        "import torchvision.models as models\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.gridspec as gridspec\n",
        "from torch.utils.data import Subset, DataLoader, Dataset\n",
        "import random\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pickle\n",
        "import pandas as pd\n",
        "\n",
        "import import_ipynb\n",
        "#from IncrementalLearning.cifar100 import ilCIFAR100\n",
        "\n",
        "from IncrementalLearning.resnet_cifar_pretrained import resnet32\n",
        "from tqdm.notebook import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "importing Jupyter notebook from /content/IncrementalLearning/resnet_cifar_pretrained.ipynb\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        },
        "id": "RAoV7Gq_aBW3",
        "outputId": "ae4eba19-2ae4-40ad-fba0-86d2bb24a66e"
      },
      "source": [
        "class mnemonics():\n",
        "  def __init__(self, randomseed):\n",
        "    self.device = 'cuda'\n",
        "    self.model = resnet32(num_classes=100).to(self.device)\n",
        "    self.feature_extractor = self.model.features\n",
        "    self.temp_model = None\n",
        "    self.lr = 2\n",
        "    self.gamma = 0.2\n",
        "    self.weight_decay = 1e-5 \n",
        "    self.milestones = [49,63]\n",
        "    self.batch_size = 128\n",
        "    self.numepochs = 70\n",
        "    self.n_classes = 0\n",
        "    self.n_known = 0\n",
        "    self.feature_size=64\n",
        "    self.momentum=0.9\n",
        "    self.criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    self.NUM_BATCHES=10\n",
        "    self.randomseed=randomseed\n",
        "    self.trainloader=None\n",
        "    self.testloader=None\n",
        "    self.CLASSES_PER_BATCH=10\n",
        "\n",
        "    self.original_training_set = ilCIFAR100(self.CLASSES_PER_BATCH, self.randomseed, train = 'train')\n",
        "    self.original_exemplar_set = ilCIFAR100(self.CLASSES_PER_BATCH, self.randomseed, train = 'exemplars')\n",
        "    self.original_test_set = ilCIFAR100(self.CLASSES_PER_BATCH,self.randomseed, train= 'test')\n",
        "\n",
        "    self.last_test = None\n",
        "    self.y_pred = []\n",
        "    self.y_test = []\n",
        "\n",
        "    self.cumulative_class_mean = []\n",
        "\n",
        "    self.classes_seen=0\n",
        "    self.diz = self.original_training_set.get_dict()\n",
        "\n",
        "    self.exemplar_features_mean = None\n",
        "    # lista di liste, ogni lista contiene gli exemplars di una classe\n",
        "    self.exemplar_sets_idxs = [] # mn_exemplat_sets\n",
        "    # lista unica, tutti gli indici degli exemplar\n",
        "    self.exemplar_idxs = []\n",
        "\n",
        "  '''\n",
        "  def update_params(self, \n",
        "                    m,\n",
        "                    finetuning_idxs, \n",
        "                    training_idxs, \n",
        "                    mnemonics_to_optimize, \n",
        "                    batch_size,\n",
        "                    new=True,\n",
        "                    lr=10, \n",
        "                    momentum=0.9, \n",
        "                    weight_decay=1e-5, \n",
        "                    milestones=[10, 20, 30, 40],\n",
        "                    gamma=0.5, \n",
        "                    tuning_epochs=4,\n",
        "                    updating_epochs=50):\n",
        "    \n",
        "    \"\"\"\n",
        "    finetuning_idxs = indexes of current task elements\n",
        "    mnemonics_idxs = indexes of exemplar elements\n",
        "    mnemonics_to_optimize = the optimized parameters in the update phase\n",
        "    \"\"\"\n",
        "\n",
        "    # make a copy of the model\n",
        "    model_copy = copy.deepcopy(self.model)\n",
        "    model_copy.train()\n",
        "    model_copy.to(self.device)\n",
        "\n",
        "    # define the loss\n",
        "    # criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    # FINE TUNING FOR 1 EPOCH eq. 8 --------------------------------------------\n",
        "    \n",
        "    # define optimizer and scheduler for fine tuning phase\n",
        "    optimizer = optim.SGD(model_copy.parameters(), lr=2, momentum=momentum, weight_decay=weight_decay)\n",
        "    \n",
        "    # create the subset dataset to load the data you want, and the loader\n",
        "    finetuning_labels = np.array([self.original_training_set.__getitem__(idx)[2] for idx in finetuning_idxs], dtype=int)\n",
        "    meta_idxs = [i for i in range(len(finetuning_idxs))]\n",
        "    random.shuffle(meta_idxs)\n",
        "\n",
        "    # split the meta idxs in batches\n",
        "    n_batches = int(np.floor(len(finetuning_idxs)/batch_size))\n",
        "    meta_idxs_batches = []\n",
        "    for i in range(n_batches):\n",
        "      meta_idxs_batches.append(np.array(meta_idxs[batch_size*i:batch_size*(i+1)]))\n",
        "    meta_idxs_batches.append(np.array(meta_idxs[batch_size*n_batches:]))\n",
        "\n",
        "    # now fine tune the copied model\n",
        "    for epoch in range(tuning_epochs):\n",
        "      for meta_idxs_batch in meta_idxs_batches:\n",
        "        inputs = mnemonics_to_optimize[0][meta_idxs_batch] # are already in cuda\n",
        "        labels = finetuning_labels[meta_idxs_batch]\n",
        "        labels = torch.tensor([self.diz[c] for c in labels])\n",
        "        labels = labels.to(self.device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model_copy(inputs)\n",
        "        labels_encoded = F.one_hot(labels,100).float().to(self.device)\n",
        "        loss = self.criterion(outputs, labels_encoded)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    \n",
        "\n",
        "    # UPDATE THE MNEMONICS eq.9/10 ---------------------------------------------\n",
        "    \n",
        "    model_copy.eval()\n",
        "    \n",
        "    optimizer = optim.SGD(mnemonics_to_optimize, lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
        "    scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=milestones, gamma=gamma)\n",
        "    \n",
        "\n",
        "    if new:\n",
        "      exlvl_training = Subset(self.original_training_set, training_idxs)\n",
        "      exlvl_loader = DataLoader(exlvl_training, batch_size=batch_size, shuffle=True, num_workers=4)\n",
        "\n",
        "      current_task_labels = set([self.original_training_set.__getitem__(idx)[2] for idx in training_idxs])\n",
        "      new_dict = {label:new_label for label, new_label in zip(current_task_labels, range(10))}\n",
        "\n",
        "      new_class_mean = {new_dict[key] : value for key, value in self.cumulative_class_mean.items()}\n",
        "      means_ready = torch.Tensor(list(new_class_mean.values())).to(self.device)\n",
        "\n",
        "    \n",
        "    else:\n",
        "      exlvl_training = Subset(self.original_exemplar_set, training_idxs)\n",
        "      exlvl_loader = DataLoader(exlvl_training, batch_size=batch_size, shuffle=True, num_workers=4)\n",
        "\n",
        "    print('lunghezza del exemplar update:', len(training_idxs))\n",
        "    for epoch in tqdm(range(updating_epochs)):\n",
        "\n",
        "      for _, inputs, labels in exlvl_loader:\n",
        "\n",
        "        if new:\n",
        "          labels = torch.tensor([new_dict[c.item()] for c in labels])\n",
        "        else:\n",
        "          labels = torch.tensor([self.diz[c.item()] for c in labels])\n",
        "\n",
        "        labels = labels.to(self.device)\n",
        "        inputs = inputs.to(self.device)\n",
        "        optimizer.zero_grad()\n",
        "        out_features = model_copy.features(inputs)\n",
        "        # compute features mean of mnemonics for each class\n",
        "        if new:\n",
        "          the_logits = F.linear(F.normalize(out_features, p=2, dim=1), F.normalize(means_ready, p=2, dim=1))\n",
        "        else:\n",
        "          ##Da capire questa cosa!!!!!!! se provassi a mettere self?\n",
        "          #OPPURE uso un altro tensore copiando all_class_means_calcolato_fuori?  \n",
        "          #dov'è il gradiente?\n",
        "          n_classes = int(len(finetuning_labels)/m)\n",
        "          all_class_means = torch.zeros((0, 64))\n",
        "          all_class_means = all_class_means.to(self.device)\n",
        "          for i in range(n_classes): # how many classes\n",
        "            mnemonics_features = model_copy.features(mnemonics_to_optimize[0][i*m:(i+1)*m])\n",
        "            this_class_means = torch.mean(mnemonics_features, dim=0) # size 64\n",
        "            this_class_means = torch.unsqueeze(this_class_means, dim=0) # add the second dimension\n",
        "            all_class_means = torch.cat((all_class_means, this_class_means), dim=0)\n",
        "          the_logits = F.linear(F.normalize(out_features, p=2, dim=1), F.normalize(all_class_means, p=2, dim=1))\n",
        "\n",
        "        #labels_encoded = F.one_hot(labels,100).float().cuda()\n",
        "\n",
        "        loss = F.cross_entropy(the_logits, labels) # al secondo batch di classi per i new mnemonics le uscite sono sempre 10 ma le label vanno da 10 a 19\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "  def exemplar_level_optimization(self, m, task_num, current_task_indices):  \n",
        "    \n",
        "    # UPDATING NEW EXEMPLAR-----------------------------------------------------\n",
        "\n",
        "    # isola gli indici dei nuovi exemplars\n",
        "    new_exemplar_idxs = []\n",
        "    for idxs in self.exemplar_sets_idxs[-10:]:\n",
        "      new_exemplar_idxs += idxs\n",
        "\n",
        "    # ora ottieni gli mnemonics che poi sono da ottimizzare\n",
        "    new_mnemonics_data = torch.zeros((10*m, 3, 32, 32))\n",
        "\n",
        "    for i, idx in enumerate(new_exemplar_idxs):\n",
        "      new_mnemonics_data[i, :, :, :] = self.original_training_set.__getitem__(int(idx))[1]\n",
        "\n",
        "    new_mnemonics = nn.ParameterList()\n",
        "    new_mnemonics.append(nn.Parameter(new_mnemonics_data))\n",
        "    new_mnemonics.to(self.device)\n",
        "    \n",
        "    #print(new_mnemonics[0][0])\n",
        "\n",
        "    options_new ={'finetuning_idxs': new_exemplar_idxs, \n",
        "                  'training_idxs': current_task_indices, \n",
        "                  'mnemonics_to_optimize':  new_mnemonics,  \n",
        "                  'batch_size':128,\n",
        "                  'm':m}\n",
        "\n",
        "    print('---start mnemonics updating---')\n",
        "\n",
        "    self.update_params(**options_new)    \n",
        "\n",
        "    for i, idx in enumerate(new_exemplar_idxs):\n",
        "      self.original_exemplar_set.dataset.data[idx] = tensor2im(new_mnemonics[0][i])\n",
        "\n",
        "    \n",
        "    # UPDATING OLD EXEMPLARS ---------------------------------------------------\n",
        "\n",
        "    if task_num:\n",
        "      # decidi quanti elementi ha ogni exemlar set in a e in b a seconda se m è \n",
        "      # pari o dispari\n",
        "      if m%2:\n",
        "        l_a = int((m+1)/2)\n",
        "      else:\n",
        "        l_a = int(m/2)\n",
        "      l_b = int(m-l_a)\n",
        "\n",
        "      # isola gli indici dei vecchi exemplars, dividendoli in due parti\n",
        "      # ogni classe deve avere circa la metà degli exemplar originali\n",
        "      old_exemplar_idxs_a = []\n",
        "      old_exemplar_idxs_b = []\n",
        "      \n",
        "      for idxs in self.exemplar_sets_idxs[:-10]:\n",
        "        old_exemplar_idxs_a += idxs[:l_a]\n",
        "        old_exemplar_idxs_b += idxs[l_a:]\n",
        "\n",
        "      old_mnemonics_data_a = torch.zeros((task_num*10*l_a, 3, 32, 32))\n",
        "      old_mnemonics_data_b = torch.zeros((task_num*10*l_b, 3, 32, 32))\n",
        "\n",
        "      for i, idx in enumerate(old_exemplar_idxs_a):\n",
        "        old_mnemonics_data_a[i, :, :, :] = self.original_training_set.__getitem__(int(idx))[1]\n",
        "      \n",
        "      for i, idx in enumerate(old_exemplar_idxs_b):\n",
        "          old_mnemonics_data_b[i, :, :, :] = self.original_training_set.__getitem__(int(idx))[1]\n",
        "      \n",
        "      old_mnemonics_a = nn.ParameterList()\n",
        "      old_mnemonics_a.append(nn.Parameter(old_mnemonics_data_a))\n",
        "      old_mnemonics_a.to(self.device)\n",
        "      old_mnemonics_b = nn.ParameterList()\n",
        "      old_mnemonics_b.append(nn.Parameter(old_mnemonics_data_b))\n",
        "      old_mnemonics_b.to(self.device)\n",
        "\n",
        "      options_old_a = {'finetuning_idxs':old_exemplar_idxs_a, \n",
        "                       'training_idxs':old_exemplar_idxs_b, \n",
        "                       'mnemonics_to_optimize':old_mnemonics_a, \n",
        "                       'batch_size':128,\n",
        "                       'm': l_a,\n",
        "                       'new':False}\n",
        "\n",
        "      options_old_b = {'finetuning_idxs':old_exemplar_idxs_b, \n",
        "                       'training_idxs':old_exemplar_idxs_a, \n",
        "                       'mnemonics_to_optimize':old_mnemonics_b, \n",
        "                       'batch_size':128,\n",
        "                       'm':l_b,\n",
        "                       'new':False}\n",
        "\n",
        "      self.update_params(**options_old_a) \n",
        "      self.update_params(**options_old_b)\n",
        "\n",
        "      # CONVERT AND STORE UPDATED EXEMPLAR as numpy array\n",
        "\n",
        "      for i, idx in enumerate(old_exemplar_idxs_a):\n",
        "        self.original_exemplar_set.dataset.data[idx] = tensor2im(old_mnemonics_a[0][i])\n",
        "\n",
        "      for i, idx in enumerate(old_exemplar_idxs_b):\n",
        "        self.original_exemplar_set.dataset.data[idx] = tensor2im(old_mnemonics_b[0][i])\n",
        "    \n",
        "\n",
        "\n",
        "    # FINE TUNE THE CURRENT NET ON ALL THE EXEMPLARS COLLECTED 'TILL NOW\n",
        "   \n",
        "  '''\n",
        "\n",
        "  def model_level_optimization(self):\n",
        "    \n",
        "    old_model = copy.deepcopy(self.model)\n",
        "    old_model.eval()\n",
        "    old_model.to(self.device)\n",
        "    n_classes = self.classes_seen+self.CLASSES_PER_BATCH\n",
        "    print(n_classes)\n",
        "    optimizer = optim.SGD(self.model.parameters(), lr=self.lr, momentum=self.momentum, weight_decay=self.weight_decay)\n",
        "    scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=self.milestones, gamma=self.gamma)\n",
        "    for epoch in tqdm(range(self.numepochs)):\n",
        "        \n",
        "      for _, inputs, labels in self.trainloader:\n",
        "        inputs = inputs.float().to(self.device)\n",
        "        labels = torch.tensor([self.diz[c.item()] for c in labels])\n",
        "\n",
        "        labels=labels.to(self.device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs=self.model(inputs)\n",
        "\n",
        "        labels_encoded = F.one_hot(labels,100).float().to(self.device) #CAMBIARE ONE_HOT\n",
        "        \n",
        "        if self.classes_seen:\n",
        "          old_target = old_model(inputs).to(self.device)\n",
        "          old_target = torch.sigmoid(old_target).to(self.device)\n",
        "          \n",
        "          target = torch.cat((old_target[:,:self.classes_seen], labels_encoded[:, self.classes_seen:]), dim=1)\n",
        "          loss = self.criterion(outputs, target)\n",
        "        else:\n",
        "          loss = self.criterion(outputs, labels_encoded) \n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "      \n",
        "      scheduler.step()\n",
        "\n",
        "\n",
        "  def classify_nme(self, input_batch):\n",
        "    min_distances = float('inf')*torch.ones(len(input_batch)).to(self.device) # shape: batch_size --> 128\n",
        "    y_pred = torch.zeros(len(input_batch), dtype=torch.int8).to(self.device) # shape: batch_size --> 128\n",
        "    input_features = self.model.features(input_batch) # shape: (batch_size, feature_size) --> (128, 64)\n",
        "\n",
        "    for i in range(len(self.exemplar_sets_idxs)):\n",
        "      ex_mean = self.exemplar_means[i,:]\n",
        "\n",
        "      # compute distances between inputs features and exemplar set means\n",
        "      pdist = nn.PairwiseDistance(p=2)\n",
        "      distances = pdist(input_features, ex_mean) # shape: batch_size --> 128\n",
        "\n",
        "      # update min distancies and predicted labels\n",
        "      mask = distances < min_distances\n",
        "      min_distances[mask] = distances[mask]\n",
        "      y_pred[mask] = self.exemplar_labels[i]\n",
        "\n",
        "    return y_pred\n",
        "    \n",
        "\n",
        "\n",
        "  def get_new_exemplars(self, batch, m):\n",
        "    loader = torch.utils.data.DataLoader(batch, batch_size=self.batch_size,shuffle=False, num_workers=4)\n",
        "    features = np.zeros((0,self.feature_size))\n",
        "    indices = np.zeros((0), dtype=int)\n",
        "    with torch.no_grad():\n",
        "      for indexes, images, labels in loader:\n",
        "        images = images.cuda()\n",
        "        feature = self.feature_extractor(images).data.cpu().numpy()\n",
        "        feature = normalize(feature, axis=1, norm='l2')\n",
        "        features = np.concatenate((features,feature), axis=0)\n",
        "        indices = np.concatenate((indices,indexes), axis=0)\n",
        "\n",
        "    class_mean = np.mean(features, axis=0)\n",
        "    class_mean = class_mean / np.linalg.norm(class_mean)  # Normalize\n",
        "\n",
        "    #self.cumulative_class_mean.append(class_mean)\n",
        "\n",
        "    exemplar_set = []\n",
        "    exemplar_features = np.zeros((0,self.feature_size))\n",
        "\n",
        "    for k in range(1, int(m)+1):\n",
        "        S = np.sum(exemplar_features, axis=0)\n",
        "        phi = features\n",
        "        mu = class_mean\n",
        "        mu_p = 1.0 / k * (phi + S)\n",
        "        mu_p = normalize(mu_p, axis=1, norm='l2')\n",
        "        i = np.argmin(np.sqrt(np.sum((mu - mu_p) ** 2, axis=1)))\n",
        "        exemplar_set.append(int(indices[i]))\n",
        "        addfeature =  np.expand_dims(features[i], axis=0)\n",
        "        exemplar_features = np.concatenate((exemplar_features,addfeature), axis=0)\n",
        "\n",
        "        #remove duplicates\n",
        "        features = np.delete(features, i, 0)\n",
        "        indices = np.delete(indices, i, 0)\n",
        "        \n",
        "    self.exemplar_sets_idxs.append(exemplar_set)\n",
        "    #self.exemplar_sets_idxs.append(random.sample(list(batch), m))\n",
        "\n",
        "\n",
        "  def reduce_old_exemplars(self, m):\n",
        "    for i, set_i in enumerate(self.exemplar_sets_idxs):\n",
        "      self.exemplar_sets_idxs[i] = random.sample(set_i, m)\n",
        "\n",
        "  def __accuracy_fc(self, dl, mapper):\n",
        "    total = 0.0\n",
        "    correct = 0.0\n",
        "    for  _, images, labels in dl:\n",
        "      labels = torch.tensor([torch.tensor(mapper[c.item()]) for c in labels])\n",
        "      labels = labels.to(self.device)\n",
        "      images = images.to(self.device)\n",
        "      outputs = self.model(images)\n",
        "      _, preds = torch.max(outputs, dim=1)\n",
        "      total += len(labels)\n",
        "      correct += torch.sum(preds == labels).item()\n",
        "\n",
        "    acc = correct / total\n",
        "    return acc\n",
        "\n",
        "\n",
        "  def __accuracy_nme(self, dl):\n",
        "    \n",
        "    total = 0.0\n",
        "    correct = 0.0\n",
        "    \n",
        "    for  _, images, labels in dl:\n",
        "      labels = labels.to(self.device)\n",
        "      images = images.to(self.device)\n",
        "      preds = self.classify_nme(images)\n",
        "      total += len(labels)\n",
        "      correct += torch.sum(preds == labels).item()\n",
        "\n",
        "      if self.last_test:\n",
        "        self.y_pred += preds.tolist()\n",
        "        self.y_test += labels.tolist()\n",
        "\n",
        "    acc = correct / total\n",
        "    return acc\n",
        "\n",
        "\n",
        "  def plot_confusion_matrix(self):\n",
        " \n",
        "    cm = confusion_matrix(self.y_test, self.y_pred)\n",
        "    cm = np.log(cm+1)\n",
        "    fig, ax = plt.subplots(figsize=(7,7))\n",
        "    sns.heatmap(cm, square=True, cbar=False, ax=ax, cmap=plt.get_cmap('seismic'))\n",
        "    ax.set_xticks(np.linspace(19,99,5))\n",
        "    ax.set_yticks(np.linspace(19,99,5))\n",
        "    ax.set_xticklabels([20,40,60,80,100], rotation=0)\n",
        "    ax.set_yticklabels([20,40,60,80,100], rotation=0)\n",
        "    ax.set_title(\"iCaRL\")\n",
        "    ax.set_xlabel(\"Predicted class\")\n",
        "    ax.set_ylabel(\"True class\")\n",
        "    plt.savefig(\"iCaRL_\"+str(self.randomseed)+\"_cm.png\")\n",
        "    plt.show()\n",
        "    return cm\n",
        "\n",
        "  def plot_data(self, train_dl):\n",
        "\n",
        "    from sklearn.manifold import TSNE\n",
        "    print('------plot data------')\n",
        "\n",
        "    #Data points\n",
        "    train_labels_array = torch.zeros(0).to('cuda')\n",
        "    train_dataset_to_reduce = np.zeros((0,64), dtype = float)\n",
        "\n",
        "    for  _, images, labels in train_dl:\n",
        "      labels = labels.to(self.device)\n",
        "      images = images.to(self.device)\n",
        "      train_dataset_to_reduce = np.concatenate((train_dataset_to_reduce, self.feature_extractor(images).cpu().detach().numpy()))\n",
        "      train_labels_array = torch.cat((train_labels_array, labels))\n",
        "\n",
        "    \n",
        "    #EX e MN loaders \n",
        "    current_exemplar_indices = np.array([], dtype=int)\n",
        "\n",
        "    for exemplar_set in self.exemplar_sets_idxs:\n",
        "      current_exemplar_indices = np.concatenate([current_exemplar_indices, np.array(exemplar_set)])\n",
        "\n",
        "    exemplar_dataset = Subset(self.original_training_set, current_exemplar_indices)\n",
        "    ex_dl = DataLoader(exemplar_dataset, batch_size=self.batch_size, shuffle=False, num_workers=4, drop_last=True) #usato solo per plottare\n",
        "\n",
        "    mn_dataset = Subset(self.original_exemplar_set, current_exemplar_indices)\n",
        "    mn_dl = DataLoader(mn_dataset, batch_size=self.batch_size, shuffle=False, num_workers=4, drop_last=True) #usato solo per plottare\n",
        "\n",
        "\n",
        "    #Exemplars\n",
        "\n",
        "    ex_labels_array = torch.zeros(0).to('cuda')\n",
        "    ex_dataset_to_reduce = np.zeros((0,64), dtype = float)\n",
        "\n",
        "    for  _, images, labels in ex_dl:\n",
        "      labels = labels.to(self.device)\n",
        "      images = images.to(self.device)\n",
        "      ex_dataset_to_reduce = np.concatenate((ex_dataset_to_reduce, self.feature_extractor(images).cpu().detach().numpy()))\n",
        "      ex_labels_array = torch.cat((ex_labels_array, labels), dim = 0)\n",
        "\n",
        "\n",
        "    #Mnemonics\n",
        "\n",
        "\n",
        "    mn_labels_array = torch.zeros(0).to('cuda')\n",
        "    mn_dataset_to_reduce = np.zeros((0,64), dtype = float)\n",
        "\n",
        "    for  _, images, labels in mn_dl:\n",
        "      labels = labels.to(self.device)\n",
        "      images = images.to(self.device)\n",
        "      mn_dataset_to_reduce = np.concatenate((mn_dataset_to_reduce, self.feature_extractor(images).cpu().detach().numpy()))\n",
        "      mn_labels_array = torch.cat((mn_labels_array, labels), dim = 0)\n",
        "\n",
        "    #PLOT'''\n",
        "    total_data_w_exemplars = np.concatenate((train_dataset_to_reduce, ex_dataset_to_reduce))\n",
        "    total_data_w_mn =  np.concatenate((train_dataset_to_reduce, mn_dataset_to_reduce))\n",
        "\n",
        "    total_transformed_ex = TSNE(n_components=2).fit_transform(total_data_w_exemplars)\n",
        "    X_transformed_w_ex = total_transformed_ex[:train_dataset_to_reduce.shape[0]]\n",
        "    ex_transformed = total_transformed_ex[train_dataset_to_reduce.shape[0]:]\n",
        "\n",
        "    total_transformed_mn = TSNE(n_components=2).fit_transform(total_data_w_mn)\n",
        "    X_transformed_w_mn = total_transformed_mn[:train_dataset_to_reduce.shape[0]]\n",
        "    mn_transformed = total_transformed_mn[train_dataset_to_reduce.shape[0]:]\n",
        "\n",
        "    fig, (ax1, ax2) = plt.subplots(1,2, figsize=(24,12))\n",
        "    ax1.scatter(X_transformed_w_ex[:,0], X_transformed_w_ex[:,1], c = train_labels_array.cpu(), alpha = 0.1)\n",
        "    ax1.scatter(ex_transformed[:,0], ex_transformed[:,1], c = ex_labels_array.cpu(), alpha = 1)\n",
        "    #ax1.title('EXEMPLARS')\n",
        "\n",
        "    ax2.scatter(X_transformed_w_mn[:,0], X_transformed_w_mn[:,1], c = train_labels_array.cpu(), alpha = 0.1)\n",
        "    ax2.scatter(mn_transformed[:,0], mn_transformed[:,1], c = mn_labels_array.cpu(), alpha = 1)\n",
        "    #ax2.title('MNEMONICS')\n",
        "    plt.show()\n",
        "\n",
        "  def trainer(self):\n",
        "    \n",
        "    train_indices = self.original_training_set.get_batch_indexes()\n",
        "    test_indices = self.original_test_set.get_batch_indexes()\n",
        "    batches=self.original_training_set.getbatches()\n",
        "    current_test_indexes=[]\n",
        "    test_acc = []\n",
        "    self.last_test = False\n",
        "\n",
        "    for i in range(self.NUM_BATCHES):\n",
        "      print('current batches', batches[i])\n",
        "      if i == self.NUM_BATCHES-1:\n",
        "        self.last_test = True\n",
        "\n",
        "      current_exemplar_indices = np.array([], dtype=int)\n",
        "    \n",
        "      for exemplar_set in self.exemplar_sets_idxs:\n",
        "        current_exemplar_indices = np.concatenate([current_exemplar_indices, np.array(exemplar_set)])\n",
        "\n",
        "      exemplar_dataset = Subset(self.original_training_set, current_exemplar_indices) \n",
        "      #DA CAMBIARE CON SELF.ORIGINAL EXEMPLAR SET\n",
        "      if i > 1: #FINETUNING\n",
        "        print('----inizio finetuning----')\n",
        "        print('numbero of classes in the exemplar sets', len(self.exemplar_sets_idxs))\n",
        "        self.numepochs = 10\n",
        "        self.lr = 0.2\n",
        "        temporary_classes_seen = self.classes_seen\n",
        "        self.classes_seen = 0\n",
        "        self.trainloader = DataLoader(exemplar_dataset, batch_size=self.batch_size, shuffle=False, num_workers=4, drop_last=True) \n",
        "        print('accuracy on exemplar set before finetuining:', 100*self.__accuracy_fc(self.trainloader,self.diz))\n",
        "        self.model.train()\n",
        "        self.model_level_optimization()\n",
        "        #BACK TO THE NORMAL PARAMETERS\n",
        "        self.model.eval()\n",
        "        self.numepochs = 70\n",
        "        self.lr = 2\n",
        "        self.classes_seen = temporary_classes_seen\n",
        "        print('accuracy on exemplar set after finetuining:', 100*self.__accuracy_fc(self.trainloader,self.diz))\n",
        "        current_test_acc = self.__accuracy_nme(self.testloader)\n",
        "        print('accuracy on test set after finetuning:', 100*current_test_acc)\n",
        "        print('-----fine finetuning------')\n",
        "        print('-'*80)\n",
        "\n",
        "      train_dataset = Subset(self.original_training_set, train_indices[i])\n",
        "      current_test_indexes += test_indices[i].tolist()\n",
        "      test_dataset = Subset(self.original_test_set,current_test_indexes)\n",
        "      self.train_loader = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True, num_workers=4, drop_last=True)\n",
        "      self.testloader = DataLoader(test_dataset, batch_size=self.batch_size, shuffle=False, num_workers=4, drop_last=True)        \n",
        "      \n",
        "\n",
        "      if i == 0:\n",
        "        self.trainloader = self.train_loader\n",
        "      else:\n",
        "        self.trainloader = DataLoader(torch.utils.data.ConcatDataset([train_dataset, exemplar_dataset]), batch_size=self.batch_size, shuffle=True,\n",
        "          num_workers=4, pin_memory=True)\n",
        "\n",
        "        \n",
        "      self.model.train()\n",
        "      self.model_level_optimization()    \n",
        "      self.classes_seen += 10\n",
        "      self.model.eval() # Set Network to evaluation mode\n",
        "      m=int(2000/(int(i*10+10)))\n",
        "\n",
        "      \n",
        "      current_test_acc = self.__accuracy_fc(self.testloader, self.diz)\n",
        "      print('accuracy on test set:', 100*current_test_acc)\n",
        "\n",
        "      \n",
        "      return self.model, batches[i]\n",
        "\n",
        "      break\n",
        "'''\n",
        "      #NUOVO PAPER DEL PORCODDIO\n",
        "      labels_of_modified = torch.zeros(0, dtype = int).to('cuda')\n",
        "      for label in batches[i]:\n",
        "        labels = torch.LongTensor([self.diz[label]]*m).to('cuda')\n",
        "        labels_of_modified = torch.cat((labels_of_modified, labels), dim=0)\n",
        "      print('labels to be created', labels_of_modified)\n",
        "      print('len to be created', len(labels_of_modified))\n",
        "      number_of_images_created = m*10\n",
        "      net_student = resnet32(num_classes=100).to(self.device)\n",
        "      data_type = torch.float\n",
        "      inputs = torch.randn((number_of_images_created, 3, 32, 32), requires_grad=True, device='cuda', dtype=data_type)\n",
        "\n",
        "      net_student = copy.deepcopy(self.model)\n",
        "      net_student.eval() #important, otherwise generated images will be non natural\n",
        "      \n",
        "      train_writer = None  # tensorboard writter\n",
        "      global_iteration = 0\n",
        "      di_lr = 0.05\n",
        "      optimizer_di = optim.Adam([inputs], lr=di_lr)\n",
        "\n",
        "      print(\"Starting model inversion\")\n",
        "      batch_idx = 0\n",
        "      inputs = get_images(net=self.model, bs=200, epochs=2000, idx=batch_idx,\n",
        "                        net_student=net_student,\n",
        "                        train_writer=train_writer, use_amp=False,\n",
        "                        optimizer=optimizer_di, inputs=inputs, \n",
        "                        var_scale=0.00005, labels=labels)\n",
        "\n",
        "\n",
        "      plt.imshow(tensor2im(inputs[0]))\n",
        "      plt.show()\n",
        "      plt.imshow(tensor2im(inputs[55]))\n",
        "      plt.show()\n",
        "      print('deepinversion finshed')\n",
        "      # update exemplars number\n",
        "      \n",
        "\n",
        "      # reduce the number of each exemplars set\n",
        "      self.reduce_old_exemplars(m) \n",
        "\n",
        "      self.cumulative_class_mean = {}\n",
        "\n",
        "      for classlabel in batches[i]:\n",
        "        indexes_class = self.original_training_set.get_class_indexes(classlabel)\n",
        "        current_class = Subset(self.original_training_set, indexes_class)\n",
        "        #self.get_new_exemplars(indexes_class, m)\n",
        "        self.get_new_exemplars(current_class, m)\n",
        "        indexes_class = self.original_training_set.get_class_indexes(classlabel)\n",
        "        current_class = Subset(self.original_training_set, indexes_class)\n",
        "\n",
        "        loader = torch.utils.data.DataLoader(current_class, batch_size=self.batch_size,shuffle=False, num_workers=4)\n",
        "        features = np.zeros((0, 64))\n",
        "        with torch.no_grad():\n",
        "          for indexes, images, labels in loader:\n",
        "            images = images.cuda()\n",
        "            feature = self.feature_extractor(images).data.cpu().numpy()\n",
        "            feature = normalize(feature, axis=1, norm='l2')\n",
        "            features = np.concatenate((features,feature), axis=0)\n",
        "\n",
        "        class_mean = np.mean(features, axis=0)\n",
        "        class_mean = class_mean / np.linalg.norm(class_mean)  # Normalize\n",
        "\n",
        "        self.cumulative_class_mean[classlabel] = class_mean\n",
        "        \n",
        "      #plt.imshow(self.original_exemplar_set.dataset.data[self.exemplar_sets_idxs[0][0]])\n",
        "      #plt.show()\n",
        "      \n",
        "  \n",
        "      #self.exemplar_level_optimization(m, i, self.original_training_set.get_batch_indexes()[i])\n",
        "\n",
        "      # compute means of exemplar set\n",
        "      # cycle for each exemplar set\n",
        "      self.exemplar_means = torch.zeros((0, self.feature_size), dtype=torch.float).to(self.device)\n",
        "      self.exemplar_labels = []\n",
        "      for j in range(len(self.exemplar_sets_idxs)):\n",
        "        exemplars_dataset = Subset(self.original_training_set, self.exemplar_sets_idxs[j])\n",
        "        exemplars_loader = torch.utils.data.DataLoader(exemplars_dataset, batch_size=self.batch_size, shuffle=False, num_workers=4)\n",
        "        ex_features = torch.zeros((0, self.feature_size), dtype=torch.float).to(self.device) # alla fine shape: (len(exemplar_set), feature_size) --> (m, 64)\n",
        "      \n",
        "        with torch.no_grad():\n",
        "          _, _, exemplar_label = self.original_training_set.__getitem__(self.exemplar_sets_idxs[j][0]) \n",
        "          self.exemplar_labels.append(exemplar_label)\n",
        "          # cycle for each batch in the current exemplar set\n",
        "          for _,  exemplars, _ in exemplars_loader:\n",
        "          \n",
        "            # get exemplars features\n",
        "            exemplars = exemplars.to(self.device)\n",
        "            features = self.model.features(exemplars) # shape: (len(exemplars), feature_size)\n",
        "          \n",
        "            # normalize \n",
        "            feature_norms = torch.norm(features, p=2, dim=1) # shape: len(exemplars)\n",
        "            feature_norms.unsqueeze_(1) # shape: (len(exemplars), 1)\n",
        "            features = features/feature_norms\n",
        "          \n",
        "            # concatenate over columns\n",
        "            ex_features = torch.cat((ex_features, features), dim=0)\n",
        "          \n",
        "        # compute current exemplar set mean and normalize it\n",
        "        ex_mean = torch.mean(ex_features, dim=0) # shape: feature_size --> 64\n",
        "        ex_mean = ex_mean/torch.norm(ex_mean)\n",
        "        ex_mean.unsqueeze_(0) # shape: (1, feature_size) --> (1, 64)\n",
        "        self.exemplar_means = torch.cat((self.exemplar_means, ex_mean), dim=0) # shape: (n_examplar sets, feature size)\n",
        "      \n",
        "      #if i == 0:\n",
        "       # self.plot_data(self.trainloader)\n",
        "      print('accuracy on training set:', 100*self.__accuracy_fc(self.trainloader,self.diz))\n",
        "      # print('accuracy on test set:', self.__accuracy_on(self.testloader,self,self.diz))\n",
        "      current_test_acc = self.__accuracy_nme(self.testloader)\n",
        "      print('accuracy on test set:', 100*current_test_acc)\n",
        "      print('-' * 80)\n",
        "      test_acc.append(current_test_acc)\n",
        "\n",
        "    # compute comfusion matrix and save results\n",
        "    cm = self.plot_confusion_matrix()\n",
        "    with open('iCaRL_'+str(self.randomseed)+\"_cm\", 'wb') as file:\n",
        "      pickle.dump(cm, file, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "    with open('iCaRL_'+str(self.randomseed)+\"_testacc\", 'wb') as file:\n",
        "      pickle.dump(test_acc, file, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "      print('PRINT IMAGES')\n",
        "      print('with data augmentation')\n",
        "      plt.imshow(self.original_training_set.dataset.data[self.exemplar_sets_idxs[0][0]])\n",
        "      plt.show()\n",
        "      print('without data augmentation')\n",
        "      plt.imshow(self.original_exemplar_set.dataset.data[self.exemplar_sets_idxs[0][0]])\n",
        "      plt.show()\n",
        "'''"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n      #NUOVO PAPER DEL PORCODDIO\\n      labels_of_modified = torch.zeros(0, dtype = int).to(\\'cuda\\')\\n      for label in batches[i]:\\n        labels = torch.LongTensor([self.diz[label]]*m).to(\\'cuda\\')\\n        labels_of_modified = torch.cat((labels_of_modified, labels), dim=0)\\n      print(\\'labels to be created\\', labels_of_modified)\\n      print(\\'len to be created\\', len(labels_of_modified))\\n      number_of_images_created = m*10\\n      net_student = resnet32(num_classes=100).to(self.device)\\n      data_type = torch.float\\n      inputs = torch.randn((number_of_images_created, 3, 32, 32), requires_grad=True, device=\\'cuda\\', dtype=data_type)\\n\\n      net_student = copy.deepcopy(self.model)\\n      net_student.eval() #important, otherwise generated images will be non natural\\n      \\n      train_writer = None  # tensorboard writter\\n      global_iteration = 0\\n      di_lr = 0.05\\n      optimizer_di = optim.Adam([inputs], lr=di_lr)\\n\\n      print(\"Starting model inversion\")\\n      batch_idx = 0\\n      inputs = get_images(net=self.model, bs=200, epochs=2000, idx=batch_idx,\\n                        net_student=net_student,\\n                        train_writer=train_writer, use_amp=False,\\n                        optimizer=optimizer_di, inputs=inputs, \\n                        var_scale=0.00005, labels=labels)\\n\\n\\n      plt.imshow(tensor2im(inputs[0]))\\n      plt.show()\\n      plt.imshow(tensor2im(inputs[55]))\\n      plt.show()\\n      print(\\'deepinversion finshed\\')\\n      # update exemplars number\\n      \\n\\n      # reduce the number of each exemplars set\\n      self.reduce_old_exemplars(m) \\n\\n      self.cumulative_class_mean = {}\\n\\n      for classlabel in batches[i]:\\n        indexes_class = self.original_training_set.get_class_indexes(classlabel)\\n        current_class = Subset(self.original_training_set, indexes_class)\\n        #self.get_new_exemplars(indexes_class, m)\\n        self.get_new_exemplars(current_class, m)\\n        indexes_class = self.original_training_set.get_class_indexes(classlabel)\\n        current_class = Subset(self.original_training_set, indexes_class)\\n\\n        loader = torch.utils.data.DataLoader(current_class, batch_size=self.batch_size,shuffle=False, num_workers=4)\\n        features = np.zeros((0, 64))\\n        with torch.no_grad():\\n          for indexes, images, labels in loader:\\n            images = images.cuda()\\n            feature = self.feature_extractor(images).data.cpu().numpy()\\n            feature = normalize(feature, axis=1, norm=\\'l2\\')\\n            features = np.concatenate((features,feature), axis=0)\\n\\n        class_mean = np.mean(features, axis=0)\\n        class_mean = class_mean / np.linalg.norm(class_mean)  # Normalize\\n\\n        self.cumulative_class_mean[classlabel] = class_mean\\n        \\n      #plt.imshow(self.original_exemplar_set.dataset.data[self.exemplar_sets_idxs[0][0]])\\n      #plt.show()\\n      \\n  \\n      #self.exemplar_level_optimization(m, i, self.original_training_set.get_batch_indexes()[i])\\n\\n      # compute means of exemplar set\\n      # cycle for each exemplar set\\n      self.exemplar_means = torch.zeros((0, self.feature_size), dtype=torch.float).to(self.device)\\n      self.exemplar_labels = []\\n      for j in range(len(self.exemplar_sets_idxs)):\\n        exemplars_dataset = Subset(self.original_training_set, self.exemplar_sets_idxs[j])\\n        exemplars_loader = torch.utils.data.DataLoader(exemplars_dataset, batch_size=self.batch_size, shuffle=False, num_workers=4)\\n        ex_features = torch.zeros((0, self.feature_size), dtype=torch.float).to(self.device) # alla fine shape: (len(exemplar_set), feature_size) --> (m, 64)\\n      \\n        with torch.no_grad():\\n          _, _, exemplar_label = self.original_training_set.__getitem__(self.exemplar_sets_idxs[j][0]) \\n          self.exemplar_labels.append(exemplar_label)\\n          # cycle for each batch in the current exemplar set\\n          for _,  exemplars, _ in exemplars_loader:\\n          \\n            # get exemplars features\\n            exemplars = exemplars.to(self.device)\\n            features = self.model.features(exemplars) # shape: (len(exemplars), feature_size)\\n          \\n            # normalize \\n            feature_norms = torch.norm(features, p=2, dim=1) # shape: len(exemplars)\\n            feature_norms.unsqueeze_(1) # shape: (len(exemplars), 1)\\n            features = features/feature_norms\\n          \\n            # concatenate over columns\\n            ex_features = torch.cat((ex_features, features), dim=0)\\n          \\n        # compute current exemplar set mean and normalize it\\n        ex_mean = torch.mean(ex_features, dim=0) # shape: feature_size --> 64\\n        ex_mean = ex_mean/torch.norm(ex_mean)\\n        ex_mean.unsqueeze_(0) # shape: (1, feature_size) --> (1, 64)\\n        self.exemplar_means = torch.cat((self.exemplar_means, ex_mean), dim=0) # shape: (n_examplar sets, feature size)\\n      \\n      #if i == 0:\\n       # self.plot_data(self.trainloader)\\n      print(\\'accuracy on training set:\\', 100*self.__accuracy_fc(self.trainloader,self.diz))\\n      # print(\\'accuracy on test set:\\', self.__accuracy_on(self.testloader,self,self.diz))\\n      current_test_acc = self.__accuracy_nme(self.testloader)\\n      print(\\'accuracy on test set:\\', 100*current_test_acc)\\n      print(\\'-\\' * 80)\\n      test_acc.append(current_test_acc)\\n\\n    # compute comfusion matrix and save results\\n    cm = self.plot_confusion_matrix()\\n    with open(\\'iCaRL_\\'+str(self.randomseed)+\"_cm\", \\'wb\\') as file:\\n      pickle.dump(cm, file, protocol=pickle.HIGHEST_PROTOCOL)\\n    with open(\\'iCaRL_\\'+str(self.randomseed)+\"_testacc\", \\'wb\\') as file:\\n      pickle.dump(test_acc, file, protocol=pickle.HIGHEST_PROTOCOL)\\n\\n      print(\\'PRINT IMAGES\\')\\n      print(\\'with data augmentation\\')\\n      plt.imshow(self.original_training_set.dataset.data[self.exemplar_sets_idxs[0][0]])\\n      plt.show()\\n      print(\\'without data augmentation\\')\\n      plt.imshow(self.original_exemplar_set.dataset.data[self.exemplar_sets_idxs[0][0]])\\n      plt.show()\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HzqwQeHB1Tg-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5d0f0f4-9bf5-4964-f1bc-39ad4537b378"
      },
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "from __future__ import unicode_literals\n",
        "\n",
        "import argparse\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "# import torch.nn.parallel\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.optim as optim\n",
        "# import torch.utils.data\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.utils as vutils\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import glob\n",
        "import collections\n",
        "\n",
        "#from resnet_cifar import ResNet34, ResNet18\n",
        "\n",
        "try:\n",
        "    from apex.parallel import DistributedDataParallel as DDP\n",
        "    from apex import amp, optimizers\n",
        "    USE_APEX = True\n",
        "except ImportError:\n",
        "    print(\"Please install apex from https://www.github.com/nvidia/apex to run this example.\")\n",
        "    print(\"will attempt to run without it\")\n",
        "    USE_APEX = False\n",
        "\n",
        "#provide intermeiate information\n",
        "debug_output = False\n",
        "debug_output = True\n",
        "\n",
        "\n",
        "class DeepInversionFeatureHook():\n",
        "    '''\n",
        "    Implementation of the forward hook to track feature statistics and compute a loss on them.\n",
        "    Will compute mean and variance, and will use l2 as a loss\n",
        "    '''\n",
        "\n",
        "    def __init__(self, module):\n",
        "        self.hook = module.register_forward_hook(self.hook_fn)\n",
        "\n",
        "    def hook_fn(self, module, input, output):\n",
        "        # hook co compute deepinversion's feature distribution regularization\n",
        "        nch = input[0].shape[1]\n",
        "\n",
        "        mean = input[0].mean([0, 2, 3])\n",
        "        var = input[0].permute(1, 0, 2, 3).contiguous().view([nch, -1]).var(1, unbiased=False)\n",
        "\n",
        "        # forcing mean and variance to match between two distributions\n",
        "        # other ways might work better, e.g. KL divergence\n",
        "        r_feature = torch.norm(module.running_var.data.type(var.type()) - var, 2) + torch.norm(\n",
        "            module.running_mean.data.type(var.type()) - mean, 2)\n",
        "\n",
        "        self.r_feature = r_feature\n",
        "        # must have no output\n",
        "\n",
        "    def close(self):\n",
        "        self.hook.remove()\n",
        "\n",
        "def get_images(net, bs=256, epochs=1000, idx=-1, var_scale=0.00005, competitive_scale=0.01,\n",
        "               net_student=None, prefix=None, train_writer = None, global_iteration=None,\n",
        "               use_amp=False, bn_reg_scale = 0.0,\n",
        "               optimizer = None, inputs = None, labels = False, l2_coeff=0.0):\n",
        "    '''\n",
        "    Function returns inverted images from the pretrained model, parameters are tight to CIFAR dataset\n",
        "    args in:\n",
        "        net: network to be inverted\n",
        "        bs: batch size\n",
        "        epochs: total number of iterations to generate inverted images, training longer helps a lot!\n",
        "        idx: an external flag for printing purposes: only print in the first round, set as -1 to disable\n",
        "        var_scale: the scaling factor for variance loss regularization. this may vary depending on bs\n",
        "            larger - more blurred but less noise\n",
        "        net_student: model to be used for Adaptive DeepInversion\n",
        "        prefix: defines the path to store images\n",
        "        competitive_scale: coefficient for Adaptive DeepInversion\n",
        "        train_writer: tensorboardX object to store intermediate losses\n",
        "        global_iteration: indexer to be used for tensorboard\n",
        "        use_amp: boolean to indicate usage of APEX AMP for FP16 calculations - twice faster and less memory on TensorCores\n",
        "        optimizer: potimizer to be used for model inversion\n",
        "        inputs: data place holder for optimization, will be reinitialized to noise\n",
        "        bn_reg_scale: weight for r_feature_regularization\n",
        "        random_labels: sample labels from random distribution or use columns of the same class\n",
        "        l2_coeff: coefficient for L2 loss on input\n",
        "    return:\n",
        "        A tensor on GPU with shape (bs, 3, 32, 32) for CIFAR\n",
        "    '''\n",
        "\n",
        "    kl_loss = nn.KLDivLoss(reduction='batchmean').cuda()\n",
        "\n",
        "    # preventing backpropagation through student for Adaptive DeepInversion\n",
        "    net_student.eval()\n",
        "\n",
        "    best_cost = 1e6\n",
        "\n",
        "    # initialize gaussian inputs\n",
        "    inputs.data = torch.randn((bs, 3, 32, 32), requires_grad=True, device='cuda')\n",
        "    # if use_amp:\n",
        "    #     inputs.data = inputs.data.half()\n",
        "\n",
        "    # set up criteria for optimization\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    optimizer.state = collections.defaultdict(dict)  # Reset state of optimizer\n",
        "\n",
        "    # target outputs to generate\n",
        "    #if labels:\n",
        "    targets = labels\n",
        "    #else:\n",
        "     #   targets = torch.LongTensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9] * 25 + [0, 1, 2, 3, 4, 5]).to('cuda')\n",
        "\n",
        "    outputs=net(inputs.data)\n",
        "    _, predicted_teach = outputs.max(1)\n",
        "\n",
        "    outputs_student=net_student(inputs.data)\n",
        "    _, predicted_std = outputs_student.max(1)\n",
        "\n",
        "    if idx == 0:\n",
        "        print('Teacher correct out of {}: {}, loss at {}'.format(bs, predicted_teach.eq(targets).sum().item(), criterion(outputs, targets).item()))\n",
        "        print('Student correct out of {}: {}, loss at {}'.format(bs, predicted_std.eq(targets).sum().item(), criterion(outputs_student, targets).item()))\n",
        "\n",
        "    ## Create hooks for feature statistics catching\n",
        "    loss_r_feature_layers = []\n",
        "    for module in net.modules():\n",
        "        if isinstance(module, nn.BatchNorm2d):\n",
        "            loss_r_feature_layers.append(DeepInversionFeatureHook(module))\n",
        "\n",
        "    # setting up the range for jitter\n",
        "    lim_0, lim_1 = 2, 2\n",
        "\n",
        "    for epoch in tqdm(range(epochs)):\n",
        "        # apply random jitter offsets\n",
        "        off1 = random.randint(-lim_0, lim_0)\n",
        "        off2 = random.randint(-lim_1, lim_1)\n",
        "        inputs_jit = torch.roll(inputs, shifts=(off1,off2), dims=(2,3))\n",
        "\n",
        "        # foward with jit images\n",
        "        optimizer.zero_grad()\n",
        "        net.zero_grad()\n",
        "        outputs = net(inputs_jit)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss_target = loss.item()\n",
        "\n",
        "        # competition loss, Adaptive DeepInvesrion\n",
        "        if competitive_scale != 0.0:\n",
        "            net_student.zero_grad()\n",
        "            outputs_student = net_student(inputs_jit)\n",
        "            T = 3.0\n",
        "\n",
        "            if 1:\n",
        "                # jensen shanon divergence:\n",
        "                # another way to force KL between negative probabilities\n",
        "                P = F.softmax(outputs_student / T, dim=1)\n",
        "                Q = F.softmax(outputs / T, dim=1)\n",
        "                M = 0.5 * (P + Q)\n",
        "\n",
        "                P = torch.clamp(P, 0.01, 0.99)\n",
        "                Q = torch.clamp(Q, 0.01, 0.99)\n",
        "                M = torch.clamp(M, 0.01, 0.99)\n",
        "                eps = 0.0\n",
        "                # loss_verifier_cig = 0.5 * kl_loss(F.log_softmax(outputs_verifier / T, dim=1), M) +  0.5 * kl_loss(F.log_softmax(outputs/T, dim=1), M)\n",
        "                loss_verifier_cig = 0.5 * kl_loss(torch.log(P + eps), M) + 0.5 * kl_loss(torch.log(Q + eps), M)\n",
        "                # JS criteria - 0 means full correlation, 1 - means completely different\n",
        "                loss_verifier_cig = 1.0 - torch.clamp(loss_verifier_cig, 0.0, 1.0)\n",
        "\n",
        "                loss = loss + competitive_scale * loss_verifier_cig\n",
        "\n",
        "        # apply total variation regularization\n",
        "        diff1 = inputs_jit[:,:,:,:-1] - inputs_jit[:,:,:,1:]\n",
        "        diff2 = inputs_jit[:,:,:-1,:] - inputs_jit[:,:,1:,:]\n",
        "        diff3 = inputs_jit[:,:,1:,:-1] - inputs_jit[:,:,:-1,1:]\n",
        "        diff4 = inputs_jit[:,:,:-1,:-1] - inputs_jit[:,:,1:,1:]\n",
        "        loss_var = torch.norm(diff1) + torch.norm(diff2) + torch.norm(diff3) + torch.norm(diff4)\n",
        "        loss = loss + var_scale*loss_var\n",
        "\n",
        "        # R_feature loss\n",
        "        loss_distr = sum([mod.r_feature for mod in loss_r_feature_layers])\n",
        "        loss = loss + bn_reg_scale*loss_distr # best for noise before BN\n",
        "\n",
        "        # l2 loss\n",
        "        if 1:\n",
        "            loss = loss + l2_coeff * torch.norm(inputs_jit, 2)\n",
        "\n",
        "        if debug_output and epoch % 200==0:\n",
        "            print(f\"It {epoch}\\t Losses: total: {loss.item():3.3f},\\ttarget: {loss_target:3.3f} \\tR_feature_loss unscaled:\\t {loss_distr.item():3.3f}\")\n",
        "            #vutils.save_image(inputs.data.clone(),\n",
        "             #                 './{}/output_{}.png'.format(prefix, epoch//200),\n",
        "              #                normalize=True, scale_each=True, nrow=10)\n",
        "\n",
        "        if best_cost > loss.item():\n",
        "            best_cost = loss.item()\n",
        "            best_inputs = inputs.data\n",
        "\n",
        "        # backward pass\n",
        "        if use_amp:\n",
        "            with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
        "                scaled_loss.backward()\n",
        "        else:\n",
        "            loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "    outputs=net(best_inputs)\n",
        "    _, predicted_teach = outputs.max(1)\n",
        "\n",
        "    outputs_student=net_student(best_inputs)\n",
        "    _, predicted_std = outputs_student.max(1)\n",
        "\n",
        "    if idx == 0:\n",
        "        print('Teacher correct out of {}: {}, loss at {}'.format(bs, predicted_teach.eq(targets).sum().item(), criterion(outputs, targets).item()))\n",
        "        print('Student correct out of {}: {}, loss at {}'.format(bs, predicted_std.eq(targets).sum().item(), criterion(outputs_student, targets).item()))\n",
        "\n",
        "    name_use = \"best_images\"\n",
        "    if prefix is not None:\n",
        "        name_use = prefix + name_use\n",
        "    next_batch = len(glob.glob(\"./%s/*.png\" % name_use)) // 1\n",
        "\n",
        "    #vutils.save_image(best_inputs[:20].clone(),\n",
        "     #                 './{}/output_{}.png'.format(name_use, next_batch),\n",
        "      #                normalize=True, scale_each = True, nrow=10)\n",
        "\n",
        "    #if train_writer is not None:\n",
        "     #   train_writer.add_scalar('gener_teacher_criteria', criterion(outputs, targets), global_iteration)\n",
        "      #  train_writer.add_scalar('gener_student_criteria', criterion(outputs_student, targets), global_iteration)\n",
        "\n",
        "       # train_writer.add_scalar('gener_teacher_acc', predicted_teach.eq(targets).sum().item() / bs, global_iteration)\n",
        "       # train_writer.add_scalar('gener_student_acc', predicted_std.eq(targets).sum().item() / bs, global_iteration)\n",
        "\n",
        "        #train_writer.add_scalar('gener_loss_total', loss.item(), global_iteration)\n",
        "        #train_writer.add_scalar('gener_loss_var', (var_scale*loss_var).item(), global_iteration)\n",
        "\n",
        "    net_student.train()\n",
        "\n",
        "    return best_inputs"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Please install apex from https://www.github.com/nvidia/apex to run this example.\n",
            "will attempt to run without it\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168,
          "referenced_widgets": [
            "d624392a703f49408db7b1f032e342c9",
            "82050864a2b94a7a8241ea2b2bf1d308",
            "f53fd5a6a6b54eae8f933e2fda3d09bc",
            "c2a985c6e58e4cc1bf2db2bf154ef322",
            "ee33d75a3e37436b8bf618c293b25094",
            "033c4ad73ab745d19d85474eb07de1cc",
            "5081e31785f24b0e962d0a6545e1ce83",
            "f0af6af94bb845a1b6e66c2355bd71f4"
          ]
        },
        "id": "OYzLuYGDLr15",
        "outputId": "9bbf722f-28b3-4a74-8801-d9d3592b2679"
      },
      "source": [
        "method = mnemonics(randomseed=203)\n",
        "model, batch = method.trainer()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "current batches [11, 5, 62, 76, 27, 3, 96, 33, 78, 30]\n",
            "10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d624392a703f49408db7b1f032e342c9",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=70.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "accuracy on test set: 84.15178571428571\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AhzQjA4HT2wH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b684582-ef5a-4c37-a6e7-5c8c1f909933"
      },
      "source": [
        "!git clone https://github.com/huyvnphan/PyTorch_CIFAR10.git\n",
        "\n",
        "! cp -r /content/PyTorch_CIFAR10/cifar10_models/resnet.py /content\n",
        "\n",
        "!pip install pytorch_lightning\n",
        "!python /content/PyTorch_CIFAR10/train.py --download_weights 1\n",
        "\n",
        "! cp -r /content/cifar10_models/state_dicts /content"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'PyTorch_CIFAR10' already exists and is not an empty directory.\n",
            "Requirement already satisfied: pytorch_lightning in /usr/local/lib/python3.6/dist-packages (1.1.4)\n",
            "Requirement already satisfied: future>=0.17.1 in /usr/local/lib/python3.6/dist-packages (from pytorch_lightning) (0.18.2)\n",
            "Requirement already satisfied: torch>=1.3 in /usr/local/lib/python3.6/dist-packages (from pytorch_lightning) (1.7.0+cu101)\n",
            "Requirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.6/dist-packages (from pytorch_lightning) (2.4.0)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.6/dist-packages (from pytorch_lightning) (4.41.1)\n",
            "Requirement already satisfied: fsspec[http]>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from pytorch_lightning) (0.8.5)\n",
            "Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.6/dist-packages (from pytorch_lightning) (1.19.5)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.6/dist-packages (from pytorch_lightning) (5.3.1)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch>=1.3->pytorch_lightning) (0.8)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch>=1.3->pytorch_lightning) (3.7.4.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (2.23.0)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.32.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.0.1)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (3.12.4)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (3.3.3)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.17.2)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (0.36.2)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (0.10.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (0.4.2)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.15.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.7.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (51.1.1)\n",
            "Requirement already satisfied: aiohttp; extra == \"http\" in /usr/local/lib/python3.6/dist-packages (from fsspec[http]>=0.8.1->pytorch_lightning) (3.7.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.2.0->pytorch_lightning) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.2.0->pytorch_lightning) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.2.0->pytorch_lightning) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.2.0->pytorch_lightning) (2020.12.5)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard>=2.2.0->pytorch_lightning) (3.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (4.6)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (4.2.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch_lightning) (1.3.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.6/dist-packages (from aiohttp; extra == \"http\"->fsspec[http]>=0.8.1->pytorch_lightning) (5.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.6/dist-packages (from aiohttp; extra == \"http\"->fsspec[http]>=0.8.1->pytorch_lightning) (1.6.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.6/dist-packages (from aiohttp; extra == \"http\"->fsspec[http]>=0.8.1->pytorch_lightning) (20.3.0)\n",
            "Requirement already satisfied: idna-ssl>=1.0; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from aiohttp; extra == \"http\"->fsspec[http]>=0.8.1->pytorch_lightning) (1.1.0)\n",
            "Requirement already satisfied: async-timeout<4.0,>=3.0 in /usr/local/lib/python3.6/dist-packages (from aiohttp; extra == \"http\"->fsspec[http]>=0.8.1->pytorch_lightning) (3.0.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard>=2.2.0->pytorch_lightning) (3.4.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch_lightning) (3.1.0)\n",
            "100% 979M/979M [00:40<00:00, 24.5MMiB/s]\n",
            "Download successful. Unzipping file...\n",
            "Unzip file successful!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UTxouRaITy8I"
      },
      "source": [
        "#DOPO AVER IMPORTATO IL CONTENUTO DEL TIPO, SCEGLOERE LA RETE CHE SI VUOLE\n",
        "#from resnet import resnet50, resnet18, resnet34\n",
        "\n",
        "trials = resnet32(pretrained = 'cifar100',num_classes=100).to('cuda')"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4PNt5gwYHm7Q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 593,
          "referenced_widgets": [
            "44719370c7ea450095f24bebfa326f8e",
            "03be1f1817f74f6784c98050ccf40c68",
            "65c8e2cc95874f02a5a1a4b8beab441f",
            "0ddb4beee7274d52b132a64f2f437aef",
            "652527b14b7e4ca8843f38a9308172a7",
            "15d0ad20c1ed4d8ca34af3c2c928a62d",
            "9c35b00182684d708a476d2b7dc8e05a",
            "790d22d6b23940bcbaad14117c025cd8"
          ]
        },
        "outputId": "53a053a9-aba2-4e39-a5f0-82a0b1419abd"
      },
      "source": [
        "#QUI FACCIO FINETUINING SULLA NUOVA RETE, CON LE MIE CLASSI\n",
        "# ORA STO USANDO LE IMMAGINI PRESE DALL'EXEMPLARS SET PER VEDERE SE HO MIGLIORAMENTI\n",
        "train_dataset = Subset(ilCIFAR100(10, 203, train = 'exemplars'), ilCIFAR100(10, 203, train = 'train').get_batch_indexes()[0])\n",
        "test_dataset = Subset(ilCIFAR100(10, 203, train = 'test'), ilCIFAR100(10, 203, train = 'test').get_batch_indexes()[0])\n",
        "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=4, drop_last=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=True, num_workers=4, drop_last=True)\n",
        "diz = ilCIFAR100(10, 203, train = 'train').get_dict()\n",
        "\n",
        "\n",
        "# Prepare Training\n",
        "optimizer = optim.SGD(trials.parameters(), lr=2, momentum=0.9, weight_decay= 1e-5)\n",
        "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[14,24], gamma=0.2)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "trials.eval()\n",
        "total = 0.0\n",
        "correct = 0.0\n",
        "for  _, images, labels in test_loader:\n",
        "  labels = torch.tensor([torch.tensor(diz[c.item()]) for c in labels])\n",
        "  labels = labels.to('cuda')\n",
        "  images = images.to('cuda')\n",
        "  outputs = trials(images)\n",
        "  _, preds = torch.max(outputs, dim=1)\n",
        "  total += len(labels)\n",
        "  correct += torch.sum(preds == labels).item()\n",
        "\n",
        "acc = correct / total\n",
        "print('test accuracy before finetuning', acc)\n",
        "\n",
        "\n",
        "trials.train()\n",
        "for epoch in tqdm(range(30)):\n",
        "  tot_loss = 0.0\n",
        "  for _, inputs, labels in train_loader:\n",
        "    inputs = inputs.float().to('cuda')\n",
        "    labels = torch.tensor([diz[c.item()] for c in labels])\n",
        "\n",
        "    labels=labels.to('cuda')\n",
        "    optimizer.zero_grad()\n",
        "    outputs=trials(inputs)\n",
        "\n",
        "    labels_encoded = F.one_hot(labels,100).float().to('cuda') #CAMBIARE ONE_HOT\n",
        "    loss = criterion(outputs, labels_encoded) \n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    tot_loss+=loss.item()\n",
        "  \n",
        "  scheduler.step()\n",
        "  print(f'loss at epoch{epoch}', tot_loss)\n",
        "\n",
        "\n",
        "trials.eval()\n",
        "total = 0.0\n",
        "correct = 0.0\n",
        "for  _, images, labels in test_loader:\n",
        "  labels = torch.tensor([torch.tensor(diz[c.item()]) for c in labels])\n",
        "  labels = labels.to('cuda')\n",
        "  images = images.to('cuda')\n",
        "  outputs = trials(images)\n",
        "  _, preds = torch.max(outputs, dim=1)\n",
        "  total += len(labels)\n",
        "  correct += torch.sum(preds == labels).item()\n",
        "\n",
        "acc = correct / total\n",
        "print('test accuracy after finetuning', acc)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "test accuracy before finetuning 0.0011160714285714285\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "44719370c7ea450095f24bebfa326f8e",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=30.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "loss at epoch0 5.259806388989091\n",
            "loss at epoch1 0.7743029594421387\n",
            "loss at epoch2 0.6121986797079444\n",
            "loss at epoch3 0.5278265280649066\n",
            "loss at epoch4 0.448295840062201\n",
            "loss at epoch5 0.3725116988644004\n",
            "loss at epoch6 0.3226623800583184\n",
            "loss at epoch7 0.2633625869639218\n",
            "loss at epoch8 0.2182471805717796\n",
            "loss at epoch9 0.1770902993157506\n",
            "loss at epoch10 0.16364197060465813\n",
            "loss at epoch11 0.12894063163548708\n",
            "loss at epoch12 0.11124967923387885\n",
            "loss at epoch13 0.08197552524507046\n",
            "loss at epoch14 0.042378253245260566\n",
            "loss at epoch15 0.028054671915015206\n",
            "loss at epoch16 0.02187966401106678\n",
            "loss at epoch17 0.020006448350613937\n",
            "loss at epoch18 0.018704776011873037\n",
            "loss at epoch19 0.01691493854741566\n",
            "loss at epoch20 0.016325193471857347\n",
            "loss at epoch21 0.015250828553689644\n",
            "loss at epoch22 0.013600802049040794\n",
            "loss at epoch23 0.013531425342080183\n",
            "loss at epoch24 0.012793005254934542\n",
            "loss at epoch25 0.012344696340733208\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zJ4w-tWhY6XC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 853,
          "referenced_widgets": [
            "2b3795afe1134cb0ad45755c3bb7ed0a",
            "31630c0adb3c491b840a0746924a5a01",
            "9de3ce5233bc4a82846cbcfded29f8b4",
            "f834166f2b4d4349b43f336aa0ebb4ad",
            "78b454e44ef34d2b9f62c3b372955d41",
            "357a278744b84bbaa75f157495caa3f4",
            "0b14aae1281541d6aa32de6fd39908c2",
            "43399d35dff94120b6399a471595bda9"
          ]
        },
        "outputId": "04fd55af-99af-489d-f80a-469ee1f21f0b"
      },
      "source": [
        "# CODICE PER CREARE LE IMMAGINI SINTETICHE\n",
        "\n",
        "labels_of_modified = torch.zeros(0, dtype = int).to('cuda')\n",
        "for label in batch:\n",
        "  labels = torch.LongTensor([diz[label]]*20).to('cuda')\n",
        "  labels_of_modified = torch.cat((labels_of_modified, labels), dim=0)\n",
        "print('len to be created', len(labels_of_modified))\n",
        "number_of_images_created = 200\n",
        "\n",
        "#SE VOGLIO USARE COME TEACHER LA NOSTRA RETE USARE IL CODICE QUI\n",
        "\n",
        "#teacher = copy.deepcopy(fake_model)\n",
        "#net_teacher = resnet32(num_classes=10).to('cuda')\n",
        "#net_teacher.load_state_dict(teacher.state_dict())\n",
        "#net_teacher.eval()\n",
        "\n",
        "#net_student = resnet32(num_classes=10).to('cuda')\n",
        "net_student = resnet18().to('cuda')\n",
        "\n",
        "trials.eval()\n",
        "\n",
        "inputs = torch.randn((number_of_images_created, 3, 32, 32), requires_grad=True, device='cuda', dtype=torch.float)\n",
        "train_writer = None  # tensorboard writter\n",
        "global_iteration = 0\n",
        "di_lr = 0.05\n",
        "optimizer_di = optim.Adam([inputs], lr=di_lr)\n",
        "plt.imshow(tensor2im(inputs[0]))\n",
        "plt.show()\n",
        "print(\"Starting model inversion\")\n",
        "batch_idx = 0\n",
        "inputs = get_images(net=model, bs=len(labels_of_modified), epochs=2000, idx=batch_idx, \n",
        "                  net_student=model, competitive_scale=10.0, l2_coeff = 3e-8, bn_reg_scale = 5.0,\n",
        "                  train_writer=train_writer, use_amp=False,\n",
        "                  optimizer=optimizer_di, inputs=inputs, \n",
        "                  var_scale=0.001, labels=labels_of_modified) #2.5e-5\n",
        "trials.eval()\n",
        "plt.imshow(tensor2im(inputs[0]))\n",
        "plt.show()\n",
        "print('deepinversion finshed')"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "len to be created 200\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Starting model inversion\n",
            "Teacher correct out of 200: 20, loss at 17.479957580566406\n",
            "Student correct out of 200: 20, loss at 17.479957580566406\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2b3795afe1134cb0ad45755c3bb7ed0a",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=2000.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "It 0\t Losses: total: 1134.013,\ttarget: 17.926 \tR_feature_loss unscaled:\t 220.351\n",
            "It 200\t Losses: total: 200.191,\ttarget: 4.649 \tR_feature_loss unscaled:\t 36.238\n",
            "It 400\t Losses: total: 154.816,\ttarget: 3.028 \tR_feature_loss unscaled:\t 27.524\n",
            "It 600\t Losses: total: 128.219,\ttarget: 2.592 \tR_feature_loss unscaled:\t 22.324\n",
            "It 800\t Losses: total: 71.187,\ttarget: 2.006 \tR_feature_loss unscaled:\t 11.062\n",
            "It 1000\t Losses: total: 69.465,\ttarget: 1.961 \tR_feature_loss unscaled:\t 10.752\n",
            "It 1200\t Losses: total: 63.365,\ttarget: 1.328 \tR_feature_loss unscaled:\t 9.679\n",
            "It 1400\t Losses: total: 81.339,\ttarget: 1.524 \tR_feature_loss unscaled:\t 13.254\n",
            "It 1600\t Losses: total: 87.226,\ttarget: 1.749 \tR_feature_loss unscaled:\t 14.403\n",
            "It 1800\t Losses: total: 80.263,\ttarget: 1.253 \tR_feature_loss unscaled:\t 13.122\n",
            "\n",
            "Teacher correct out of 200: 160, loss at 1.0464648008346558\n",
            "Student correct out of 200: 160, loss at 1.0464648008346558\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "deepinversion finshed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "id": "sokaKsjfV9_n",
        "outputId": "801496a2-6a09-4110-ec4a-ea96383cf781"
      },
      "source": [
        "plt.imshow(tensor2im(inputs[199]))\n",
        "plt.show()"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD5CAYAAADhukOtAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO2dd3iU5fL+7yEQEgiYAKFLCR0RASNNBJVy7KBgQUUsiAcEwWNHj2DBesDeqDYUVEBQREBAmoIkgBRDJ5QQEpIQSALpz++PXX4X+p2bIJAN57zzuS4uNnNndp88u7Pv7jvvzIhzDoZh/O9TqqQXYBhGYLBgNwyPYMFuGB7Bgt0wPIIFu2F4BAt2w/AIpc/EWUSuAvAWgCAAE5xzr5z0wUJDXJnzwlQtqHR5/jiuULWHVMyiPoUFfB2FmTX4Y4WlUy3/sP542REh/P62lKFaXu0jVIvI1P9mAAgtm0O1/JR81V6hkVCf3dsaUK1u/d1UqxDUgmobc5NVe8MD1fg6CvdQ7VhpvsdVyqdQraBQ9ysVFk59MnOyqVax1FGqZeTkUa1cVhB/vNLk9V26LPUpnauvPzPnILLzjqhPtpxunl1EggBsBdAdwD4AqwH0dc79wXxCq1dxUf2uV7WISh3oYwUVZKr2Rl1XUp9jmXxzc5Y/xR+r0/dUO/i9/nibezehPmW78hd3wmsLqHbL0mNUa9lgJ9WSJh5U7d3m8mAZcO10qn30yUCqdYnYTLWmu99R7XNee4Sv49hQqm2M5Ht8X9txVEvPaqraK3S6gfos27Gdat3Lr6Haom2JVLt45XlUW1FNfwNpEs7fhKvs7anav9v4BFIyd6jBfiYf49sC2O6c2+mcywUwFYC+AsMwSpwzCfZaAPae8PM+v80wjHOQYj9BJyIDRSRGRGIKjvLvQoZhFC9nEuwJAM4/4efaftufcM6Nc85FO+eig8rx742GYRQvZxLsqwE0EpH6IhIM4DYAs8/OsgzDONucdurNOZcvIkMAzIMv9TbJObfpZD5B6aURPruKqm04bxn16x9dU7V/M+Mm6tNjDv8U0fjN8VQb81s9qj22p69qT1z4DfXp278V1ZZ8cSXVygTz1FufqnyvHq3TTbVPv4intfp35+/Ry4fz1Nvj5fk+jk66QLU37MHPgv/x22NUm/rkw1QbP/V8qvWrSO7vqeXUJ7yHnh4GgLSa8VS7LKsO1WI3bKXagfJ6ejY1nWddZoWtVe2/Ck8NnlGe3Tn3A4AfzuQ+DMMIDHYFnWF4BAt2w/AIFuyG4REs2A3DI1iwG4ZHOKOz8X+b8DwUXvd/rrsBADTdcT91azIjVLWXH84LYcqu0auuAOCFKbdRbYReRwIAWNBML2a4JJwXE20p9wvVanTlxRHV46+lWovOB6j26Np1qn3M0LuoT8h3PL12ZDzfq4TtPL058OAk1T7iiwHU557n+fPZe9W/qNbz6kZUaz5Vr2Jc1HY+9elamRdDTR3VjGrVLgim2pKLeWFW9dH663vDAF7d2Dc2SrUnOF4pZ0d2w/AIFuyG4REs2A3DI1iwG4ZHsGA3DI9w2m2pTofooJZudbnvVO3ie1+mfjEZeq+z6VKX+mwIi6dalRDe7ue3bN68LqSm3jNu27Hu1Kd9t5eoVnHeo1Rb1f9ZqrW+72qqvXvJYdU+oxovhBm9rhLVqp3P++St/uQyqkU9pz/P1ebzYpcJT31AtSkTeAnG9tgpVNt8iV4sdXjRx9Rn/+UfUq3L4TiqJa5dSLXYfYeodrCM3qprfT1eYPVIzT6q/efl85CennbW21IZhvFfhAW7YXgEC3bD8AgW7IbhESzYDcMjWLAbhkcIaOotNLKBq99bT0VJHT5d5Na2elFL7OyG1Kd5P977LW4xL0qIun0e1WbkfqnaQ0p3pj43rcmgWrl2fDTUpur8b0ua8yPVcnbr6cjSe3lhTb2oJVSbvoEXwjS4qAvVLn/yU9WeMZRPyPmjzAiq5Re8R7XKM/lzFt1TT+eV3cYLciZcwSe7RN7J07adavLegDc+PpPfZ8Peqv3OO/iYsrg+en+63LkbUZiaaak3w/AyFuyG4REs2A3DI1iwG4ZHsGA3DI9gwW4YHuGMUm8iEg8gA0ABgHznXPTJfr98jWDX7F499RIz9xXqd6h9pGq/7Q5eoVb7S55eC55cnWpxv/BJsxeu09NXV/16HfV5o9s1VIsPaU21sm8PptoDr/Lqqh8z9erBwwt4OqmwAe91VjFkINV27+Bjo+6stl21/3ySXmxtvuWvxdfX6mOtAKBVNE9hbq2k98K7et3l1GdJ41iqjYj8mGpPTW1DtZSveNXbNQ31qWljczpSnz617lTtibvGIefYfjX1djYaTl7hnEs5C/djGEYxYh/jDcMjnGmwOwDzRSRWRPjnPcMwSpwz/RjfyTmXICJVASwQkc3OuaUn/oL/TWAgAARX5N/XDMMoXs7oyO6cS/D/nwxgJoC2yu+Mc85FO+eiS5ezbw2GUVKcdvSJSHkRqXD8NoAeADaerYUZhnF2Oe3Um4hEwXc0B3xfB75wzo0+mU/p0GYuvP4n+v3dzcfx9Ci4RbVnHYynPu0Htqfarsh/U03eakK1tY3Je9k7V1Cf77fwcTxjRs+hWloaT9mlZS6mWmT4hap9ZQZvONmuVmOqpZzk/bv8RXrlFQBMzX1Gtb/w0K/U52CfTKolTOLPZ42e06jWwekVgjM3TqU+cqnezBEAZnz5ANWqNX+DakFT+DiyLwr05+y2q1pSn7xdx1R7UvpC5ObpDSdP+zu7c24ngItO198wjMBiX6INwyNYsBuGR7BgNwyPYMFuGB7Bgt0wPMLZKIQ5ZcpE7EfkrfoMs5q/8Iqhz3vpzSP/04ynvFauuYNqadt/p1qzYXOptuRBvSHi6zfwmXNfV+Jz2RKyVlLt16P7qFZhzTtUW1H4lGpvte8q6vPps49TrXzmLKoNPsDTaAfK/qbax97DG042qcLTYT91D6NabD5ff0qZGN2nUSH1uSq1ItUiBy6iWvgz/PlMbNyIalE/nqfaq9b6F/XJqaDvfepqfd8BO7IbhmewYDcMj2DBbhgewYLdMDyCBbtheISAno0vW86hUWu9b9y+zlWp3+WLvlft9QfGU58fLwun2gtP8jPuOe9Op1qr+/VxPI1fGkN9ar6l988DgJs+1f8uANhWeAPVqg7ko4Seidui2l9O+z/Vx/+fmp8+SrVG9/Cim93P6QUcALB67F2qfVGbrdTnqkR+dv/f9fKp9nkzXjBSOlovahk86W3qsyG4MtVCT5LV2FSdZwzSNvJ+cld31Yt1PonUC8AA4I7n/6na5b5g6mNHdsPwCBbshuERLNgNwyNYsBuGR7BgNwyPYMFuGB7hjMY//V2q16ru+g3qp2pb9vI+aJvv14tkcmfFU5/rr+YjjSatH0+1yFlrqVZqQBfV/uCO7tRneRDvPRaawlM1TVL5JK2VOXxsVF5Kmi5cf4D61HU8rbVtNx81ldOlHdXqL9B73mU9xtdeO4QX3XSYw7Wt+bdTLe5r/bVT78b3qU/uAT1tCAAVD/Wl2vp4svcA5n/L73PXQj0V/O6jPE05dZs+MiohczhyCrapPejsyG4YHsGC3TA8ggW7YXgEC3bD8AgW7IbhESzYDcMjFFn1JiKTAFwHINk518JvqwRgGoB6AOIB3OKcO1TUfVXNrYyh8XoKYkKbj6lfTNOlqn3kdJ7yWjSGpzo2v8z7u82K6EC1eYvyVHtsfn/qExZ/LdVih/FRU11f59VL7R/nlXmdxuqprTGZdahP09IfUO1Qxyeo1mbVKKrt6aqnyibMuoD6jMBkqp13hE8AXnUpr3Bs21vvGbdh9T3Up1HXm6i2Yave0w4A5j1yI9WqzOFptJ6D9FFfmW14j8VaFfUKzIMxidTnVI7sHwP4a13fkwAWOucaAVjo/9kwjHOYIoPdP2/9r1cL9ARwfELjJwB6neV1GYZxljnd7+zVnHPHPy8cAMD7AxuGcU5wxifonO96W3rNrYgMFJEYEYlJO1bk13rDMIqJ0w32JBGpAQD+/+kF4M65cc65aOdcdKXQiNN8OMMwzpTTDfbZAI6fgu4PgFcpGIZxTlBk1ZuIfAngcgBVACQBGAngWwBfAagDYDd8qTde8uPn/NCqbnjUraq2YixPh61PuV+1ZwRXoj4XrTtKtbyoT6m2eEk61WrX26Xa/7VDT4MAwIvbM6iWOJn7Db6WNzY89B7f6tq1u6n2KilZ1Kf89k5U295+HNXqF/L1p6WEqva9jVZRn51zeZPNO25uSrUJ375KtRELRqn2I/31ho0A8GrMEqrV+o4/Vk6pi6m2Ygdvirk1b75qb1Huc+rzwL8bqPavX3kFybt3q1VvRebZnXOspq9rUb6GYZw72BV0huERLNgNwyNYsBuGR7BgNwyPYMFuGB4hoLPekmqeh7dG/kPV+gZPoH5pS7ep9ksq680rAeC7hquplr1UrzICgJt7r6Bazbl6+mrB9j+oz7QLUqkWP3ga1bZ3+J1q3f49j2r7B+oVbPnNv6M+1dJ/odpNW/gev1dpN9UqkhF3KxeHUJ/b6gyl2pQNfI9fAK96mz5onWrfvPst6jMum896++eV+iw9AIidoaeVASCoFK/2u2SPvseDb69IfT7vqs/ZO/IeP37bkd0wPIIFu2F4BAt2w/AIFuyG4REs2A3DI1iwG4ZHCGjqrfLBUPT7sJWqzb63OvV7p55e8bRo7X7qs2vwQar9MJlX5H72fQ+qlV+3WLV/VK4u9Xn++RSqPVvwItVKv8lTVDePbE610bEVVPueybyJYod2w6l2+7NtqTZsI298eWyqnt5s3ZVXr43Yrlc3AkDM0ViqZSTxpph7Zuoz51rU470VJl9Wn2p1Juyg2u0NeDXi7jk89dZ+u74nb1flFXZbvtFTxCsP8SpWO7IbhkewYDcMj2DBbhgewYLdMDyCBbtheIQie9CdTSpFRLtul+sFKjk3FXC/X3qr9qwu91Kfzcv5uKNLgngxw6c5ZakWHqGPJ3pgPO8z17oS72mXekwfawUAR4fys+A/V65NtchRervucs0fpD6Ty75DtS8GdKTaT2MGUG1VH73gaVxb3lft9bANVAs+8h7V9kz/mmo5UZ+p9t7hudQnLkkvngGAZel3UC1s4mtU2x9BGzBj56Fo1f50fT57Ja7gGdW+dH020jML1R50dmQ3DI9gwW4YHsGC3TA8ggW7YXgEC3bD8AgW7IbhEYoshBGRSQCuA5DsnGvht40CcD+A49UmI5xzPxR1XwUuFRlOH710y4bHqN+sWrrPBZ/rhSkA0LhhTaqNcXFUezFjJtXScvSUzIK79b5vAHDFh+up1u4w7wsXHXEz1a6P4QVAL+3U+5Zd9z0vMgmd2Zpqj6wKotpDfdZSreHRS1V7arKeZgKAesm8J9/COU9RbcQOXiTzjluo2t9Of5z6DL5gK9XKh/5ItcX/aEa1Cp+Wo9o9x35W7bv+cT312VRRX2NmGd5f8VSO7B8D0AaPveGca+X/V2SgG4ZRshQZ7M65pQCKHNpoGMa5zZl8Zx8iIutFZJKI2OB1wzjHOd1g/wBAAwCtACQCGMN+UUQGikiMiMTk5vLLSg3DKF5OK9idc0nOuQLnXCGA8QDohdzOuXHOuWjnXHRwsN5FxTCM4ue0gl1Eapzw440ANp6d5RiGUVwUWfUmIl8CuBxAFQBJAEb6f24FwAGIB/CAcy6xqAer0rCmu+E1PQWUvDKP+q3O0McTdT3KRxOFbZ5PtRVRehUdAJTtPo5qzWJ3qfavc3ja8Ns746m2bOm3VGvU+guqVZzFM6b7BujpnwoPZlOf76rcTbX0rbz6LuiaY1Tr17+Sap+SkUN94tMaUO39uXupNqCRXtkGAOX39lTt4fG8su36Km2otvnauVST53hvwNcXfUW1Nh31NOC+Jw5TnykT9f14ZtM87MxKVaveisyzO+f6KuaJRfkZhnFuYVfQGYZHsGA3DI9gwW4YHsGC3TA8ggW7YXiEgI5/ysjNx5J9+mX20fG8seGwib+r9snzC6lPTvCVVIvYNIFq3+dEUS1urD66KjJ+J/WpWv08qt3amI8E2tuMj5TqMIhXXr0/Z5VqP/gybyoZu+pqql3dk14ciRbTtUSNj03jL1PtBUP43v/wHK8a63EdT3ndVY0/11nL9LRcgyaR1GdaX944csDSS6j2cAWeKmvc6CQNVaOeU+0PvcErBOf3Gq/aj+zlVZZ2ZDcMj2DBbhgewYLdMDyCBbtheAQLdsPwCBbshuERApp6iyyXiwfa7Fa1uOxU6pd6t165NKgZX/7WYF5123YhnwM3+OapVGv/kJ7u6LOiGvVZ3egtqj38gz4PDQBuXX2Uahe+ySvR5Dk9Zbdn6h7q02P+nVRrWutDqg3tyRttToh5Q7Ufacorw3p35mu85RjvhbBuNa++S68Vptoza/K01vUxCVSbXGMK1S74ne/j+xfy9OyRunpz0Q8ONaE+vY7pVZFBhTyO7MhuGB7Bgt0wPIIFu2F4BAt2w/AIFuyG4RECejY+9VA4Pvuql6o1CebvOwte08/S1lzLz3C2Xcl70E1ZwEf//HQBL655+vUtqn324hTq07pjKNVmleb97ireVINqpevwUUJRHZeo9gpP6H38AOBAT/05AYCDKQ9RbUTiZqol9s1U7SH9h1GfjNr8bHby1gNUa9l8BNX+SNcLV3bWnU59Zi7l2ZXHjtxFtcVDeXFNyzd5YdPiQ/VV+829eH/IUi5ct8tJfKhiGMb/FBbshuERLNgNwyNYsBuGR7BgNwyPYMFuGB6hyNSbiJwP4FMA1eAb9zTOOfeWiFQCMA1APfhGQN3inDt0svsKzg9BnbRGqta5Ph/HUym2vGovJbw4onqDV6gWcTkfF/TNet7Da94fn6j2nglfU5+6a6tS7el9N1KtX/yLVPsVPP1zfhd9jXWO8NRbwehnqPbh0C+pVmvSR1TbMWOeam9ZuRP1aZnFn5eZXfl4sKl136Xa/g16erP9O3pBFgC0nv0d1UrFD6Da/FFDqHZR8uVUu7heGdW+aBufenzTb5VVe1AqD+lTObLnA3jEOdccQHsAD4pIcwBPAljonGsEYKH/Z8MwzlGKDHbnXKJzbo3/dgaAOAC1APQEcPww8gkAfmWGYRglzt/6zi4i9QC0BrAKQLUTJrcegO9jvmEY5yinHOwiEgZgOoDhzrkjJ2rON/dZvU5PRAaKSIyIxORmp5/RYg3DOH1OKdhFpAx8gT7FOTfDb04SkRp+vQaAZM3XOTfOORftnIsODtGv5zUMo/gpMthFROCbxx7nnBt7gjQbQH//7f4AZp395RmGcbYQ3yfwk/yCSCcAywBsAHC8JGwEfN/bvwJQB8Bu+FJv+mwnP6Ui67gyPfWKs8ea/UH9GsXdr9pn3sX7gd21YRfV9m3T038A8Fl1/lUjdN9B1d6umZ4GAYD2SbzaacPD3C/rTf7emTMoiGoP36VXmz19yXvUp++Q7VRLXaP33QOA/b/w9NXup3JUe1KTgdSnWm/+fF5/CR9RNffNF6i2o7Oeenu253+oT9q0i6m2YkBrqiU+wyvzkjP52Ks/EvTUW/0reA+6Pdn6yKt9P36MnNRE0bQi8+zOueUAVGcAXYvyNwzj3MCuoDMMj2DBbhgewYLdMDyCBbtheAQLdsPwCAFtOFm/IAsvZejVVz8f2kb9Em9vptqbxvLl/+uqKlRrkbqPatH5POU1sVwf1Z69fy31GX2lnq4DgF+H51Itr0FDqu38go872vmSXh029cWxqh0A5r1ZlmpdKvARVVlDL6RayyaPqPaZh3nqKi+Np97yPlpMtTq9nqLaNQn662pQR318EgAsCuXPWcL3/Llu0JKP7Hr7XX5crXhE35Pl/XiK9c3E11X7q7kF1MeO7IbhESzYDcMjWLAbhkewYDcMj2DBbhgewYLdMDxCkVVvZ5PqTSLdXe/3VLXUabyh4BPhetPGyZ0TqM/F7/H5a7PyO1Jtf3XebPDo4RaqveXW5dRnTu3uVMvISKVat7a8Wu7pejz1VibrPtU+/4i+7wDw9XU8vdZixTKqlVrG04OHgvar9qatNlCfakG82VFudb3yEQD2DR1NtZpDHlbtP10bTH2i0nlzzrAmPHWY8WYi1Rbt5anDvB161VuvUnw/lg9qrNo3vfUjsvamqoVrdmQ3DI9gwW4YHsGC3TA8ggW7YXgEC3bD8AgBLYQ5lOUwfY1+of6QqkdUOwDcV7eiam+RyAsPHr+xHtVuOcALHdaF8fFPbVKXqPbJC56gPpefZCJWqyj+XjsifA/VPs/Tz3QDQLuqX6n2xIgQ6hO5Xj+zCwDNsnl/usN58VT76dqX9cc6yv+uHiv5CLDPc/hjHR1wB9Xysiqp9ukTn6Y+Rw4/SrX3L+bjvFKjeDh1nhZGtQmV9dfx7Hv46/TBX8qp9rGZvLjKjuyG4REs2A3DI1iwG4ZHsGA3DI9gwW4YHsGC3TA8QpGpNxE5H8Cn8I1kdgDGOefeEpFRAO4HcDw/MMI598PJ7qtKdjDu2aQXeHyVyAtXutyuF8mU+ngn9Rm8l6eTxtV4n2qvTn6MaiGt9e1q0p4XTlRN4X/X2Fr6iCQAGFYmiWrpLSOo9vh+vZ9cAS6jPk1nfkm1RW2qU63iyH9S7bVcfVhQs+QR1GfzRZ2pNrLNfKr17coLm8p2/1q1P9nwG+pTp9wWqkUsWUG11S3q8XX0+JVqlbfrRUp1lvBRZDOv1FPE6St5kdSp5NnzATzinFsjIhUAxIrIAr/2hnOOD80yDOOc4VRmvSUCSPTfzhCROAC1inthhmGcXf7Wd3YRqQegNXwTXAFgiIisF5FJIsI/WxqGUeKccrCLSBiA6QCGO+eOAPgAQAMAreA78o8hfgNFJEZEYrKy+eWthmEUL6cU7CJSBr5An+KcmwEAzrkk51yBc64QwHgAbTVf59w451y0cy66fIh+Pa9hGMVPkcEuIgJgIoA459zYE+w1Tvi1GwFsPPvLMwzjbHEqZ+MvBdAPwAYROV6WNAJAXxFpBV86Lh7AA0XdUUaeYFGy3m/rhqit1O/goOtVe0oUf6yUBjx9cl7KvVQ7fCV/z2rcLk21x73M01OPh4VSLag5T8fcuroH1UZG8AqquO/1VN/mQ7w/Wsaz/BNXr2/5fsyZx7XUdnqKamteb+qTH7SGak3mD6PaqkbPU21hHf2103jFAtUOAItD+Dis0Jcuolq1YVlUW1uOpwczt+hp1toX8r3KPrxLtZcpiKE+p3I2fjkArYHdSXPqhmGcW9gVdIbhESzYDcMjWLAbhkewYDcMj2DBbhgeIaDjnyrUqOpa9b9Z1UrXWUv9jq7Uxy7ld+LppEjH0zhXbWxCtYjDh6mWFKFO1cERl0F9np+9iWoD4oZQ7T/LeWromnbNqZY9vIpq73g9b8o46/03qBZ99UqqXRDSkmo/bCqv2ruP11OvAHBvWnuq5S/k48GSO+lNTAGg0se6veYBPl7rWMNMqj12nr6/AFB/ykSqpa/hV49OraWnKVPL1qc+X8ToezUqZx52FabZ+CfD8DIW7IbhESzYDcMjWLAbhkewYDcMj2DBbhgeIaCz3kIygKb6uDQk5fOqoKjaehOchj/q88QA4FD1eVRbFP0p1dCwIZV2v6ZXtw1uzefUdWy2mGqJNXgKLfQinhLtlryUajOi9Cacn3/+CfXpUMCrAHOWNaXaoQv5XLwlD/+u2meXm0N9dozNp1qz6ASqpfThVYd73yhU7YXT+Uu/VsdbqdZ4L09TLmoxkGr1d99PtbREPaVbN0dvRAkAWRfrVZ2Z67Kpjx3ZDcMjWLAbhkewYDcMj2DBbhgewYLdMDyCBbtheISAVr1VKxfq7miip7aS0jpQv83rntSFhTxl9NTvfMZaUhB/rF0ptam2t+8U1X7JiH9Tn8IcsnYAz0bxWV6tD/MUypxDF1JtRHe9am/qQj5XLjqLV+1Fdeazwz6uyVND7WNzVXu92/VqOAC4JUFPkwHApo5tqHb7Ht4OceoSPY1WfsBC6lPxx4eo1uInXo34Yjh/zjISSc4ZwPolekVfnYJk6lOqg950dMPGvcjMzLaqN8PwMhbshuERLNgNwyNYsBuGR7BgNwyPUOTZeBEJAbAUQFn4Cme+cc6NFJH6AKYCqAwgFkA/55x+CtZPmVLhLqJ0F1WrO2IH9XsnKFK1DzvWl/p068YLUHKWXkG1TkkzqfbEpZeq9ogEfsa61gw+4ml5OvfbeuEtVLs6/TWqJcytpdr/GM57/D2yi5/1fevKy6i2fOIiqi0a9ItqvzSdF5JccBsflTXns3Cq7dwdQrWgUXrvt6a/8pFXG+P01xsAbCx/gGo9jvBj59htfFRWmWX6KKfNVXl24tHEu1X7V+nTkJyfdNpn43MAXOmcuwi+8cxXiUh7AK8CeMM51xDAIQD3ncJ9GYZRQhQZ7M7H8XabZfz/HIArAXzjt38CoFexrNAwjLPCqc5nD/JPcE0GsADADgDpzrnjBcj7AOifHw3DOCc4pWB3zhU451oBqA2gLQDe0eAviMhAEYkRkZjCk3+lNwyjGPlbZ+Odc+kAFgPoACBcRI63+6gNQG0l4pwb55yLds5Fl5LgM1qsYRinT5HBLiKRIhLuvx0KoDuAOPiCvo//1/oDmFVcizQM48w5ldRbS/hOwAXB9+bwlXPueRGJgi/1VgnAWgB3Oud4tQWA+pUquJE9Lla1tysPoH5Nyun9thqH8bE/deovo9q8sG5US8hIolqn6vtV+8FnalKf3Md56u2FN6tS7Z3NPMXz1K51VGtfUy/yeXF9KvUZ3SyNaq8H88KgXg34SKYt0/T0VeumvF9fj/CPqJZ9+1Sq1enCe9CFFUxW7Tm1+WvgyrJ8fz9YwouQMvJ/pFrNON5fL/+YnmLbR0Y8AcCKgXNVe7v3shGzr0BNvRXZcNI5tx5Aa8W+E77v74Zh/BdgV9AZhkewYDcMj2DBbhgewYLdMDyCBbtheISA9qATkYMAdvt/rAKAN4oLHLaOP2Pr+DP/beuo65xT854BDfY/PbBIjHMuukQe3NZh6/DgOuxjvGF4BO46nFcAAAL7SURBVAt2w/AIJRns40rwsU/E1vFnbB1/5n9mHSX2nd0wjMBiH+MNwyOUSLCLyFUiskVEtosIn49U/OuIF5ENIrJORGIC+LiTRCRZRDaeYKskIgtEZJv//4gSWscoEUnw78k6EbkmAOs4X0QWi8gfIrJJRIb57QHdk5OsI6B7IiIhIvKbiPzuX8dzfnt9EVnlj5tpIn+zQYRzLqD/4CuV3QEgCkAwgN8BNA/0OvxriQdQpQQetzOANgA2nmB7DcCT/ttPAni1hNYxCsCjAd6PGgDa+G9XALAVQPNA78lJ1hHQPQEgAML8t8sAWAWgPYCvANzmt38IYNDfud+SOLK3BbDdObfT+VpPTwXAJwT+D+KcWwrgr0XkPeHrGwAEqIEnWUfAcc4lOufW+G9nwNccpRYCvCcnWUdAcT7OepPXkgj2WgD2nvBzSTardADmi0isiAwsoTUcp5pzLtF/+wCAaiW4liEist7/Mb/Yv06ciIjUg69/wiqU4J78ZR1AgPekOJq8ev0EXSfnXBsAVwN4UEQ6l/SCAN87O3xvRCXBBwAawDcjIBHAmEA9sIiEAZgOYLhz7k8tbQK5J8o6Ar4n7gyavDJKItgTAJx/ws+0WWVx45xL8P+fDGAmSrbzTpKI1AAA//98TEsx4pxL8r/QCgGMR4D2RETKwBdgU5xzM/zmgO+Jto6S2hP/Y//tJq+Mkgj21QAa+c8sBgO4DcDsQC9CRMqLSIXjtwH0AMBn9BQ/s+Fr3AmUYAPP48Hl50YEYE9ERABMBBDnnBt7ghTQPWHrCPSeFFuT10CdYfzL2cZr4DvTuQPA0yW0hij4MgG/A9gUyHUA+BK+j4N58H33ug++mXkLAWwD8BOASiW0js8AbACwHr5gqxGAdXSC7yP6egDr/P+uCfSenGQdAd0TAC3ha+K6Hr43lmdPeM3+BmA7gK8BlP0792tX0BmGR/D6CTrD8AwW7IbhESzYDcMjWLAbhkewYDcMj2DBbhgewYLdMDyCBbtheIT/BzFgLEPDXFx/AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g00ncUTSWhBA",
        "outputId": "171ed697-c8d4-43d3-8394-ef6a36cfef00"
      },
      "source": [
        "len(inputs)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "200"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0fG-9jw4UAgZ",
        "outputId": "53d2f582-200b-4b44-f919-f11fe441d38f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        }
      },
      "source": [
        "#IN MODO DA AVERE UN DIZIONARIO UNICO PER IL FINETUNING SINTETICI + IMMAGINI\n",
        "fake_diz = {0:11, 1:5, 2:62, 3:76, 4:27, 5:3, 6:96, 7:33, 8:78, 9:30}\n",
        "labels_of_modified = torch.tensor([fake_diz[c.item()] for c in labels_of_modified]).to('cuda')"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-44-60559553e90e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#IN MODO DA AVERE UN DIZIONARIO UNICO PER IL FINETUNING SINTETICI + IMMAGINI\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mfake_diz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m11\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m62\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m76\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m27\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m96\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m33\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m78\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m9\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mlabels_of_modified\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfake_diz\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlabels_of_modified\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-44-60559553e90e>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#IN MODO DA AVERE UN DIZIONARIO UNICO PER IL FINETUNING SINTETICI + IMMAGINI\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mfake_diz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m11\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m62\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m76\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m27\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m96\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m33\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m78\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m9\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mlabels_of_modified\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfake_diz\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlabels_of_modified\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m: 11"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3g__5sQg-uzp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        },
        "outputId": "847e4c83-8d14-45e4-fc52-1c826f1a91f7"
      },
      "source": [
        "# CREO UN DATALOADER UNICO CON DATI+IMMAGINI SINTETICHE\n",
        "inputs_data = torch.tensor(inputs, requires_grad=False).cpu()\n",
        "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=4, drop_last=True)\n",
        "\n",
        "porcoddio = torch.utils.data.TensorDataset(torch.randn(len(inputs.data), requires_grad=False).cpu(), inputs_data, labels_of_modified.cpu())\n",
        "porcoddiol = DataLoader(porcoddio, batch_size = 128, shuffle=False, drop_last=False)#, num_workers=4, drop_last=False) #A SECONDA DEL BATCH\n",
        "\n",
        "tt = DataLoader(torch.utils.data.ConcatDataset((train_dataset, porcoddio)), batch_size=128, shuffle=False)#, num_workers=4, pin_memory=True)\n",
        "\n",
        "\n",
        "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=4, drop_last=False)\n",
        "\n",
        "for param in model.parameters():\n",
        "  param.requires_grad = False\n",
        "\n",
        "model.train('False')\n",
        "correct = 0.0\n",
        "total = 0.0\n",
        "for _, image, label in tt:\n",
        "  labels = torch.tensor(torch.tensor([diz[c.item()] for c in label]))\n",
        "  labels = labels.to('cuda')\n",
        "  image = image.to('cuda')\n",
        "  outputs = model(image)\n",
        "  _, preds = torch.max(outputs, dim=1)\n",
        "  total += len(labels)\n",
        "  correct += torch.sum(preds == labels).item()\n",
        "\n",
        "acc = correct / total\n",
        "model.train('False')\n",
        "print('test accuracy data + syntetic exemplars', acc, total)\n"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-45-d7e151abce20>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtrain_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_last\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mporcoddio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensorDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_of_modified\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mporcoddiol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mporcoddio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_last\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#, num_workers=4, drop_last=False) #A SECONDA DEL BATCH\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *tensors)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtensor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NalwamQY5jQ8",
        "outputId": "b8d2373f-b565-493c-bc64-ef3a27ad19ed"
      },
      "source": [
        "#PER VERIFICARE COME LA NOSTRA RETE CLASSIFICA LE IMMAGINI SINTETICHE\n",
        "trials.eval()\n",
        "\n",
        "total = 200.0\n",
        "correct = 0.0\n",
        "\n",
        "label = torch.tensor([torch.tensor(diz[c.item()]) for c in labels_of_modified]).to('cuda')\n",
        "outputs = model(inputs.data)\n",
        "_, preds = torch.max(outputs, dim=1)\n",
        "correct += torch.sum(preds == label).item()\n",
        "#total += len(label)\n",
        "acc = correct / total\n",
        "print('test accuracy with syntetic exemplars', acc)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test accuracy with syntetic exemplars 0.815\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FKLFf0sLiKFe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "fcdbd4e864714267b71068777e36fba8",
            "1de959783e4346eaa898c0fc046d033a",
            "825ce2f2713c4e4a8f2df986e1b020a8",
            "a387fbb509c644ffa2667320297a7fc1",
            "c78f7dbd2c0d47e8b810ee9171d37809",
            "ef67c09006fa4d13aa45af4ae514807f",
            "46eb4e799199490198d9b051f6a9558c",
            "f9510ed97f424e4abf6e3ad82780c73b"
          ]
        },
        "outputId": "02b1ca83-de25-48a3-8eed-aa914a8c62d5"
      },
      "source": [
        "#ALLENO UN MODELLO DA ZERO CON DATI+EXEMPLAR SINTETICI\n",
        "#tt = DataLoader(torch.utils.data.ConcatDataset((train_dataset, porcoddio)), batch_size=128, shuffle=True)#, num_workers=4, pin_memory=True)\n",
        "train_dataset = Subset(ilCIFAR100(10, 203, train = 'train'), ilCIFAR100(10, 203, train = 'train').get_batch_indexes()[0])\n",
        "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=4, drop_last=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=4, drop_last=True)\n",
        "fake_model = resnet32(num_classes=100).to('cuda')\n",
        "fake_model.train()\n",
        "\n",
        "optimizer = optim.SGD(fake_model.parameters(), lr=2, momentum=0.9, weight_decay= 1e-5)\n",
        "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[49,63], gamma=0.2)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "for epoch in tqdm(range(70)):\n",
        "  tot_loss = 0.0 \n",
        "  for _, inputs, labels in porcoddiol:\n",
        "    inputs = inputs.float().to('cuda')\n",
        "    labels = torch.tensor([diz[c.item()] for c in labels])\n",
        "\n",
        "    labels=labels.to('cuda')\n",
        "    optimizer.zero_grad()\n",
        "    outputs=fake_model(inputs)\n",
        "\n",
        "    labels_encoded = F.one_hot(labels, 100).float().to('cuda') #CAMBIARE ONE_HOT\n",
        "    loss = criterion(outputs, labels_encoded) \n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    tot_loss+=loss.item()\n",
        "  scheduler.step()\n",
        "  print(f'loss at epoch {epoch}', tot_loss)\n",
        "fake_model.eval()\n",
        "\n",
        "total = 0.0\n",
        "correct = 0.0\n",
        "for  _, images, labels in test_loader:\n",
        "  labels = torch.tensor([torch.tensor(diz[c.item()]) for c in labels])\n",
        "  labels = labels.to('cuda')\n",
        "  images = images.to('cuda')\n",
        "  outputs = fake_model(images)\n",
        "  _, preds = torch.max(outputs, dim=1)\n",
        "  total += len(labels)\n",
        "  correct += torch.sum(preds == labels).item()\n",
        "\n",
        "acc = correct / total\n",
        "print('test accuracy with syntetic exemplars', acc)"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fcdbd4e864714267b71068777e36fba8",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=70.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "loss at epoch 0 1.1401179432868958\n",
            "loss at epoch 1 0.14988379925489426\n",
            "loss at epoch 2 0.08994603902101517\n",
            "loss at epoch 3 0.0893772542476654\n",
            "loss at epoch 4 0.07708057947456837\n",
            "loss at epoch 5 0.06381942145526409\n",
            "loss at epoch 6 0.06636650674045086\n",
            "loss at epoch 7 0.05756368674337864\n",
            "loss at epoch 8 0.048954011872410774\n",
            "loss at epoch 9 0.04114489909261465\n",
            "loss at epoch 10 0.03869227133691311\n",
            "loss at epoch 11 0.0397450802847743\n",
            "loss at epoch 12 0.035477601923048496\n",
            "loss at epoch 13 0.03008199017494917\n",
            "loss at epoch 14 0.026650787331163883\n",
            "loss at epoch 15 0.02303885668516159\n",
            "loss at epoch 16 0.02075072145089507\n",
            "loss at epoch 17 0.01891388278454542\n",
            "loss at epoch 18 0.016376782907173038\n",
            "loss at epoch 19 0.013720250222831964\n",
            "loss at epoch 20 0.011536647565662861\n",
            "loss at epoch 21 0.00941559043712914\n",
            "loss at epoch 22 0.007467508432455361\n",
            "loss at epoch 23 0.006025638082064688\n",
            "loss at epoch 24 0.004759210045449436\n",
            "loss at epoch 25 0.003683226532302797\n",
            "loss at epoch 26 0.0028607500134967268\n",
            "loss at epoch 27 0.002222150273155421\n",
            "loss at epoch 28 0.001764998130965978\n",
            "loss at epoch 29 0.0014101785491220653\n",
            "loss at epoch 30 0.0011517351667862386\n",
            "loss at epoch 31 0.000964469276368618\n",
            "loss at epoch 32 0.0008177048584911972\n",
            "loss at epoch 33 0.000702557823387906\n",
            "loss at epoch 34 0.0006119985482655466\n",
            "loss at epoch 35 0.0005410571902757511\n",
            "loss at epoch 36 0.0004855934967054054\n",
            "loss at epoch 37 0.000440836840425618\n",
            "loss at epoch 38 0.00040347881440538913\n",
            "loss at epoch 39 0.000372380658518523\n",
            "loss at epoch 40 0.00034643237449927256\n",
            "loss at epoch 41 0.00032428606937173754\n",
            "loss at epoch 42 0.0003051665480597876\n",
            "loss at epoch 43 0.0002886551810661331\n",
            "loss at epoch 44 0.0002743904260569252\n",
            "loss at epoch 45 0.0002619471924845129\n",
            "loss at epoch 46 0.00025093218573601916\n",
            "loss at epoch 47 0.00024106195633066818\n",
            "loss at epoch 48 0.00023221477022161707\n",
            "loss at epoch 49 0.00022503321815747768\n",
            "loss at epoch 50 0.000223566334170755\n",
            "loss at epoch 51 0.0002221808972535655\n",
            "loss at epoch 52 0.00022086485842010006\n",
            "loss at epoch 53 0.00021960796584608033\n",
            "loss at epoch 54 0.0002184020631830208\n",
            "loss at epoch 55 0.00021723996178479865\n",
            "loss at epoch 56 0.00021611478587146848\n",
            "loss at epoch 57 0.00021502127492567524\n",
            "loss at epoch 58 0.00021395697694970295\n",
            "loss at epoch 59 0.00021291822486091405\n",
            "loss at epoch 60 0.00021190127154113725\n",
            "loss at epoch 61 0.00021090320660732687\n",
            "loss at epoch 62 0.00020992320060031489\n",
            "loss at epoch 63 0.00020904182019876316\n",
            "loss at epoch 64 0.00020885153207927942\n",
            "loss at epoch 65 0.00020866266277153045\n",
            "loss at epoch 66 0.00020847533596679568\n",
            "loss at epoch 67 0.0002082892387988977\n",
            "loss at epoch 68 0.00020810449495911598\n",
            "loss at epoch 69 0.00020792015129700303\n",
            "\n",
            "test accuracy with syntetic exemplars 0.21875\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "5b6e922aba81479fa8f3a01aef71eb5c",
            "681f49e1c68a45d589dd407747be64c8",
            "7158724f1c5a4f8894d2daafdd4b751e",
            "07e9a6340b0f4c47b99a88142de4c1cc",
            "9719812fbf0d4a29bc690a129f00a83e",
            "920abf2d81174771898ad0a35429dbff",
            "aa37213d5f9d4b55910280aead830a5d",
            "365456adc01b4304aa32562e82c0f0e9"
          ]
        },
        "id": "hDD941kelDM6",
        "outputId": "5ddc9373-c036-4e6e-bb1d-5905aaa16019"
      },
      "source": [
        "#ALLENO UN MODELLO DA ZERO CON SOLO I DATI E CONFRONTO I RISULTATI CON QUELLI PRECEDENTI (PORCODIO)\n",
        "actual_train=ilCIFAR100(10,203,train='train')\n",
        "images_random=[]\n",
        "for label in actual_train.getbatches()[0]:\n",
        "  x=actual_train.get_class_indexes(label)\n",
        "  images_random+=random.sample(list(x),20)\n",
        "\n",
        "print(len(images_random))\n",
        "actual_trainrandom = Subset(ilCIFAR100(10, 203, train = 'exemplars'), images_random)\n",
        "train_loader = DataLoader(actual_trainrandom, batch_size=128, shuffle=True, num_workers=4, drop_last=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=4, drop_last=True)\n",
        "fake_model = resnet32(num_classes=100).to('cuda')\n",
        "fake_model.train()\n",
        "\n",
        "optimizer = optim.SGD(fake_model.parameters(), lr=2, momentum=0.9, weight_decay= 1e-5)\n",
        "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[49,63], gamma=0.2)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "for epoch in tqdm(range(70)):\n",
        "  tot_loss = 0.0 \n",
        "  for _, inputs, labels in train_loader:\n",
        "    inputs = inputs.float().to('cuda')\n",
        "    labels = torch.tensor([diz[c.item()] for c in labels])\n",
        "\n",
        "    labels=labels.to('cuda')\n",
        "    optimizer.zero_grad()\n",
        "    outputs=fake_model(inputs)\n",
        "\n",
        "    labels_encoded = F.one_hot(labels, 100).float().to('cuda') #CAMBIARE ONE_HOT\n",
        "    loss = criterion(outputs, labels_encoded) \n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    tot_loss+=loss.item()\n",
        "  scheduler.step()\n",
        "  print(f'loss at epoch {epoch}', tot_loss)\n",
        "fake_model.eval()\n",
        "\n",
        "total = 0.0\n",
        "correct = 0.0\n",
        "for  _, images, labels in test_loader:\n",
        "  labels = torch.tensor([torch.tensor(diz[c.item()]) for c in labels])\n",
        "  labels = labels.to('cuda')\n",
        "  images = images.to('cuda')\n",
        "  outputs = fake_model(images)\n",
        "  _, preds = torch.max(outputs, dim=1)\n",
        "  total += len(labels)\n",
        "  correct += torch.sum(preds == labels).item()\n",
        "\n",
        "acc = correct / total\n",
        "print('test accuracy without syntetic exemplars', acc)"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "200\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5b6e922aba81479fa8f3a01aef71eb5c",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=70.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "loss at epoch 0 0.7293068766593933\n",
            "loss at epoch 1 0.3170860707759857\n",
            "loss at epoch 2 0.08909604698419571\n",
            "loss at epoch 3 0.05097970739006996\n",
            "loss at epoch 4 0.05021580681204796\n",
            "loss at epoch 5 0.051798753440380096\n",
            "loss at epoch 6 0.05571480467915535\n",
            "loss at epoch 7 0.05696161091327667\n",
            "loss at epoch 8 0.05416073277592659\n",
            "loss at epoch 9 0.04937516525387764\n",
            "loss at epoch 10 0.04545574262738228\n",
            "loss at epoch 11 0.03990786522626877\n",
            "loss at epoch 12 0.03609680011868477\n",
            "loss at epoch 13 0.03393993899226189\n",
            "loss at epoch 14 0.03618962690234184\n",
            "loss at epoch 15 0.032981205731630325\n",
            "loss at epoch 16 0.03181668370962143\n",
            "loss at epoch 17 0.031785786151885986\n",
            "loss at epoch 18 0.030267102643847466\n",
            "loss at epoch 19 0.02826591022312641\n",
            "loss at epoch 20 0.02775735780596733\n",
            "loss at epoch 21 0.027542902156710625\n",
            "loss at epoch 22 0.026358477771282196\n",
            "loss at epoch 23 0.02535872906446457\n",
            "loss at epoch 24 0.024469945579767227\n",
            "loss at epoch 25 0.02441021427512169\n",
            "loss at epoch 26 0.02322368137538433\n",
            "loss at epoch 27 0.022490885108709335\n",
            "loss at epoch 28 0.022599034011363983\n",
            "loss at epoch 29 0.02056173048913479\n",
            "loss at epoch 30 0.021080417558550835\n",
            "loss at epoch 31 0.021515492349863052\n",
            "loss at epoch 32 0.020548855885863304\n",
            "loss at epoch 33 0.01934831775724888\n",
            "loss at epoch 34 0.018433989956974983\n",
            "loss at epoch 35 0.017767850309610367\n",
            "loss at epoch 36 0.017676429823040962\n",
            "loss at epoch 37 0.017322944477200508\n",
            "loss at epoch 38 0.01849815994501114\n",
            "loss at epoch 39 0.01614019274711609\n",
            "loss at epoch 40 0.015529568307101727\n",
            "loss at epoch 41 0.01471499539911747\n",
            "loss at epoch 42 0.01592610962688923\n",
            "loss at epoch 43 0.01339767687022686\n",
            "loss at epoch 44 0.013154740445315838\n",
            "loss at epoch 45 0.012157394550740719\n",
            "loss at epoch 46 0.012850627303123474\n",
            "loss at epoch 47 0.013104426674544811\n",
            "loss at epoch 48 0.011682502925395966\n",
            "loss at epoch 49 0.011246034875512123\n",
            "loss at epoch 50 0.010075236670672894\n",
            "loss at epoch 51 0.010681865736842155\n",
            "loss at epoch 52 0.009748237207531929\n",
            "loss at epoch 53 0.01024404726922512\n",
            "loss at epoch 54 0.008715128526091576\n",
            "loss at epoch 55 0.009609580039978027\n",
            "loss at epoch 56 0.009475764818489552\n",
            "loss at epoch 57 0.008558257482945919\n",
            "loss at epoch 58 0.008333403617143631\n",
            "loss at epoch 59 0.008356505073606968\n",
            "loss at epoch 60 0.009043611586093903\n",
            "loss at epoch 61 0.008091977797448635\n",
            "loss at epoch 62 0.008529801853001118\n",
            "loss at epoch 63 0.007632899098098278\n",
            "loss at epoch 64 0.007582022808492184\n",
            "loss at epoch 65 0.007963174022734165\n",
            "loss at epoch 66 0.007191656623035669\n",
            "loss at epoch 67 0.007101262453943491\n",
            "loss at epoch 68 0.007264147978276014\n",
            "loss at epoch 69 0.007515011355280876\n",
            "\n",
            "test accuracy without syntetic exemplars 0.40625\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zMVWq87mcCqu"
      },
      "source": [
        "fake_model.eval()\n",
        "total = 0.0\n",
        "correct = 0.0\n",
        "for  _, images, labels in test_loader:\n",
        "  labels = torch.tensor([torch.tensor(diz[c.item()]) for c in labels])\n",
        "  labels = labels.to('cuda')\n",
        "  images = images.to('cuda')\n",
        "  outputs = fake_model(images)\n",
        "  _, preds = torch.max(outputs, dim=1)\n",
        "  total += len(labels)\n",
        "  correct += torch.sum(preds == labels).item()\n",
        "\n",
        "acc = correct / total\n",
        "print('test accuracy with syntetic exemplars', acc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "umrCzlLBgXYq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}