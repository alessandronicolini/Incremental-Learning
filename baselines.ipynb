{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "baselines.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alessandronicolini/Incremental-Learning/blob/main/baselines.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hOXnxAapLbVU"
      },
      "source": [
        "setup to import important files from google drive or from github \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i6y5HJY-Zdew",
        "outputId": "68ccf31e-5639-4944-c5de-c1e77d1864a1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# upload work files from your google drive account\n",
        "\n",
        "from google.colab import drive\n",
        "import sys\n",
        "\n",
        "drive.mount('/content/gdrive')\n",
        "gdrive_path = 'Colab Notebooks/Incremental-Learning' # change according your google drive proj dir path\n",
        "sys.path.append('/content/gdrive/My Drive/'+ gdrive_path) # update paths list where 'import' command searches for elements"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pbdul6vvZiSE",
        "outputId": "2766f985-bf1d-4255-ea2b-3531aecc91d3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# upload work files from your git hub repository\n",
        "import sys\n",
        "\n",
        "!git clone https://github.com/alessandronicolini/Incremental-Learning.git # clone proj repository\n",
        "!rm -rf Incremental-Learning/README.md \n",
        "!rm -rf Incremental-Learning/baselines.ipynb\n",
        "sys.path.append('Incremental-Learning/')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Incremental-Learning'...\n",
            "remote: Enumerating objects: 41, done.\u001b[K\n",
            "remote: Counting objects: 100% (41/41), done.\u001b[K\n",
            "remote: Compressing objects: 100% (29/29), done.\u001b[K\n",
            "remote: Total 41 (delta 16), reused 25 (delta 9), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (41/41), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4jIc1PVTNP_"
      },
      "source": [
        "import libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sufg5mojTQ3r"
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import Subset, DataLoader\n",
        "from torchvision import transforms\n",
        "import numpy as np\n",
        "\n",
        "# project classes --------------------------------------------------------------\n",
        "from ilcifar100 import ilCIFAR100\n",
        "from resnet_cifar import resnet32"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n8WFt77-NOFj"
      },
      "source": [
        "# Define params\n",
        "\n",
        "NUM_RUN = 3\n",
        "SEEDS = [1,2,3]\n",
        "NUM_EPOCHS = 70\n",
        "BATCH_SIZE = 128\n",
        "NUM_CLASS_BATCH = 10"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVaFpW0UFjgC"
      },
      "source": [
        "# Define transformations for training\n",
        "train_transform = transforms.Compose([transforms.RandomCrop(32, padding=4),\n",
        "                                      transforms.RandomHorizontalFlip(),\n",
        "                                      transforms.ToTensor(), # Turn PIL Image to torch.Tensor\n",
        "                                      transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "# Define transformations for evaluation\n",
        "test_transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))                                    \n",
        "])"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_Blk7veRWwY"
      },
      "source": [
        "FINE TUNING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NyUjA03BRQsH",
        "outputId": "72bf1d21-455d-460f-faf1-1f9f829e900b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for current_run in range(NUM_RUN):\n",
        "    \n",
        "    original_training = ilCIFAR100(root=\"dataset\", seed=SEEDS[current_run], transform = train_transform)\n",
        "    original_test = ilCIFAR100(root=\"dataset\", seed=SEEDS[current_run], transform=test_transform)\n",
        "    \n",
        "    for current_class_batch in range(NUM_CLASS_BATCH):\n",
        "        \n",
        "        train_indices = original_training.batches[current_class_batch]['train']\n",
        "        train_set = Subset(dataset=original_training, indices=train_indices)\n",
        "\n",
        "        val_indices = original_training.batches[current_class_batch]['val']\n",
        "        val_set = Subset(dataset=original_training, indices=val_indices)\n",
        "\n",
        "        test_indices = original_test.batches[current_class_batch]\n",
        "        test_set = Subset(dataset=original_test, indices=test_indices)\n",
        "\n",
        "        train_dataloader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
        "        val_dataloader = DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
        "        train_dataloader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
        "\n",
        "        # write model train here"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IsYAMeA9Cb1b"
      },
      "source": [
        "!rm -r Incremental-Learning/"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}