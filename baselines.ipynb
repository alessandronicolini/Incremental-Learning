{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "baselines.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alessandronicolini/IncrementalLearning/blob/main/baselines.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hOXnxAapLbVU"
      },
      "source": [
        "Import project files files from github \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pbdul6vvZiSE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50c1084c-bad0-4c73-81e2-665b52b70803"
      },
      "source": [
        "# upload work files from your git hub repository\n",
        "import sys\n",
        "\n",
        "!git clone https://github.com/alessandronicolini/IncrementalLearning.git # clone proj repository\n",
        "!rm -rf IncrementalLearning/README.md \n",
        "!rm -rf IncrementalLearning/baselines.ipynb\n",
        "\n",
        "path = 'IncrementalLearning/'\n",
        "if path not in sys.path:\n",
        "    sys.path.append('IncrementalLearning/')\n",
        "\n",
        "!pip3 install import_ipynb"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'IncrementalLearning' already exists and is not an empty directory.\n",
            "Requirement already satisfied: import_ipynb in /usr/local/lib/python3.6/dist-packages (0.1.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4jIc1PVTNP_"
      },
      "source": [
        "import libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sufg5mojTQ3r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2adc1ba7-0736-4d6c-8e35-71c7780e213b"
      },
      "source": [
        "import torch\n",
        "import os\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Subset, DataLoader\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms\n",
        "import numpy as np\n",
        "import import_ipynb\n",
        "import copy\n",
        "# project classes --------------------------------------------------------------\n",
        "from cifar100 import ilCIFAR100\n",
        "from resnet_cifar import resnet32\n",
        "# from info_recorder.info_log import InfoLog\n",
        "# from models_benchmark.benchmark import Benchmark"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "importing Jupyter notebook from cifar100.ipynb\n",
            "Files already downloaded and verified\n",
            "[60, 34, 84, 67, 85, 44, 18, 48, 1, 47, 61, 35, 82, 58, 76, 29, 71, 0, 79, 93, 56, 90, 20, 43, 26, 7, 73, 25, 9, 65, 95, 51, 11, 2, 74, 28, 96, 27, 99, 64, 70, 42, 62, 8, 98, 77, 39, 88, 10, 94, 3, 52, 68, 32, 5, 72, 38, 75, 69, 30, 40, 41, 24, 55, 91, 45, 12, 16, 22, 53, 63, 57, 31, 33, 21, 83, 49, 81, 59, 78, 97, 19, 46, 17, 36, 87, 6, 13, 14, 89, 80, 23, 86, 37, 54, 92, 50, 66, 15, 4]\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "<class 'numpy.ndarray'>\n",
            "<class 'numpy.ndarray'>\n",
            "<class 'numpy.ndarray'>\n",
            "<class 'numpy.ndarray'>\n",
            "<class 'numpy.ndarray'>\n",
            "<class 'numpy.ndarray'>\n",
            "<class 'numpy.ndarray'>\n",
            "<class 'numpy.ndarray'>\n",
            "<class 'numpy.ndarray'>\n",
            "<class 'numpy.ndarray'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5MbZh4btZAL0"
      },
      "source": [
        "Initial setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n8WFt77-NOFj"
      },
      "source": [
        "# Define params\n",
        "\n",
        "NUM_RUN = 2\n",
        "SEEDS = [1,2,3]\n",
        "NUM_EPOCHS = 70\n",
        "BATCH_SIZE = 128\n",
        "CLASSES_PER_BATCH = 10\n",
        "DEVICE = 'cuda'\n",
        "NUM_BATCHES=10\n",
        "WEIGHT_DECAY = 1e-5\n",
        "LR = 2\n",
        "MOMENTUM = 0.9\n",
        "MILESTONES = [49, 63]\n",
        "GAMMA = 0.2\n",
        "randomseed=10\n",
        "SAVINGS_DIR = \"savings\"\n",
        "# Create savings folder\n",
        "try:\n",
        "    os.mkdir(SAVINGS_DIR)\n",
        "except FileExistsError:\n",
        "    pass"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oxC1KcPZHTC"
      },
      "source": [
        "Define transforms"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVaFpW0UFjgC"
      },
      "source": [
        "# Define transformations for training\n",
        "train_transform = transforms.Compose([transforms.RandomCrop(32, padding=4),\n",
        "                                      transforms.RandomHorizontalFlip(),\n",
        "                                      transforms.ToTensor(), # Turn PIL Image to torch.Tensor\n",
        "                                      transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "# Define transformations for evaluation\n",
        "test_transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))                                    \n",
        "])"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_Blk7veRWwY"
      },
      "source": [
        "Prepare dataloders for each train, val test subset of the original dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFcJL2szqDAa"
      },
      "source": [
        "FINE TUNING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SE4x-xxEUwlF"
      },
      "source": [
        "def trainmodel(model,trainloader,valloader,learningrate,weightdecay,numepochs,milestones,gamma,criterion,optimizer,scheduler,dictionary):\n",
        "  model=model.to(DEVICE)\n",
        "  best_model_wts = copy.deepcopy(model.state_dict())\n",
        "  best_acc = 0.0\n",
        "  for epoch in range(numepochs):\n",
        "    #print(epoch)\n",
        "    for phase in ['train','val']:\n",
        "      if phase=='train':\n",
        "        model.train()\n",
        "        loader=trainloader\n",
        "      else:\n",
        "        model.eval()\n",
        "        loader=valloader\n",
        "      running_loss=0.0\n",
        "      running_corrects=0\n",
        "      num_elements=0\n",
        "      for inputs,labels in loader:\n",
        "        labels = torch.tensor([torch.tensor(diz[c.item()]) for c in labels])\n",
        "        num_elements+=len(inputs)\n",
        "        inputs=inputs.to(DEVICE)\n",
        "        labels=labels.to(DEVICE)\n",
        "        optimizer.zero_grad()\n",
        "        with torch.set_grad_enabled(phase == 'train'):\n",
        "          outputs=model(inputs)\n",
        "          labels_encoded=torch.eye(100)[labels] #CAMBIARE ONE_HOT\n",
        "          labels_encoded=labels_encoded.to(DEVICE)\n",
        "          _,preds=torch.max(outputs,1)\n",
        "          loss=criterion(outputs,labels_encoded)\n",
        "          if phase=='train':\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        running_corrects += torch.sum(preds == labels.data)\n",
        "      if phase == 'train':\n",
        "        scheduler.step()\n",
        "\n",
        "      epoch_loss = running_loss / num_elements\n",
        "      epoch_acc = running_corrects.double() / num_elements\n",
        "      \n",
        "            # deep copy the model\n",
        "      if phase == 'val' and epoch_acc > best_acc:\n",
        "          best_acc = epoch_acc\n",
        "          best_model_wts = copy.deepcopy(model.state_dict())\n",
        "          #print(best_acc)\n",
        "    # load best model weights\n",
        "  model.load_state_dict(best_model_wts)\n",
        "  return model\n"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hnBcaqvMcyPi",
        "outputId": "a1b21e53-e1c7-4fe4-f955-4b6b245f5ef5"
      },
      "source": [
        "original_training_set = ilCIFAR100(CLASSES_PER_BATCH,randomseed)\n",
        "original_test_set = ilCIFAR100(CLASSES_PER_BATCH,randomseed, train=False)\n",
        "train_indices, val_indices = original_training_set.get_train_val(0.1)\n",
        "test_indices = original_test_set.get_batch_indexes()\n",
        "current_test_indexes=[]\n",
        "model=resnet32(num_classes=100)\n",
        "diz = original_training_set.get_dict()\n",
        "acc=[]\n",
        "criterion=nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
        "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=MILESTONES, gamma=GAMMA)\n",
        "for i in range(NUM_BATCHES):\n",
        "  print(i)\n",
        "  train_dataset=Subset(original_training_set,train_indices[i])\n",
        "  val_dataset=Subset(original_training_set,val_indices[i])\n",
        "  current_test_indexes+=test_indices[i].tolist()\n",
        "  test_dataset=Subset(original_test_set,current_test_indexes)\n",
        "  train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "  val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "  test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "  net=trainmodel(model,train_dataloader,val_dataloader,LR,WEIGHT_DECAY,NUM_EPOCHS,MILESTONES,GAMMA,criterion,optimizer,scheduler,diz)\n",
        "  net = net.to(DEVICE) # this will bring the network to GPU if DEVICE is cuda\n",
        "  net.train(False) # Set Network to evaluation mode\n",
        "  running_corrects = 0\n",
        "  y_true = []\n",
        "  y_pred = []\n",
        "  for inputs, labels in test_dataloader:\n",
        "      labels = torch.tensor([torch.tensor(diz[c.item()]) for c in labels])\n",
        "      inputs = inputs.to(DEVICE)\n",
        "      labels = labels.to(DEVICE)\n",
        "      outputs = net(inputs)\n",
        "      _, preds = torch.max(outputs.data, 1)\n",
        "      running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "      labels=labels.detach().cpu().numpy()\n",
        "      labels=labels.tolist()\n",
        "      y_true.extend(labels)\n",
        "      preds=preds.detach().cpu().numpy()\n",
        "      y_pred_tmp = [p for p in preds]\n",
        "      y_pred.extend(y_pred_tmp)\n",
        "\n",
        "  # Calculate Accuracy\n",
        "  accuracy = running_corrects / float(len(test_dataloader.dataset))\n",
        "  print('Test Accuracy: %.2f' % (100.0 * accuracy))\n",
        "  print('-' * 80)\n",
        "  acc.append(accuracy)\n",
        "\n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "[73, 4, 54, 61, 99, 1, 26, 59, 62, 35, 83, 20, 98, 66, 91, 41, 9, 31, 46, 5, 53, 17, 77, 45, 48, 79, 36, 33, 58, 22, 38, 81, 78, 71, 30, 56, 75, 2, 37, 0, 15, 8, 12, 19, 34, 23, 49, 92, 88, 42, 90, 28, 27, 65, 87, 84, 96, 51, 32, 10, 14, 93, 52, 85, 95, 82, 64, 76, 80, 44, 57, 18, 11, 72, 68, 3, 24, 39, 55, 6, 47, 13, 63, 70, 7, 40, 25, 74, 89, 86, 29, 94, 67, 97, 69, 16, 43, 60, 50, 21]\n",
            "Files already downloaded and verified\n",
            "[73, 4, 54, 61, 99, 1, 26, 59, 62, 35, 83, 20, 98, 66, 91, 41, 9, 31, 46, 5, 53, 17, 77, 45, 48, 79, 36, 33, 58, 22, 38, 81, 78, 71, 30, 56, 75, 2, 37, 0, 15, 8, 12, 19, 34, 23, 49, 92, 88, 42, 90, 28, 27, 65, 87, 84, 96, 51, 32, 10, 14, 93, 52, 85, 95, 82, 64, 76, 80, 44, 57, 18, 11, 72, 68, 3, 24, 39, 55, 6, 47, 13, 63, 70, 7, 40, 25, 74, 89, 86, 29, 94, 67, 97, 69, 16, 43, 60, 50, 21]\n",
            "<class 'numpy.ndarray'>\n",
            "<class 'numpy.ndarray'>\n",
            "<class 'numpy.ndarray'>\n",
            "<class 'numpy.ndarray'>\n",
            "<class 'numpy.ndarray'>\n",
            "<class 'numpy.ndarray'>\n",
            "<class 'numpy.ndarray'>\n",
            "<class 'numpy.ndarray'>\n",
            "<class 'numpy.ndarray'>\n",
            "<class 'numpy.ndarray'>\n",
            "0\n",
            "Test Accuracy: 82.10\n",
            "--------------------------------------------------------------------------------\n",
            "1\n",
            "Test Accuracy: 36.75\n",
            "--------------------------------------------------------------------------------\n",
            "2\n",
            "Test Accuracy: 26.13\n",
            "--------------------------------------------------------------------------------\n",
            "3\n",
            "Test Accuracy: 21.22\n",
            "--------------------------------------------------------------------------------\n",
            "4\n",
            "Test Accuracy: 15.68\n",
            "--------------------------------------------------------------------------------\n",
            "5\n",
            "Test Accuracy: 12.63\n",
            "--------------------------------------------------------------------------------\n",
            "6\n",
            "Test Accuracy: 10.81\n",
            "--------------------------------------------------------------------------------\n",
            "7\n",
            "Test Accuracy: 9.15\n",
            "--------------------------------------------------------------------------------\n",
            "8\n",
            "Test Accuracy: 8.59\n",
            "--------------------------------------------------------------------------------\n",
            "9\n",
            "Test Accuracy: 8.34\n",
            "--------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JkYF_sGNjbeI"
      },
      "source": [
        ""
      ],
      "execution_count": 23,
      "outputs": []
    }
  ]
}