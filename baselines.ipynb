{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "baselines.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alessandronicolini/IncrementalLearning/blob/main/baselines.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hOXnxAapLbVU"
      },
      "source": [
        "Import project files files from github \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pbdul6vvZiSE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d712ee2-5b85-4b91-baf8-4b6f5414f763"
      },
      "source": [
        "# upload work files from your git hub repository\n",
        "import sys\n",
        "\n",
        "!git clone https://github.com/alessandronicolini/IncrementalLearning.git # clone proj repository\n",
        "!rm -rf IncrementalLearning/README.md \n",
        "!rm -rf IncrementalLearning/baselines.ipynb\n",
        "\n",
        "path = 'IncrementalLearning/'\n",
        "if path not in sys.path:\n",
        "    sys.path.append('IncrementalLearning/')\n",
        "\n",
        "!pip3 install import_ipynb"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'IncrementalLearning' already exists and is not an empty directory.\n",
            "Requirement already satisfied: import_ipynb in /usr/local/lib/python3.6/dist-packages (0.1.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4jIc1PVTNP_"
      },
      "source": [
        "import libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sufg5mojTQ3r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86b779e4-1e4f-4ac7-84e4-3801de0f0deb"
      },
      "source": [
        "import torch\n",
        "import os\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Subset, DataLoader\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms\n",
        "import numpy as np\n",
        "import import_ipynb\n",
        "\n",
        "# project classes --------------------------------------------------------------\n",
        "from cifar100 import ilCIFAR100\n",
        "# from baselines.resnet import resnet32\n",
        "# from info_recorder.info_log import InfoLog\n",
        "# from models_benchmark.benchmark import Benchmark"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "importing Jupyter notebook from cifar100.ipynb\n",
            "Files already downloaded and verified\n",
            "[60, 34, 84, 67, 85, 44, 18, 48, 1, 47, 61, 35, 82, 58, 76, 29, 71, 0, 79, 93, 56, 90, 20, 43, 26, 7, 73, 25, 9, 65, 95, 51, 11, 2, 74, 28, 96, 27, 99, 64, 70, 42, 62, 8, 98, 77, 39, 88, 10, 94, 3, 52, 68, 32, 5, 72, 38, 75, 69, 30, 40, 41, 24, 55, 91, 45, 12, 16, 22, 53, 63, 57, 31, 33, 21, 83, 49, 81, 59, 78, 97, 19, 46, 17, 36, 87, 6, 13, 14, 89, 80, 23, 86, 37, 54, 92, 50, 66, 15, 4]\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "<class 'numpy.ndarray'>\n",
            "<class 'numpy.ndarray'>\n",
            "<class 'numpy.ndarray'>\n",
            "<class 'numpy.ndarray'>\n",
            "<class 'numpy.ndarray'>\n",
            "<class 'numpy.ndarray'>\n",
            "<class 'numpy.ndarray'>\n",
            "<class 'numpy.ndarray'>\n",
            "<class 'numpy.ndarray'>\n",
            "<class 'numpy.ndarray'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5MbZh4btZAL0"
      },
      "source": [
        "Initial setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n8WFt77-NOFj"
      },
      "source": [
        "# Define params\n",
        "\n",
        "NUM_RUN = 2\n",
        "SEEDS = [1,2,3]\n",
        "NUM_EPOCHS = 5\n",
        "BATCH_SIZE = 64\n",
        "CLASSES_PER_BATCH = 10\n",
        "DEVICE = 'cuda'\n",
        "\n",
        "WEIGHT_DECAY = 1e-5\n",
        "LR = 2\n",
        "MOMENTUM = 0.9\n",
        "MILESTONES = [49, 63]\n",
        "GAMMA = 0.2\n",
        "\n",
        "SAVINGS_DIR = \"savings\"\n",
        "# Create savings folder\n",
        "try:\n",
        "    os.mkdir(SAVINGS_DIR)\n",
        "except FileExistsError:\n",
        "    pass"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oxC1KcPZHTC"
      },
      "source": [
        "Define transforms"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVaFpW0UFjgC"
      },
      "source": [
        "# Define transformations for training\n",
        "train_transform = transforms.Compose([transforms.RandomCrop(32, padding=4),\n",
        "                                      transforms.RandomHorizontalFlip(),\n",
        "                                      transforms.ToTensor(), # Turn PIL Image to torch.Tensor\n",
        "                                      transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "# Define transformations for evaluation\n",
        "test_transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))                                    \n",
        "])"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_Blk7veRWwY"
      },
      "source": [
        "Prepare dataloders for each train, val test subset of the original dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NyUjA03BRQsH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46daa257-23be-46a0-b357-e64bbdade710"
      },
      "source": [
        "# dataloaders dict initialization\n",
        "dataloaders = {run:{batch:{'train':None, 'val':None, 'test':None} for batch in range(CLASSES_PER_BATCH)} for run in range(NUM_RUN)}\n",
        "\n",
        "# make dataloaders\n",
        "for run in range(NUM_RUN):\n",
        "    \n",
        "    # download dataset only on the first run\n",
        "    download = True\n",
        "    if run != 0:\n",
        "        download = False\n",
        "    \n",
        "    original_training_set = ilCIFAR100(10,12)\n",
        "    original_test_set = ilCIFAR100(CLASSES_PER_BATCH, train=False, seed=SEEDS[run])\n",
        "    \n",
        "    train_indices, val_indices = original_training_set.get_train_val(0.1)\n",
        "    test_indices = original_training_set.get_batch_indexes()\n",
        "\n",
        "    for class_batch in range(CLASSES_PER_BATCH):\n",
        "        \n",
        "        # test indices are the uninion of test indices of batches seen up to now\n",
        "            \n",
        "\n",
        "        # make subsets\n",
        "        train_set = Subset(dataset=original_training_set, indices=train_indices[class_batch])\n",
        "        val_set = Subset(dataset=original_training_set, indices=val_indices[class_batch])\n",
        "        test_set = Subset(dataset=original_test_set, indices=test_indices[class_batch])\n",
        "\n",
        "        # make dataloaders\n",
        "        train_dataloader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\n",
        "        val_dataloader = DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=True)\n",
        "        test_dataloader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "        # add dataloaders to the dataloaders dict\n",
        "        dataloaders[run][class_batch]['train'] = train_dataloader\n",
        "        dataloaders[run][class_batch]['val'] = val_dataloader\n",
        "        dataloaders[run][class_batch]['test'] = test_dataloader"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "[60, 34, 84, 67, 85, 44, 18, 48, 1, 47, 61, 35, 82, 58, 76, 29, 71, 0, 79, 93, 56, 90, 20, 43, 26, 7, 73, 25, 9, 65, 95, 51, 11, 2, 74, 28, 96, 27, 99, 64, 70, 42, 62, 8, 98, 77, 39, 88, 10, 94, 3, 52, 68, 32, 5, 72, 38, 75, 69, 30, 40, 41, 24, 55, 91, 45, 12, 16, 22, 53, 63, 57, 31, 33, 21, 83, 49, 81, 59, 78, 97, 19, 46, 17, 36, 87, 6, 13, 14, 89, 80, 23, 86, 37, 54, 92, 50, 66, 15, 4]\n",
            "Files already downloaded and verified\n",
            "[17, 72, 97, 8, 32, 15, 63, 57, 60, 83, 48, 26, 12, 62, 3, 49, 55, 77, 0, 92, 34, 29, 75, 13, 40, 85, 2, 74, 69, 1, 89, 27, 54, 98, 28, 56, 93, 35, 14, 22, 61, 43, 59, 71, 78, 18, 70, 88, 86, 41, 6, 11, 82, 46, 67, 7, 21, 95, 68, 42, 87, 19, 45, 31, 47, 25, 73, 30, 94, 23, 79, 39, 76, 58, 5, 64, 99, 91, 52, 24, 33, 80, 16, 66, 90, 96, 50, 84, 36, 44, 81, 10, 9, 38, 20, 4, 51, 65, 37, 53]\n",
            "<class 'numpy.ndarray'>\n",
            "<class 'numpy.ndarray'>\n",
            "<class 'numpy.ndarray'>\n",
            "<class 'numpy.ndarray'>\n",
            "<class 'numpy.ndarray'>\n",
            "<class 'numpy.ndarray'>\n",
            "<class 'numpy.ndarray'>\n",
            "<class 'numpy.ndarray'>\n",
            "<class 'numpy.ndarray'>\n",
            "<class 'numpy.ndarray'>\n",
            "Files already downloaded and verified\n",
            "[60, 34, 84, 67, 85, 44, 18, 48, 1, 47, 61, 35, 82, 58, 76, 29, 71, 0, 79, 93, 56, 90, 20, 43, 26, 7, 73, 25, 9, 65, 95, 51, 11, 2, 74, 28, 96, 27, 99, 64, 70, 42, 62, 8, 98, 77, 39, 88, 10, 94, 3, 52, 68, 32, 5, 72, 38, 75, 69, 30, 40, 41, 24, 55, 91, 45, 12, 16, 22, 53, 63, 57, 31, 33, 21, 83, 49, 81, 59, 78, 97, 19, 46, 17, 36, 87, 6, 13, 14, 89, 80, 23, 86, 37, 54, 92, 50, 66, 15, 4]\n",
            "Files already downloaded and verified\n",
            "[7, 11, 10, 46, 21, 94, 85, 39, 32, 77, 27, 90, 4, 74, 20, 55, 81, 50, 65, 47, 69, 56, 64, 34, 87, 3, 96, 59, 40, 48, 54, 67, 95, 22, 30, 29, 86, 98, 93, 62, 8, 91, 58, 23, 57, 43, 35, 60, 28, 82, 26, 80, 33, 78, 37, 66, 73, 51, 97, 25, 88, 70, 15, 31, 17, 36, 84, 38, 44, 68, 42, 14, 92, 52, 18, 79, 53, 89, 45, 99, 41, 5, 72, 83, 9, 75, 12, 49, 24, 13, 19, 16, 6, 2, 71, 1, 63, 61, 76, 0]\n",
            "<class 'numpy.ndarray'>\n",
            "<class 'numpy.ndarray'>\n",
            "<class 'numpy.ndarray'>\n",
            "<class 'numpy.ndarray'>\n",
            "<class 'numpy.ndarray'>\n",
            "<class 'numpy.ndarray'>\n",
            "<class 'numpy.ndarray'>\n",
            "<class 'numpy.ndarray'>\n",
            "<class 'numpy.ndarray'>\n",
            "<class 'numpy.ndarray'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFcJL2szqDAa"
      },
      "source": [
        "FINE TUNING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IsYAMeA9Cb1b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        },
        "outputId": "96ea17f5-cf34-412c-d79d-1812a069199c"
      },
      "source": [
        "for run in range(NUM_RUN):\n",
        "    benchmark = Benchmark(\n",
        "        num_epochs=NUM_EPOCHS, \n",
        "        dataloaders=dataloaders,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        saving_folder=\"savings/fine_tuning\")\n",
        "    \n",
        "    benchmark.set_criterion(nn.CrossEntropyLoss())\n",
        "    benchmark.set_infoLog(InfoLog, run)\n",
        "    benchmark.set_model(resnet32())\n",
        "\n",
        "    for class_batch in range(NUM_CLASS_BATCH):\n",
        "        parameters_to_optimize = benchmark.model.parameters()\n",
        "        benchmark.set_optimizer(optim.SGD(parameters_to_optimize, lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY))\n",
        "        benchmark.set_scheduler(optim.lr_scheduler.MultiStepLR(benchmark.optimizer, milestones=MILESTONES, gamma=GAMMA))\n",
        "\n",
        "        benchmark.do_class_batch(run, class_batch)\n",
        "\n",
        "        benchmark.model.add_nodes(10) # forse Ã¨ meglio metterlo all'inizio"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-3b6ba8ab9469>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrun\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNUM_RUN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     benchmark = Benchmark(\n\u001b[0m\u001b[1;32m      3\u001b[0m         \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNUM_EPOCHS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mdataloaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataloaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Benchmark' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SE4x-xxEUwlF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}