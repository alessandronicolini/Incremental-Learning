{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "baselines.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alessandronicolini/IncrementalLearning/blob/main/baselines.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hOXnxAapLbVU"
      },
      "source": [
        "Import project files files from github \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pbdul6vvZiSE"
      },
      "source": [
        "# upload work files from your git hub repository\n",
        "import sys\n",
        "\n",
        "!git clone https://github.com/alessandronicolini/IncrementalLearning.git # clone proj repository\n",
        "!rm -rf IncrementalLearning/README.md \n",
        "!rm -rf IncrementalLearning/baselines.ipynb\n",
        "\n",
        "path = 'IncrementalLearning/'\n",
        "if path not in sys.path:\n",
        "    sys.path.append('IncrementalLearning/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4jIc1PVTNP_"
      },
      "source": [
        "import libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sufg5mojTQ3r"
      },
      "source": [
        "import torch\n",
        "import os\n",
        "from torch.utils.data import Subset, DataLoader\n",
        "from torchvision import transforms\n",
        "import numpy as np\n",
        "\n",
        "# project classes --------------------------------------------------------------\n",
        "from dataset.ilcifar100 import ilCIFAR100\n",
        "from baselines.resnet import resnet32\n",
        "from info_recorder.info_log import InfoLog\n",
        "from models_benchmark.benchmark import Benchmark"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5MbZh4btZAL0"
      },
      "source": [
        "Initial setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n8WFt77-NOFj"
      },
      "source": [
        "# Define params\n",
        "\n",
        "NUM_RUN = 3\n",
        "SEEDS = [1,2,3]\n",
        "NUM_EPOCHS = 70\n",
        "BATCH_SIZE = 128\n",
        "NUM_CLASS_BATCH = 10\n",
        "DEVICE = 'cuda'\n",
        "\n",
        "# Create savings folder\n",
        "try:\n",
        "    os.mkdir(\"savings\")\n",
        "except FileExistsError:\n",
        "    pass"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oxC1KcPZHTC"
      },
      "source": [
        "Define transforms"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVaFpW0UFjgC"
      },
      "source": [
        "# Define transformations for training\n",
        "train_transform = transforms.Compose([transforms.RandomCrop(32, padding=4),\n",
        "                                      transforms.RandomHorizontalFlip(),\n",
        "                                      transforms.ToTensor(), # Turn PIL Image to torch.Tensor\n",
        "                                      transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "# Define transformations for evaluation\n",
        "test_transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))                                    \n",
        "])"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_Blk7veRWwY"
      },
      "source": [
        "Prepare dataloders for each train, val test subset of the original dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NyUjA03BRQsH"
      },
      "source": [
        "# dataloaders dict initialization\n",
        "dataloaders = {run:{batch:{'train':None, 'val':None, 'test':None} for batch in range(NUM_CLASS_BATCH)} for run in range(NUM_RUN)}\n",
        "\n",
        "# make dataloaders\n",
        "for run in range(NUM_RUN):\n",
        "    \n",
        "    original_training_set = ilCIFAR100(root=\"cifar\", seed=SEEDS[run], transform=train_transform)\n",
        "    original_test_set = ilCIFAR100(root=\"cifar\", train=False, seed=SEEDS[run], transform=test_transform)\n",
        "    \n",
        "    for class_batch in range(NUM_CLASS_BATCH):\n",
        "        \n",
        "        # get indices\n",
        "        train_indices = original_training_set.batches[class_batch]['train']\n",
        "        val_indices = original_training_set.batches[class_batch]['val']\n",
        "        test_indices = original_test_set.batches[class_batch]\n",
        "\n",
        "        # make subsets\n",
        "        train_set = Subset(dataset=original_training_set, indices=train_indices)\n",
        "        val_set = Subset(dataset=original_training_set, indices=val_indices)\n",
        "        test_set = Subset(dataset=original_test_set, indices=test_indices)\n",
        "\n",
        "        # make dataloaders\n",
        "        train_dataloader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\n",
        "        val_dataloader = DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=True)\n",
        "        test_dataloader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "        # add dataloaders to the dataloaders dict\n",
        "        dataloaders[run][class_batch]['train'] = train_dataloader\n",
        "        dataloaders[run][class_batch]['val'] = val_dataloader\n",
        "        dataloaders[run][class_batch]['test'] = test_dataloader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IsYAMeA9Cb1b"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}