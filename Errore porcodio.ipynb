{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Immagini sintetiche integrate nell'allenamento.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "8cf040765ab74dbaadad053a354daf90": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_2d54b65c0d66461eaa32669b22d60a92",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_d7bbbea14a4642ccb0fae794fe27452b",
              "IPY_MODEL_cfcb505a079d45109b8809cb39663d9e"
            ]
          }
        },
        "2d54b65c0d66461eaa32669b22d60a92": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d7bbbea14a4642ccb0fae794fe27452b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_68849cae536d4e989e533ad376b910cd",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 70,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 70,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3fe118bf3d6b447580d04e3d768b978a"
          }
        },
        "cfcb505a079d45109b8809cb39663d9e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_6d57853ebd9f40cfa4820a95f0f4990e",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 70/70 [09:09&lt;00:00,  7.85s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_dde161228c3842548624717186633725"
          }
        },
        "68849cae536d4e989e533ad376b910cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3fe118bf3d6b447580d04e3d768b978a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6d57853ebd9f40cfa4820a95f0f4990e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "dde161228c3842548624717186633725": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "34fc9558c5c844f8bbbd057d938a13de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_6b2dd5c6fe0949b8825b7df113dee6e1",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_40a58b073e924c459db2c10cced3a3e2",
              "IPY_MODEL_7469612806524cc285c1be1e11da6b83"
            ]
          }
        },
        "6b2dd5c6fe0949b8825b7df113dee6e1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "40a58b073e924c459db2c10cced3a3e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_ad4ffa5060b041649e20b07c54196ecf",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 30,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 30,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a0205e1a3dcb4bee8f8807601a19dfdb"
          }
        },
        "7469612806524cc285c1be1e11da6b83": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_ea81207411964d6f97ad1c3e65d3494f",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 30/30 [10:29&lt;00:00, 20.98s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_950639fdad4649d9ab674fb968c180ed"
          }
        },
        "ad4ffa5060b041649e20b07c54196ecf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a0205e1a3dcb4bee8f8807601a19dfdb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ea81207411964d6f97ad1c3e65d3494f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "950639fdad4649d9ab674fb968c180ed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8ab9014f6da940dcbca287ae87403955": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_9e1312a916f548cc82647afb4bc1f93e",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_7f041afe43e14af79fd2f6597cc43b75",
              "IPY_MODEL_80948880c9864340a539797dcfb95562"
            ]
          }
        },
        "9e1312a916f548cc82647afb4bc1f93e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7f041afe43e14af79fd2f6597cc43b75": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_79cdaacab4f2456e9d70b6eebf42e594",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1500,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1500,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8679353713f544a38be7e308738302c8"
          }
        },
        "80948880c9864340a539797dcfb95562": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_154763e3f13d4164acf06ac0f3a8a3d3",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1500/1500 [13:54&lt;00:00,  1.80it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_971fba7bbe9642a4b7ceeb872429bcec"
          }
        },
        "79cdaacab4f2456e9d70b6eebf42e594": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8679353713f544a38be7e308738302c8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "154763e3f13d4164acf06ac0f3a8a3d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "971fba7bbe9642a4b7ceeb872429bcec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2a31ea6f07bf47d3b943744ebb4dc2da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_979432db69974b7f983e64b9556ae519",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_e0c734b418f24d02af2c89cb85818e61",
              "IPY_MODEL_1b873a72e9eb4f50a22f743a0c683077"
            ]
          }
        },
        "979432db69974b7f983e64b9556ae519": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e0c734b418f24d02af2c89cb85818e61": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_a53c583c6ca440948a71a37875694ccb",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 70,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 70,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b6ddd90b25544dc1a9478d0546c9d537"
          }
        },
        "1b873a72e9eb4f50a22f743a0c683077": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_40e4b81f48f94a379a06f520765652f9",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 70/70 [14:09&lt;00:00, 12.13s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_296fe8ad9d23414ab1cd80d06b00960c"
          }
        },
        "a53c583c6ca440948a71a37875694ccb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b6ddd90b25544dc1a9478d0546c9d537": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "40e4b81f48f94a379a06f520765652f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "296fe8ad9d23414ab1cd80d06b00960c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "bf39eaf1eea44f0c8d762ab8df73c5ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_5b83f1a6811942b59e33e81f32675894",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_7bb470edfcca4f4689ab9d218954c24f",
              "IPY_MODEL_897f829eb1fd4021a8b3bf00e4b8acff"
            ]
          }
        },
        "5b83f1a6811942b59e33e81f32675894": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7bb470edfcca4f4689ab9d218954c24f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_654ea4fa57a3410598bbefcdb6982985",
            "_dom_classes": [],
            "description": " 11%",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 70,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 8,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_91405581e3964a22ab923b4cf26c0836"
          }
        },
        "897f829eb1fd4021a8b3bf00e4b8acff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_799ea6b942454dd08f0d86d34a9a9788",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 8/70 [04:37&lt;35:48, 34.65s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_065731971c364ca29be56a98229abbda"
          }
        },
        "654ea4fa57a3410598bbefcdb6982985": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "91405581e3964a22ab923b4cf26c0836": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "799ea6b942454dd08f0d86d34a9a9788": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "065731971c364ca29be56a98229abbda": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c1fc62df0e444a798ab7588d23503d91": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_fd2c504656334a0b923944a1ec25c41b",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_794d19adb4154452b5054d4242ddba45",
              "IPY_MODEL_c966a8cd8b9545a7a6689e4cb0357e9e"
            ]
          }
        },
        "fd2c504656334a0b923944a1ec25c41b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "794d19adb4154452b5054d4242ddba45": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_e680fd4baa2d4bf9be5fa51f07028b89",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1000,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1000,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3b8bbc46531843e4ba199661b6aa6963"
          }
        },
        "c966a8cd8b9545a7a6689e4cb0357e9e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_fb7755e922e848f2a5970a0eda5b8d48",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1000/1000 [01:54&lt;00:00,  8.73it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b08e3481631d4bc18fa297155be7f744"
          }
        },
        "e680fd4baa2d4bf9be5fa51f07028b89": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3b8bbc46531843e4ba199661b6aa6963": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "fb7755e922e848f2a5970a0eda5b8d48": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b08e3481631d4bc18fa297155be7f744": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alessandronicolini/IncrementalLearning/blob/main/Errore%20porcodio.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WH7YHXeh0hFj",
        "outputId": "f6d393b8-0320-4f4b-906c-d3f8fdcb4c55"
      },
      "source": [
        "!pip3 install 'import_ipynb'\n",
        "!pip3 install 'tqdm'\n",
        " \n",
        "!rm -r IncrementalLearning\n",
        "# upload work files from your git hub repository\n",
        "import sys\n",
        " \n",
        "!git clone https://github.com/alessandronicolini/IncrementalLearning.git # clone proj repository\n",
        "!rm -rf IncrementalLearning/README.md \n",
        "!rm -rf IncrementalLearning/baselines.ipynb\n",
        " \n",
        "path = 'IncrementalLearning/'\n",
        "if path not in sys.path:\n",
        "    sys.path.append('IncrementalLearning/')\n",
        " \n",
        "!pip3 install import_ipynb"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: import_ipynb in /usr/local/lib/python3.6/dist-packages (0.1.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.41.1)\n",
            "Cloning into 'IncrementalLearning'...\n",
            "remote: Enumerating objects: 190, done.\u001b[K\n",
            "remote: Counting objects: 100% (190/190), done.\u001b[K\n",
            "remote: Compressing objects: 100% (189/189), done.\u001b[K\n",
            "remote: Total 664 (delta 119), reused 0 (delta 0), pack-reused 474\u001b[K\n",
            "Receiving objects: 100% (664/664), 951.29 KiB | 724.00 KiB/s, done.\n",
            "Resolving deltas: 100% (394/394), done.\n",
            "Requirement already satisfied: import_ipynb in /usr/local/lib/python3.6/dist-packages (0.1.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UEF9KBox0cAd"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import VisionDataset\n",
        "from PIL import Image\n",
        "import random\n",
        "import torchvision\n",
        "ROOT = './data'\n",
        "class ilCIFAR100(VisionDataset):\n",
        "    \"\"\"\n",
        "    Extends CIFAR100 class. Split the dataset into 10 batches, each one containing 10 classes.\n",
        "    You can retrieve the batches from the attribute \"batches\", it has different structure according to\n",
        "    test and train CIFAR100 splits:\n",
        "        - train -> batches is a dictionary {0:{'train':indexes, 'val':indexes}...} \n",
        "        - test -> batches is a dictionary {0:indexes...}\n",
        "    where the keys are the batch number.\n",
        "\n",
        "    Args:\n",
        "        root (string): Root directory of dataset where directory\n",
        "            `cifar-10-batches-py` exists or will be saved to if download is set to True.\n",
        "        seed(int): used to ensure reproducibility in shuffling operations.\n",
        "        val_size(float, optional): between 0 and 1, fraction of data used for validation.\n",
        "        train (bool, optional): If True, creates dataset from training set, otherwise\n",
        "            creates from test set.\n",
        "        transform (callable, optional): A function/transform that takes in an PIL image\n",
        "            and returns a transformed version. E.g, `transforms.RandomCrop`\n",
        "        target_transform (callable, optional): A function/transform that takes in the\n",
        "            target and transforms it.\n",
        "        download (bool, optional): If true, downloads the dataset from the internet and\n",
        "            puts it in root directory. If dataset is already downloaded, it is not\n",
        "            downloaded again.\n",
        "    \"\"\"\n",
        "    def __init__(self,classes_per_batch, seed, val_size=0.1, train=True, transform=None, target_transform=False, \n",
        "    download=True):\n",
        "        \n",
        "        super(ilCIFAR100, self).__init__(root=0)\n",
        "        self.classes_per_batch=classes_per_batch\n",
        "\n",
        "        \n",
        "        self.__rs = seed # set random seed \n",
        "        self.train=train\n",
        "        self.__transform_train = transforms.Compose([\n",
        "            transforms.RandomCrop(32, padding=4),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))\n",
        "        ])\n",
        "\n",
        "        self.__transform_test = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))\n",
        "        ])\n",
        "        # if train:\n",
        "        #     self.batches = self.__make_train_batches(val_size)\n",
        "        # else:\n",
        "        #     self.batches = self.__make_test_batches()\n",
        "        if self.train == 'train':\n",
        "          self.dataset = torchvision.datasets.CIFAR100(root=ROOT, train=True,\n",
        "                                            download=True, transform=self.__transform_train)\n",
        "        elif self.train == 'exemplars':\n",
        "          self.dataset = torchvision.datasets.CIFAR100(root=ROOT, train=True,\n",
        "                                            download=True, transform=self.__transform_test)\n",
        "        else:\n",
        "          self.dataset = torchvision.datasets.CIFAR100(root=ROOT, train=False,\n",
        "                                        download=True, transform=self.__transform_test)\n",
        "          \n",
        "        self.targets = np.array(self.dataset.targets) # make targets an array to exploit masking\n",
        "        random.seed(seed)\n",
        "        self.classes = random.sample(range(0, 100), 100)\n",
        "        #self.classes = self.classes.reshape((10, -1)) # each row contains the classes for the corrisponding batch\n",
        "        #print(self.classes)\n",
        "        self.__dictionary = {}\n",
        "        for i, c in enumerate(self.classes):\n",
        "          self.__dictionary[c] = i\n",
        "\n",
        "\n",
        "    def get_dict(self):\n",
        "      return self.__dictionary\n",
        "    def __getitem__(self, index):\n",
        "        return index,self.dataset.__getitem__(index)[0],self.dataset.__getitem__(index)[1]\n",
        "    def __len__(self):\n",
        "        return self.dataset.__len__()\n",
        "    def getbatches(self):\n",
        "      classlist=self.classes\n",
        "      batches=[]\n",
        "      for i in range(0,int(100/self.classes_per_batch)):\n",
        "        #print(i)\n",
        "        batch=classlist[int(i*self.classes_per_batch):int(i*10+self.classes_per_batch)]\n",
        "        batches.append(batch)\n",
        "      return batches\n",
        "    def get_batch_indexes(self):\n",
        "      classlist=self.classes\n",
        "      numclass=self.classes_per_batch\n",
        "      batch_indexes=[]\n",
        "      for i in range(0,int(100/self.classes_per_batch)):\n",
        "        batch=classlist[int(i*numclass):int(i*numclass+numclass)]\n",
        "        mask=np.isin(self.targets,batch)\n",
        "        indexes=np.array(np.arange(len(self.dataset.targets)))\n",
        "        indexes=indexes[mask]\n",
        "        batch_indexes.append(indexes)\n",
        "      return batch_indexes\n",
        "    def get_class_indexes(self,label):\n",
        "      indexes = np.array(np.arange(len(self.dataset.targets)))\n",
        "      labels = self.dataset.targets\n",
        "      mask = np.isin(labels, label)\n",
        "      indexes = indexes[mask]\n",
        "\n",
        "      return indexes\n",
        "    def get_train_val(self,valid):\n",
        "      batches=self.get_batch_indexes()\n",
        "      train=[]\n",
        "      val=[]\n",
        "      for batch in batches:\n",
        "        #print(type(batch))\n",
        "        random.shuffle(batch)\n",
        "        valbatch=batch[0:int(valid*len(batch))]\n",
        "        trainbatch=batch[int(valid*len(batch)):]\n",
        "        train.append(trainbatch)\n",
        "        val.append(valbatch)\n",
        "      return train,val\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import time\n",
        "import os\n",
        "import argparse\n",
        "import numpy as np\n",
        "\n",
        "def tensor2im(input_image, imtype=np.uint8):\n",
        "    #CIFAR100\n",
        "    mean = [0.5071, 0.4867, 0.4408] \n",
        "    std = [0.2675, 0.2565, 0.2761]\n",
        "    #CIFAR10\n",
        "    #mean = [0.4914, 0.4822, 0.4465]\n",
        "    #std = [0.2023, 0.1994, 0.2010]\n",
        "    if not isinstance(input_image, np.ndarray):\n",
        "        if isinstance(input_image, torch.Tensor):\n",
        "            image_tensor = input_image.data\n",
        "        else:\n",
        "            return input_image\n",
        "        image_numpy = image_tensor.cpu().detach().float().numpy()\n",
        "        if image_numpy.shape[0] == 1:\n",
        "            image_numpy = np.tile(image_numpy, (3, 1, 1))\n",
        "        for i in range(len(mean)): \n",
        "            image_numpy[i] = image_numpy[i] * std[i] + mean[i]\n",
        "        image_numpy = image_numpy * 255\n",
        "        image_numpy = np.transpose(image_numpy, (1, 2, 0))\n",
        "    else:\n",
        "        image_numpy = input_image\n",
        "    return image_numpy.astype(imtype)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5O4jUchQ1EAI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6aea21e5-55ab-4406-dbc3-b474a9d5f7c0"
      },
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import math\n",
        "from sklearn.preprocessing import normalize\n",
        "import copy\n",
        "import torchvision.datasets as dsets\n",
        "import torchvision.models as models\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.gridspec as gridspec\n",
        "from torch.utils.data import Subset, DataLoader, Dataset\n",
        "import random\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pickle\n",
        "import pandas as pd\n",
        "\n",
        "import import_ipynb\n",
        "#from IncrementalLearning.cifar100 import ilCIFAR100\n",
        "\n",
        "from IncrementalLearning.resnet_cifar import resnet32\n",
        "from tqdm.notebook import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "importing Jupyter notebook from /content/IncrementalLearning/resnet_cifar.ipynb\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HzqwQeHB1Tg-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12f829e9-a9fd-4f5d-ad8a-e14bbf46cb1f"
      },
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "from __future__ import unicode_literals\n",
        "\n",
        "import argparse\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "# import torch.nn.parallel\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.optim as optim\n",
        "# import torch.utils.data\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.utils as vutils\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import glob\n",
        "import collections\n",
        "\n",
        "#from resnet_cifar import ResNet34, ResNet18\n",
        "\n",
        "try:\n",
        "    from apex.parallel import DistributedDataParallel as DDP\n",
        "    from apex import amp, optimizers\n",
        "    USE_APEX = True\n",
        "except ImportError:\n",
        "    print(\"Please install apex from https://www.github.com/nvidia/apex to run this example.\")\n",
        "    print(\"will attempt to run without it\")\n",
        "    USE_APEX = False\n",
        "\n",
        "#provide intermeiate information\n",
        "debug_output = False\n",
        "debug_output = True\n",
        "\n",
        "\n",
        "class DeepInversionFeatureHook():\n",
        "    '''\n",
        "    Implementation of the forward hook to track feature statistics and compute a loss on them.\n",
        "    Will compute mean and variance, and will use l2 as a loss\n",
        "    '''\n",
        "\n",
        "    def __init__(self, module):\n",
        "        self.hook = module.register_forward_hook(self.hook_fn)\n",
        "\n",
        "    def hook_fn(self, module, input, output):\n",
        "        # hook co compute deepinversion's feature distribution regularization\n",
        "        nch = input[0].shape[1]\n",
        "\n",
        "        mean = input[0].mean([0, 2, 3])\n",
        "        var = input[0].permute(1, 0, 2, 3).contiguous().view([nch, -1]).var(1, unbiased=False)\n",
        "\n",
        "        # forcing mean and variance to match between two distributions\n",
        "        # other ways might work better, e.g. KL divergence\n",
        "        r_feature = torch.norm(module.running_var.data.type(var.type()) - var, 2) + torch.norm(\n",
        "            module.running_mean.data.type(var.type()) - mean, 2)\n",
        "\n",
        "        self.r_feature = r_feature\n",
        "        # must have no output\n",
        "\n",
        "    def close(self):\n",
        "        self.hook.remove()\n",
        "\n",
        "def get_images(net, bs=256, epochs=1000, idx=-1, var_scale=0.00005, competitive_scale=0.01,\n",
        "               net_student=None, prefix=None, train_writer = None, global_iteration=None,\n",
        "               use_amp=False, bn_reg_scale = 0.0,\n",
        "               optimizer = None, inputs = None, labels = False, l2_coeff=0.0):\n",
        "    '''\n",
        "    Function returns inverted images from the pretrained model, parameters are tight to CIFAR dataset\n",
        "    args in:\n",
        "        net: network to be inverted\n",
        "        bs: batch size\n",
        "        epochs: total number of iterations to generate inverted images, training longer helps a lot!\n",
        "        idx: an external flag for printing purposes: only print in the first round, set as -1 to disable\n",
        "        var_scale: the scaling factor for variance loss regularization. this may vary depending on bs\n",
        "            larger - more blurred but less noise\n",
        "        net_student: model to be used for Adaptive DeepInversion\n",
        "        prefix: defines the path to store images\n",
        "        competitive_scale: coefficient for Adaptive DeepInversion\n",
        "        train_writer: tensorboardX object to store intermediate losses\n",
        "        global_iteration: indexer to be used for tensorboard\n",
        "        use_amp: boolean to indicate usage of APEX AMP for FP16 calculations - twice faster and less memory on TensorCores\n",
        "        optimizer: potimizer to be used for model inversion\n",
        "        inputs: data place holder for optimization, will be reinitialized to noise\n",
        "        bn_reg_scale: weight for r_feature_regularization\n",
        "        random_labels: sample labels from random distribution or use columns of the same class\n",
        "        l2_coeff: coefficient for L2 loss on input\n",
        "    return:\n",
        "        A tensor on GPU with shape (bs, 3, 32, 32) for CIFAR\n",
        "    '''\n",
        "\n",
        "    kl_loss = nn.KLDivLoss(reduction='batchmean').cuda()\n",
        "\n",
        "    # preventing backpropagation through student for Adaptive DeepInversion\n",
        "    net_student.train()\n",
        "\n",
        "    best_cost = 1e6\n",
        "\n",
        "    # initialize gaussian inputs\n",
        "    inputs.data = torch.randn((bs, 3, 32, 32), requires_grad=True, device='cuda')\n",
        "    # if use_amp:\n",
        "    #     inputs.data = inputs.data.half()\n",
        "\n",
        "    # set up criteria for optimization\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    optimizer.state = collections.defaultdict(dict)  # Reset state of optimizer\n",
        "\n",
        "    # target outputs to generate\n",
        "    #if labels:\n",
        "    targets = labels\n",
        "    #else:\n",
        "     #   targets = torch.LongTensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9] * 25 + [0, 1, 2, 3, 4, 5]).to('cuda')\n",
        "\n",
        "    outputs=net(inputs.data)\n",
        "    _, predicted_teach = outputs.max(1)\n",
        "\n",
        "    outputs_student=net_student(inputs.data)\n",
        "    _, predicted_std = outputs_student.max(1)\n",
        "\n",
        "    if idx == 0:\n",
        "        print('Teacher correct out of {}: {}, loss at {}'.format(bs, predicted_teach.eq(targets).sum().item(), criterion(outputs, targets).item()))\n",
        "        print('Student correct out of {}: {}, loss at {}'.format(bs, predicted_std.eq(targets).sum().item(), criterion(outputs_student, targets).item()))\n",
        "\n",
        "    ## Create hooks for feature statistics catching\n",
        "    loss_r_feature_layers = []\n",
        "    for module in net.modules():\n",
        "        if isinstance(module, nn.BatchNorm2d):\n",
        "            loss_r_feature_layers.append(DeepInversionFeatureHook(module))\n",
        "\n",
        "    # setting up the range for jitter\n",
        "    lim_0, lim_1 = 2, 2\n",
        "\n",
        "    for epoch in tqdm(range(epochs)):\n",
        "        # apply random jitter offsets\n",
        "        off1 = random.randint(-lim_0, lim_0)\n",
        "        off2 = random.randint(-lim_1, lim_1)\n",
        "        inputs_jit = torch.roll(inputs, shifts=(off1,off2), dims=(2,3))\n",
        "\n",
        "        # foward with jit images\n",
        "        optimizer.zero_grad()\n",
        "        net.zero_grad()\n",
        "        outputs = net(inputs_jit)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss_target = loss.item()\n",
        "\n",
        "        # competition loss, Adaptive DeepInvesrion\n",
        "        if competitive_scale != 0.0:\n",
        "            net_student.zero_grad()\n",
        "            outputs_student = net_student(inputs_jit)\n",
        "            T = 3.0\n",
        "\n",
        "            if 1:\n",
        "                # jensen shanon divergence:\n",
        "                # another way to force KL between negative probabilities\n",
        "                P = F.softmax(outputs_student / T, dim=1)\n",
        "                Q = F.softmax(outputs / T, dim=1)\n",
        "                M = 0.5 * (P + Q)\n",
        "\n",
        "                P = torch.clamp(P, 0.01, 0.99)\n",
        "                Q = torch.clamp(Q, 0.01, 0.99)\n",
        "                M = torch.clamp(M, 0.01, 0.99)\n",
        "                eps = 0.0\n",
        "                # loss_verifier_cig = 0.5 * kl_loss(F.log_softmax(outputs_verifier / T, dim=1), M) +  0.5 * kl_loss(F.log_softmax(outputs/T, dim=1), M)\n",
        "                loss_verifier_cig = 0.5 * kl_loss(torch.log(P + eps), M) + 0.5 * kl_loss(torch.log(Q + eps), M)\n",
        "                # JS criteria - 0 means full correlation, 1 - means completely different\n",
        "                loss_verifier_cig = 1.0 - torch.clamp(loss_verifier_cig, 0.0, 1.0)\n",
        "\n",
        "                loss = loss + competitive_scale * loss_verifier_cig\n",
        "\n",
        "        # apply total variation regularization\n",
        "        diff1 = inputs_jit[:,:,:,:-1] - inputs_jit[:,:,:,1:]\n",
        "        diff2 = inputs_jit[:,:,:-1,:] - inputs_jit[:,:,1:,:]\n",
        "        diff3 = inputs_jit[:,:,1:,:-1] - inputs_jit[:,:,:-1,1:]\n",
        "        diff4 = inputs_jit[:,:,:-1,:-1] - inputs_jit[:,:,1:,1:]\n",
        "        loss_var = torch.norm(diff1) + torch.norm(diff2) + torch.norm(diff3) + torch.norm(diff4)\n",
        "        loss = loss + var_scale*loss_var\n",
        "\n",
        "        # R_feature loss\n",
        "        loss_distr = sum([mod.r_feature for mod in loss_r_feature_layers])\n",
        "        loss = loss + bn_reg_scale*loss_distr # best for noise before BN\n",
        "\n",
        "        # l2 loss\n",
        "        if 1:\n",
        "            loss = loss + l2_coeff * torch.norm(inputs_jit, 2)\n",
        "\n",
        "        if debug_output and epoch % 200==0:\n",
        "            print(f\"It {epoch}\\t Losses: total: {loss.item():3.3f},\\ttarget: {loss_target:3.3f} \\tR_feature_loss unscaled:\\t {loss_distr.item():3.3f}\")\n",
        "            #vutils.save_image(inputs.data.clone(),\n",
        "             #                 './{}/output_{}.png'.format(prefix, epoch//200),\n",
        "              #                normalize=True, scale_each=True, nrow=10)\n",
        "\n",
        "        if best_cost > loss.item():\n",
        "            best_cost = loss.item()\n",
        "            best_inputs = inputs.data\n",
        "\n",
        "        # backward pass\n",
        "        if use_amp:\n",
        "            with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
        "                scaled_loss.backward()\n",
        "        else:\n",
        "            loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "    outputs=net(best_inputs)\n",
        "    _, predicted_teach = outputs.max(1)\n",
        "\n",
        "    outputs_student=net_student(best_inputs)\n",
        "    _, predicted_std = outputs_student.max(1)\n",
        "\n",
        "    if idx == 0:\n",
        "        print('Teacher correct out of {}: {}, loss at {}'.format(bs, predicted_teach.eq(targets).sum().item(), criterion(outputs, targets).item()))\n",
        "        print('Student correct out of {}: {}, loss at {}'.format(bs, predicted_std.eq(targets).sum().item(), criterion(outputs_student, targets).item()))\n",
        "\n",
        "    name_use = \"best_images\"\n",
        "    if prefix is not None:\n",
        "        name_use = prefix + name_use\n",
        "    next_batch = len(glob.glob(\"./%s/*.png\" % name_use)) // 1\n",
        "\n",
        "    #vutils.save_image(best_inputs[:20].clone(),\n",
        "     #                 './{}/output_{}.png'.format(name_use, next_batch),\n",
        "      #                normalize=True, scale_each = True, nrow=10)\n",
        "\n",
        "    #if train_writer is not None:\n",
        "     #   train_writer.add_scalar('gener_teacher_criteria', criterion(outputs, targets), global_iteration)\n",
        "      #  train_writer.add_scalar('gener_student_criteria', criterion(outputs_student, targets), global_iteration)\n",
        "\n",
        "       # train_writer.add_scalar('gener_teacher_acc', predicted_teach.eq(targets).sum().item() / bs, global_iteration)\n",
        "       # train_writer.add_scalar('gener_student_acc', predicted_std.eq(targets).sum().item() / bs, global_iteration)\n",
        "\n",
        "        #train_writer.add_scalar('gener_loss_total', loss.item(), global_iteration)\n",
        "        #train_writer.add_scalar('gener_loss_var', (var_scale*loss_var).item(), global_iteration)\n",
        "\n",
        "    net_student.train()\n",
        "\n",
        "    return best_inputs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Please install apex from https://www.github.com/nvidia/apex to run this example.\n",
            "will attempt to run without it\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "id": "RAoV7Gq_aBW3",
        "outputId": "56828663-f8d3-46eb-b559-cc6aeeb319d4"
      },
      "source": [
        "class mnemonics():\n",
        "  def __init__(self, randomseed):\n",
        "    self.device = 'cuda'\n",
        "    self.model = resnet32(num_classes=100).to(self.device)\n",
        "    self.feature_extractor = self.model.features\n",
        "    self.temp_model = None\n",
        "    self.lr = 2\n",
        "    self.gamma = 0.2\n",
        "    self.weight_decay = 1e-5 \n",
        "    self.milestones = [49,63]\n",
        "    self.batch_size = 128\n",
        "    self.numepochs = 1\n",
        "    self.n_classes = 0\n",
        "    self.n_known = 0\n",
        "    self.feature_size=64\n",
        "    self.momentum=0.9\n",
        "    self.criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    self.NUM_BATCHES=10\n",
        "    self.randomseed=randomseed\n",
        "    self.trainloader=None\n",
        "    self.testloader=None\n",
        "    self.CLASSES_PER_BATCH=10\n",
        "\n",
        "    self.original_training_set = ilCIFAR100(self.CLASSES_PER_BATCH, self.randomseed, train = 'train')\n",
        "    self.original_exemplar_set = ilCIFAR100(self.CLASSES_PER_BATCH, self.randomseed, train = 'exemplars')\n",
        "    self.original_test_set = ilCIFAR100(self.CLASSES_PER_BATCH,self.randomseed, train= 'test')\n",
        "\n",
        "    self.last_test = None\n",
        "    self.y_pred = []\n",
        "    self.y_test = []\n",
        "\n",
        "    self.cumulative_class_mean = []\n",
        "\n",
        "    self.classes_seen=0\n",
        "    self.diz = self.original_training_set.get_dict()\n",
        "\n",
        "  def modification_of_random_exemplars(self, indexes, n_epochs):\n",
        "    # CODICE PER CREARE LE IMMAGINI SINTETICHE, ORA ME LE CREO VOLTA PER VOLTA, QUINDI AD OGNI BATCH DIMENTICO QUELLE GIA FATTE E NE CREO NUOVE\n",
        "    print('len of task:', len(indexes))\n",
        "\n",
        "    inputs_dataset = torch.zeros((len(indexes),3,32,32))\n",
        "    labels_of_modified = torch.zeros(len(indexes), dtype = int).to('cuda')\n",
        "\n",
        "    for i, el in enumerate(indexes):\n",
        "      inputs_dataset[i,:,:,:] = self.original_training_set.__getitem__(el)[1]\n",
        "      labels_of_modified[i] = self.diz[self.original_training_set.__getitem__(el)[2]]\n",
        "\n",
        "\n",
        "    number_of_images_created = len(indexes)\n",
        "\n",
        "    teacher = copy.deepcopy(self.model)\n",
        "    net_teacher = resnet32(num_classes=100).to('cuda')\n",
        "    net_teacher.load_state_dict(teacher.state_dict())\n",
        "    net_teacher.eval()\n",
        "\n",
        "    net_student = resnet32(num_classes=100).to('cuda')\n",
        "    #net_student = resnet18().to('cuda')\n",
        "\n",
        "    self.model.eval()\n",
        "\n",
        "    #inputs = torch.randn((number_of_images_created, 3, 32, 32), requires_grad=True, device='cuda', dtype=torch.float)\n",
        "    inputs_dataset = torch.tensor(inputs_dataset.data, requires_grad=True, dtype = torch.float, device = 'cuda')\n",
        "\n",
        "    train_writer = None  # tensorboard writter\n",
        "    global_iteration = 0\n",
        "    di_lr = 0.05\n",
        "    optimizer_di = optim.Adam([inputs_dataset], lr=di_lr)\n",
        "    #plt.imshow(tensor2im(inputs_dataset[0]))\n",
        "    #plt.show()\n",
        "    print(\"Starting model inversion\")\n",
        "    batch_idx = 0\n",
        "    inputs = get_images(net=net_teacher, bs=len(labels_of_modified), epochs=n_epochs, idx=batch_idx, \n",
        "                      net_student=net_student, competitive_scale=10.0, l2_coeff = 3e-8, bn_reg_scale = 1.0,\n",
        "                      train_writer=train_writer, use_amp=False,\n",
        "                      optimizer=optimizer_di, inputs=inputs_dataset, \n",
        "                      var_scale=2e-5, labels=labels_of_modified) #2.5e-5\n",
        "    #plt.imshow(tensor2im(inputs[0]))\n",
        "    #plt.show()\n",
        "    print('deepinversion finshed')\n",
        "    output.clear()\n",
        "    inputs_data = torch.tensor(inputs, requires_grad=False).cpu()\n",
        "    fake_diz = {0:11, 1:5, 2:62, 3:76, 4:27, 5:3, 6:96, 7:33, 8:78, 9:30}\n",
        "    labels = torch.tensor([fake_diz[c.item()] for c in labels_of_modified])\n",
        "    return inputs_data, labels\n",
        "\n",
        "  def model_level_optimization(self):\n",
        "    \n",
        "    old_model = copy.deepcopy(self.model)\n",
        "    old_model.eval()\n",
        "    old_model.to(self.device)\n",
        "    n_classes = self.classes_seen+self.CLASSES_PER_BATCH\n",
        "    print(n_classes)\n",
        "    optimizer = optim.SGD(self.model.parameters(), lr=self.lr, momentum=self.momentum, weight_decay=self.weight_decay)\n",
        "    scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=self.milestones, gamma=self.gamma)\n",
        "    for epoch in tqdm(range(self.numepochs)):\n",
        "        \n",
        "      for _, inputs, labels in self.trainloader:\n",
        "        inputs = inputs.float().to(self.device)\n",
        "        labels = torch.tensor([self.diz[c.item()] for c in labels])\n",
        "\n",
        "        labels=labels.to(self.device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs=self.model(inputs)\n",
        "\n",
        "        labels_encoded = F.one_hot(labels,100).float().to(self.device) #CAMBIARE ONE_HOT\n",
        "        \n",
        "        if self.classes_seen:\n",
        "          old_target = old_model(inputs).to(self.device)\n",
        "          old_target = torch.sigmoid(old_target).to(self.device)\n",
        "          \n",
        "          target = torch.cat((old_target[:,:self.classes_seen], labels_encoded[:, self.classes_seen:]), dim=1)\n",
        "          loss = self.criterion(outputs, target)\n",
        "        else:\n",
        "          loss = self.criterion(outputs, labels_encoded) \n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "      \n",
        "      scheduler.step()\n",
        "\n",
        "\n",
        "  def classify_nme(self, input_batch):\n",
        "    min_distances = float('inf')*torch.ones(len(input_batch)).to(self.device) # shape: batch_size --> 128\n",
        "    y_pred = torch.zeros(len(input_batch), dtype=torch.int8).to(self.device) # shape: batch_size --> 128\n",
        "    input_features = self.model.features(input_batch) # shape: (batch_size, feature_size) --> (128, 64)\n",
        "\n",
        "    for i in range(len(self.exemplar_sets_idxs)):\n",
        "      ex_mean = self.exemplar_means[i,:]\n",
        "\n",
        "      # compute distances between inputs features and exemplar set means\n",
        "      pdist = nn.PairwiseDistance(p=2)\n",
        "      distances = pdist(input_features, ex_mean) # shape: batch_size --> 128\n",
        "\n",
        "      # update min distancies and predicted labels\n",
        "      mask = distances < min_distances\n",
        "      min_distances[mask] = distances[mask]\n",
        "      y_pred[mask] = self.exemplar_labels[i]\n",
        "\n",
        "    return y_pred\n",
        "    \n",
        "\n",
        "\n",
        "  def get_new_exemplars(self, batch, m):\n",
        "    loader = torch.utils.data.DataLoader(batch, batch_size=self.batch_size,shuffle=False, num_workers=4)\n",
        "    features = np.zeros((0,self.feature_size))\n",
        "    indices = np.zeros((0), dtype=int)\n",
        "    with torch.no_grad():\n",
        "      for indexes, images, labels in loader:\n",
        "        images = images.cuda()\n",
        "        feature = self.feature_extractor(images).data.cpu().numpy()\n",
        "        feature = normalize(feature, axis=1, norm='l2')\n",
        "        features = np.concatenate((features,feature), axis=0)\n",
        "        indices = np.concatenate((indices,indexes), axis=0)\n",
        "\n",
        "    class_mean = np.mean(features, axis=0)\n",
        "    class_mean = class_mean / np.linalg.norm(class_mean)  # Normalize\n",
        "\n",
        "    #self.cumulative_class_mean.append(class_mean)\n",
        "\n",
        "    exemplar_set = []\n",
        "    exemplar_features = np.zeros((0,self.feature_size))\n",
        "\n",
        "    for k in range(1, int(m)+1):\n",
        "        S = np.sum(exemplar_features, axis=0)\n",
        "        phi = features\n",
        "        mu = class_mean\n",
        "        mu_p = 1.0 / k * (phi + S)\n",
        "        mu_p = normalize(mu_p, axis=1, norm='l2')\n",
        "        i = np.argmin(np.sqrt(np.sum((mu - mu_p) ** 2, axis=1)))\n",
        "        exemplar_set.append(int(indices[i]))\n",
        "        addfeature =  np.expand_dims(features[i], axis=0)\n",
        "        exemplar_features = np.concatenate((exemplar_features,addfeature), axis=0)\n",
        "\n",
        "        #remove duplicates\n",
        "        features = np.delete(features, i, 0)\n",
        "        indices = np.delete(indices, i, 0)\n",
        "        \n",
        "    self.exemplar_sets_idxs.append(exemplar_set)\n",
        "    #self.exemplar_sets_idxs.append(random.sample(list(batch), m))\n",
        "\n",
        "\n",
        "  def reduce_old_exemplars(self, m):\n",
        "    for i, set_i in enumerate(self.exemplar_sets_idxs):\n",
        "      self.exemplar_sets_idxs[i] = random.sample(set_i, m)\n",
        "\n",
        "  def __accuracy_fc(self, dl, mapper):\n",
        "    total = 0.0\n",
        "    correct = 0.0\n",
        "    for  _, images, labels in dl:\n",
        "      labels = torch.tensor([torch.tensor(mapper[c.item()]) for c in labels])\n",
        "      labels = labels.to(self.device)\n",
        "      images = images.to(self.device)\n",
        "      outputs = self.model(images)\n",
        "      _, preds = torch.max(outputs, dim=1)\n",
        "      total += len(labels)\n",
        "      correct += torch.sum(preds == labels).item()\n",
        "\n",
        "    acc = correct / total\n",
        "    return acc\n",
        "\n",
        "\n",
        "  def __accuracy_nme(self, dl):\n",
        "    \n",
        "    total = 0.0\n",
        "    correct = 0.0\n",
        "    \n",
        "    for  _, images, labels in dl:\n",
        "      labels = labels.to(self.device)\n",
        "      images = images.to(self.device)\n",
        "      preds = self.classify_nme(images)\n",
        "      total += len(labels)\n",
        "      correct += torch.sum(preds == labels).item()\n",
        "\n",
        "      if self.last_test:\n",
        "        self.y_pred += preds.tolist()\n",
        "        self.y_test += labels.tolist()\n",
        "\n",
        "    acc = correct / total\n",
        "    return acc\n",
        "\n",
        "\n",
        "  def plot_confusion_matrix(self):\n",
        " \n",
        "    cm = confusion_matrix(self.y_test, self.y_pred)\n",
        "    cm = np.log(cm+1)\n",
        "    fig, ax = plt.subplots(figsize=(7,7))\n",
        "    sns.heatmap(cm, square=True, cbar=False, ax=ax, cmap=plt.get_cmap('seismic'))\n",
        "    ax.set_xticks(np.linspace(19,99,5))\n",
        "    ax.set_yticks(np.linspace(19,99,5))\n",
        "    ax.set_xticklabels([20,40,60,80,100], rotation=0)\n",
        "    ax.set_yticklabels([20,40,60,80,100], rotation=0)\n",
        "    ax.set_title(\"iCaRL\")\n",
        "    ax.set_xlabel(\"Predicted class\")\n",
        "    ax.set_ylabel(\"True class\")\n",
        "    plt.savefig(\"iCaRL_\"+str(self.randomseed)+\"_cm.png\")\n",
        "    plt.show()\n",
        "    return cm\n",
        "\n",
        "  def plot_data(self, train_dl):\n",
        "\n",
        "    from sklearn.manifold import TSNE\n",
        "    print('------plot data------')\n",
        "\n",
        "    #Data points\n",
        "    train_labels_array = torch.zeros(0).to('cuda')\n",
        "    train_dataset_to_reduce = np.zeros((0,64), dtype = float)\n",
        "\n",
        "    for  _, images, labels in train_dl:\n",
        "      labels = labels.to(self.device)\n",
        "      images = images.to(self.device)\n",
        "      train_dataset_to_reduce = np.concatenate((train_dataset_to_reduce, self.feature_extractor(images).cpu().detach().numpy()))\n",
        "      train_labels_array = torch.cat((train_labels_array, labels))\n",
        "\n",
        "    \n",
        "    #EX e MN loaders \n",
        "    current_exemplar_indices = np.array([], dtype=int)\n",
        "\n",
        "    for exemplar_set in self.exemplar_sets_idxs:\n",
        "      current_exemplar_indices = np.concatenate([current_exemplar_indices, np.array(exemplar_set)])\n",
        "\n",
        "    exemplar_dataset = Subset(self.original_training_set, current_exemplar_indices)\n",
        "    ex_dl = DataLoader(exemplar_dataset, batch_size=self.batch_size, shuffle=False, num_workers=4, drop_last=True) #usato solo per plottare\n",
        "\n",
        "    mn_dataset = Subset(self.original_exemplar_set, current_exemplar_indices)\n",
        "    mn_dl = DataLoader(mn_dataset, batch_size=self.batch_size, shuffle=False, num_workers=4, drop_last=True) #usato solo per plottare\n",
        "\n",
        "\n",
        "    #Exemplars\n",
        "\n",
        "    ex_labels_array = torch.zeros(0).to('cuda')\n",
        "    ex_dataset_to_reduce = np.zeros((0,64), dtype = float)\n",
        "\n",
        "    for  _, images, labels in ex_dl:\n",
        "      labels = labels.to(self.device)\n",
        "      images = images.to(self.device)\n",
        "      ex_dataset_to_reduce = np.concatenate((ex_dataset_to_reduce, self.feature_extractor(images).cpu().detach().numpy()))\n",
        "      ex_labels_array = torch.cat((ex_labels_array, labels), dim = 0)\n",
        "\n",
        "\n",
        "    #Mnemonics\n",
        "\n",
        "\n",
        "    mn_labels_array = torch.zeros(0).to('cuda')\n",
        "    mn_dataset_to_reduce = np.zeros((0,64), dtype = float)\n",
        "\n",
        "    for  _, images, labels in mn_dl:\n",
        "      labels = labels.to(self.device)\n",
        "      images = images.to(self.device)\n",
        "      mn_dataset_to_reduce = np.concatenate((mn_dataset_to_reduce, self.feature_extractor(images).cpu().detach().numpy()))\n",
        "      mn_labels_array = torch.cat((mn_labels_array, labels), dim = 0)\n",
        "\n",
        "    #PLOT'''\n",
        "    total_data_w_exemplars = np.concatenate((train_dataset_to_reduce, ex_dataset_to_reduce))\n",
        "    total_data_w_mn =  np.concatenate((train_dataset_to_reduce, mn_dataset_to_reduce))\n",
        "\n",
        "    total_transformed_ex = TSNE(n_components=2).fit_transform(total_data_w_exemplars)\n",
        "    X_transformed_w_ex = total_transformed_ex[:train_dataset_to_reduce.shape[0]]\n",
        "    ex_transformed = total_transformed_ex[train_dataset_to_reduce.shape[0]:]\n",
        "\n",
        "    total_transformed_mn = TSNE(n_components=2).fit_transform(total_data_w_mn)\n",
        "    X_transformed_w_mn = total_transformed_mn[:train_dataset_to_reduce.shape[0]]\n",
        "    mn_transformed = total_transformed_mn[train_dataset_to_reduce.shape[0]:]\n",
        "\n",
        "    fig, (ax1, ax2) = plt.subplots(1,2, figsize=(24,12))\n",
        "    ax1.scatter(X_transformed_w_ex[:,0], X_transformed_w_ex[:,1], c = train_labels_array.cpu(), alpha = 0.1)\n",
        "    ax1.scatter(ex_transformed[:,0], ex_transformed[:,1], c = ex_labels_array.cpu(), alpha = 1)\n",
        "    #ax1.title('EXEMPLARS')\n",
        "\n",
        "    ax2.scatter(X_transformed_w_mn[:,0], X_transformed_w_mn[:,1], c = train_labels_array.cpu(), alpha = 0.1)\n",
        "    ax2.scatter(mn_transformed[:,0], mn_transformed[:,1], c = mn_labels_array.cpu(), alpha = 1)\n",
        "    #ax2.title('MNEMONICS')\n",
        "    plt.show()\n",
        "\n",
        "  def trainer(self):\n",
        "    \n",
        "    train_indices = self.original_training_set.get_batch_indexes()\n",
        "    test_indices = self.original_test_set.get_batch_indexes()\n",
        "    batches=self.original_training_set.getbatches()\n",
        "    current_test_indexes=[]\n",
        "    test_acc = []\n",
        "    incremental_classes_label = np.array([])\n",
        "    self.last_test = False\n",
        "\n",
        "    n_epochs = 2 #NUMERO DI ITERAZIONI PER GENERARE IMMAGINI\n",
        "\n",
        "    for i in range(self.NUM_BATCHES):\n",
        "      print('current batches', batches[i])\n",
        "      if i == self.NUM_BATCHES-1:\n",
        "        self.last_test = True\n",
        "      '''\n",
        "      #DA CAMBIARE CON SELF.ORIGINAL EXEMPLAR SET\n",
        "      if i > 1: #FINETUNING\n",
        "        print('----inizio finetuning----')\n",
        "        print('numbero of classes in the exemplar sets', len(self.exemplar_sets_idxs))\n",
        "        self.numepochs = 10\n",
        "        self.lr = 0.2\n",
        "        temporary_classes_seen = self.classes_seen\n",
        "        self.classes_seen = 0\n",
        "        self.trainloader = DataLoader(exemplar_dataset, batch_size=self.batch_size, shuffle=False, num_workers=4, drop_last=True) \n",
        "        print('accuracy on exemplar set before finetuining:', 100*self.__accuracy_fc(self.trainloader,self.diz))\n",
        "        self.model.train()\n",
        "        self.model_level_optimization()\n",
        "        #BACK TO THE NORMAL PARAMETERS\n",
        "        self.model.eval()\n",
        "        self.numepochs = 70\n",
        "        self.lr = 2\n",
        "        self.classes_seen = temporary_classes_seen\n",
        "        print('accuracy on exemplar set after finetuining:', 100*self.__accuracy_fc(self.trainloader,self.diz))\n",
        "        current_test_acc = self.__accuracy_nme(self.testloader)\n",
        "        print('accuracy on test set after finetuning:', 100*current_test_acc)\n",
        "        print('-----fine finetuning------')\n",
        "        print('-'*80)\n",
        "      '''\n",
        "      train_dataset = Subset(self.original_training_set, train_indices[i])\n",
        "      current_test_indexes += test_indices[i].tolist()\n",
        "      test_dataset = Subset(self.original_test_set,current_test_indexes)\n",
        "      self.train_loader = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True, num_workers=4, drop_last=True)\n",
        "      self.testloader = DataLoader(test_dataset, batch_size=self.batch_size, shuffle=False, num_workers=4, drop_last=True)        \n",
        "      \n",
        "\n",
        "      if i == 0:\n",
        "        self.trainloader = self.train_loader\n",
        "      else:\n",
        "        self.trainloader = DataLoader(torch.utils.data.ConcatDataset((train_dataset, porcoddio)), batch_size=self.batch_size, shuffle=True)#, num_workers=4, pin_memory=True)\n",
        "\n",
        "        \n",
        "      self.model.train()\n",
        "      self.model_level_optimization()    \n",
        "      self.classes_seen += 10\n",
        "      self.model.eval() # Set Network to evaluation mode\n",
        "      m=int(2000/(int(i*10+10)))\n",
        "\n",
        "      max_number_to_generate = 666 #round(666/..)\n",
        "      incremental_classes_label = np.concatenate((incremental_classes_label, batches[i]))\n",
        "      actual_number_of_images_per_classes_generate = int(max_number_to_generate/len(incremental_classes_label))\n",
        "\n",
        "      images_task_1 = np.array([], dtype = int)\n",
        "      images_task_2 = np.array([], dtype = int)\n",
        "      images_task_3 = np.array([], dtype = int)\n",
        "\n",
        "      for i in incremental_classes_label:\n",
        "        random_images = random.sample(list(self.original_training_set.get_class_indexes(i)), m)\n",
        "        images_task_1 = np.concatenate((images_task_1, random_images[:int(m/3)]))\n",
        "        images_task_2 = np.concatenate((images_task_2, random_images[int(m/3):2*(int(m/3))]))\n",
        "        images_task_3 = np.concatenate((images_task_3, random_images[2*(int(m/3)):]))\n",
        "\n",
        "      task_1, labels_1 = self.modification_of_random_exemplars(images_task_1, n_epochs)\n",
        "      task_2, labels_2 = self.modification_of_random_exemplars(images_task_2, n_epochs)\n",
        "      task_3, labels_3 = self.modification_of_random_exemplars(images_task_3, n_epochs)\n",
        "      print('type 1', type(task_1))\n",
        "      syntetic_exemplars = torch.cat((task_1, task_2, task_3), dim = 0)\n",
        "      syntetic_labels = torch.cat((labels_1, labels_2, labels_3))\n",
        "\n",
        "      print('shape synt', syntetic_exemplars.shape)\n",
        "      print('synt type', syntetic_exemplars[0])\n",
        "      print('len synt', len(syntetic_exemplars))\n",
        "      porcoddio = torch.utils.data.TensorDataset(torch.randn(len(syntetic_labels), requires_grad=False).cpu(), torch.randn(20000,3,32,32), syntetic_labels)\n",
        "      \n",
        "      # reduce the number of each exemplars set\n",
        "      #self.reduce_old_exemplars(m) \n",
        "\n",
        "      #self.cumulative_class_mean = {}\n",
        "'''\n",
        "      for classlabel in batches[i]:\n",
        "        indexes_class = self.original_training_set.get_class_indexes(classlabel)\n",
        "        current_class = Subset(self.original_training_set, indexes_class)\n",
        "        #self.get_new_exemplars(indexes_class, m)\n",
        "        self.get_new_exemplars(current_class, m)\n",
        "        indexes_class = self.original_training_set.get_class_indexes(classlabel)\n",
        "        current_class = Subset(self.original_training_set, indexes_class)\n",
        "\n",
        "        loader = torch.utils.data.DataLoader(current_class, batch_size=self.batch_size,shuffle=False, num_workers=4)\n",
        "        features = np.zeros((0, 64))\n",
        "        with torch.no_grad():\n",
        "          for indexes, images, labels in loader:\n",
        "            images = images.cuda()\n",
        "            feature = self.feature_extractor(images).data.cpu().numpy()\n",
        "            feature = normalize(feature, axis=1, norm='l2')\n",
        "            features = np.concatenate((features,feature), axis=0)\n",
        "\n",
        "        class_mean = np.mean(features, axis=0)\n",
        "        class_mean = class_mean / np.linalg.norm(class_mean)  # Normalize\n",
        "\n",
        "        self.cumulative_class_mean[classlabel] = class_mean\n",
        "        \n",
        "      #plt.imshow(self.original_exemplar_set.dataset.data[self.exemplar_sets_idxs[0][0]])\n",
        "      #plt.show()\n",
        "      \n",
        "  \n",
        "      #self.exemplar_level_optimization(m, i, self.original_training_set.get_batch_indexes()[i])\n",
        "\n",
        "      # compute means of exemplar set\n",
        "      # cycle for each exemplar set\n",
        "      self.exemplar_means = torch.zeros((0, self.feature_size), dtype=torch.float).to(self.device)\n",
        "      self.exemplar_labels = []\n",
        "      for j in range(len(self.exemplar_sets_idxs)):\n",
        "        exemplars_dataset = Subset(self.original_training_set, self.exemplar_sets_idxs[j])\n",
        "        exemplars_loader = torch.utils.data.DataLoader(exemplars_dataset, batch_size=self.batch_size, shuffle=False, num_workers=4)\n",
        "        ex_features = torch.zeros((0, self.feature_size), dtype=torch.float).to(self.device) # alla fine shape: (len(exemplar_set), feature_size) --> (m, 64)\n",
        "      \n",
        "        with torch.no_grad():\n",
        "          _, _, exemplar_label = self.original_training_set.__getitem__(self.exemplar_sets_idxs[j][0]) \n",
        "          self.exemplar_labels.append(exemplar_label)\n",
        "          # cycle for each batch in the current exemplar set\n",
        "          for _,  exemplars, _ in exemplars_loader:\n",
        "          \n",
        "            # get exemplars features\n",
        "            exemplars = exemplars.to(self.device)\n",
        "            features = self.model.features(exemplars) # shape: (len(exemplars), feature_size)\n",
        "          \n",
        "            # normalize \n",
        "            feature_norms = torch.norm(features, p=2, dim=1) # shape: len(exemplars)\n",
        "            feature_norms.unsqueeze_(1) # shape: (len(exemplars), 1)\n",
        "            features = features/feature_norms\n",
        "          \n",
        "            # concatenate over columns\n",
        "            ex_features = torch.cat((ex_features, features), dim=0)\n",
        "          \n",
        "        # compute current exemplar set mean and normalize it\n",
        "        ex_mean = torch.mean(ex_features, dim=0) # shape: feature_size --> 64\n",
        "        ex_mean = ex_mean/torch.norm(ex_mean)\n",
        "        ex_mean.unsqueeze_(0) # shape: (1, feature_size) --> (1, 64)\n",
        "        self.exemplar_means = torch.cat((self.exemplar_means, ex_mean), dim=0) # shape: (n_examplar sets, feature size)\n",
        "\n",
        "      #if i == 0:\n",
        "       # self.plot_data(self.trainloader)\n",
        "      print('accuracy on training set:', 100*self.__accuracy_fc(self.trainloader,self.diz))\n",
        "      # print('accuracy on test set:', self.__accuracy_on(self.testloader,self,self.diz))\n",
        "      current_test_acc = self.__accuracy_fc(self.testloader, self.diz)\n",
        "      print('accuracy on test set:', 100*current_test_acc)\n",
        "      print('-' * 80)\n",
        "      test_acc.append(current_test_acc)\n",
        "\n",
        "    # compute comfusion matrix and save results\n",
        "    cm = self.plot_confusion_matrix()\n",
        "    with open('iCaRL_'+str(self.randomseed)+\"_cm\", 'wb') as file:\n",
        "      pickle.dump(cm, file, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "    with open('iCaRL_'+str(self.randomseed)+\"_testacc\", 'wb') as file:\n",
        "      pickle.dump(test_acc, file, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n      #NUOVO PAPER DEL PORCODDIO\\n      labels_of_modified = torch.zeros(0, dtype = int).to(\\'cuda\\')\\n      for label in batches[i]:\\n        labels = torch.LongTensor([self.diz[label]]*m).to(\\'cuda\\')\\n        labels_of_modified = torch.cat((labels_of_modified, labels), dim=0)\\n      print(\\'labels to be created\\', labels_of_modified)\\n      print(\\'len to be created\\', len(labels_of_modified))\\n      number_of_images_created = m*10\\n      net_student = resnet32(num_classes=100).to(self.device)\\n      data_type = torch.float\\n      inputs = torch.randn((number_of_images_created, 3, 32, 32), requires_grad=True, device=\\'cuda\\', dtype=data_type)\\n\\n      net_student = copy.deepcopy(self.model)\\n      net_student.eval() #important, otherwise generated images will be non natural\\n      \\n      train_writer = None  # tensorboard writter\\n      global_iteration = 0\\n      di_lr = 0.05\\n      optimizer_di = optim.Adam([inputs], lr=di_lr)\\n\\n      print(\"Starting model inversion\")\\n      batch_idx = 0\\n      inputs = get_images(net=self.model, bs=200, epochs=2000, idx=batch_idx,\\n                        net_student=net_student,\\n                        train_writer=train_writer, use_amp=False,\\n                        optimizer=optimizer_di, inputs=inputs, \\n                        var_scale=0.00005, labels=labels)\\n\\n\\n      plt.imshow(tensor2im(inputs[0]))\\n      plt.show()\\n      plt.imshow(tensor2im(inputs[55]))\\n      plt.show()\\n      print(\\'deepinversion finshed\\')\\n      # update exemplars number\\n      \\n\\n      # reduce the number of each exemplars set\\n      self.reduce_old_exemplars(m) \\n\\n      self.cumulative_class_mean = {}\\n\\n      for classlabel in batches[i]:\\n        indexes_class = self.original_training_set.get_class_indexes(classlabel)\\n        current_class = Subset(self.original_training_set, indexes_class)\\n        #self.get_new_exemplars(indexes_class, m)\\n        self.get_new_exemplars(current_class, m)\\n        indexes_class = self.original_training_set.get_class_indexes(classlabel)\\n        current_class = Subset(self.original_training_set, indexes_class)\\n\\n        loader = torch.utils.data.DataLoader(current_class, batch_size=self.batch_size,shuffle=False, num_workers=4)\\n        features = np.zeros((0, 64))\\n        with torch.no_grad():\\n          for indexes, images, labels in loader:\\n            images = images.cuda()\\n            feature = self.feature_extractor(images).data.cpu().numpy()\\n            feature = normalize(feature, axis=1, norm=\\'l2\\')\\n            features = np.concatenate((features,feature), axis=0)\\n\\n        class_mean = np.mean(features, axis=0)\\n        class_mean = class_mean / np.linalg.norm(class_mean)  # Normalize\\n\\n        self.cumulative_class_mean[classlabel] = class_mean\\n        \\n      #plt.imshow(self.original_exemplar_set.dataset.data[self.exemplar_sets_idxs[0][0]])\\n      #plt.show()\\n      \\n  \\n      #self.exemplar_level_optimization(m, i, self.original_training_set.get_batch_indexes()[i])\\n\\n      # compute means of exemplar set\\n      # cycle for each exemplar set\\n      self.exemplar_means = torch.zeros((0, self.feature_size), dtype=torch.float).to(self.device)\\n      self.exemplar_labels = []\\n      for j in range(len(self.exemplar_sets_idxs)):\\n        exemplars_dataset = Subset(self.original_training_set, self.exemplar_sets_idxs[j])\\n        exemplars_loader = torch.utils.data.DataLoader(exemplars_dataset, batch_size=self.batch_size, shuffle=False, num_workers=4)\\n        ex_features = torch.zeros((0, self.feature_size), dtype=torch.float).to(self.device) # alla fine shape: (len(exemplar_set), feature_size) --> (m, 64)\\n      \\n        with torch.no_grad():\\n          _, _, exemplar_label = self.original_training_set.__getitem__(self.exemplar_sets_idxs[j][0]) \\n          self.exemplar_labels.append(exemplar_label)\\n          # cycle for each batch in the current exemplar set\\n          for _,  exemplars, _ in exemplars_loader:\\n          \\n            # get exemplars features\\n            exemplars = exemplars.to(self.device)\\n            features = self.model.features(exemplars) # shape: (len(exemplars), feature_size)\\n          \\n            # normalize \\n            feature_norms = torch.norm(features, p=2, dim=1) # shape: len(exemplars)\\n            feature_norms.unsqueeze_(1) # shape: (len(exemplars), 1)\\n            features = features/feature_norms\\n          \\n            # concatenate over columns\\n            ex_features = torch.cat((ex_features, features), dim=0)\\n          \\n        # compute current exemplar set mean and normalize it\\n        ex_mean = torch.mean(ex_features, dim=0) # shape: feature_size --> 64\\n        ex_mean = ex_mean/torch.norm(ex_mean)\\n        ex_mean.unsqueeze_(0) # shape: (1, feature_size) --> (1, 64)\\n        self.exemplar_means = torch.cat((self.exemplar_means, ex_mean), dim=0) # shape: (n_examplar sets, feature size)\\n      \\n      #if i == 0:\\n       # self.plot_data(self.trainloader)\\n      print(\\'accuracy on training set:\\', 100*self.__accuracy_fc(self.trainloader,self.diz))\\n      # print(\\'accuracy on test set:\\', self.__accuracy_on(self.testloader,self,self.diz))\\n      current_test_acc = self.__accuracy_nme(self.testloader)\\n      print(\\'accuracy on test set:\\', 100*current_test_acc)\\n      print(\\'-\\' * 80)\\n      test_acc.append(current_test_acc)\\n\\n    # compute comfusion matrix and save results\\n    cm = self.plot_confusion_matrix()\\n    with open(\\'iCaRL_\\'+str(self.randomseed)+\"_cm\", \\'wb\\') as file:\\n      pickle.dump(cm, file, protocol=pickle.HIGHEST_PROTOCOL)\\n    with open(\\'iCaRL_\\'+str(self.randomseed)+\"_testacc\", \\'wb\\') as file:\\n      pickle.dump(test_acc, file, protocol=pickle.HIGHEST_PROTOCOL)\\n\\n      print(\\'PRINT IMAGES\\')\\n      print(\\'with data augmentation\\')\\n      plt.imshow(self.original_training_set.dataset.data[self.exemplar_sets_idxs[0][0]])\\n      plt.show()\\n      print(\\'without data augmentation\\')\\n      plt.imshow(self.original_exemplar_set.dataset.data[self.exemplar_sets_idxs[0][0]])\\n      plt.show()\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170,
          "referenced_widgets": [
            "8cf040765ab74dbaadad053a354daf90",
            "2d54b65c0d66461eaa32669b22d60a92",
            "d7bbbea14a4642ccb0fae794fe27452b",
            "cfcb505a079d45109b8809cb39663d9e",
            "68849cae536d4e989e533ad376b910cd",
            "3fe118bf3d6b447580d04e3d768b978a",
            "6d57853ebd9f40cfa4820a95f0f4990e",
            "dde161228c3842548624717186633725"
          ]
        },
        "id": "OYzLuYGDLr15",
        "outputId": "c8d9f4c6-5207-4d8c-f79b-b612701d055d"
      },
      "source": [
        "method = mnemonics(randomseed=203)\n",
        "model, batch = method.trainer()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "current batches [11, 5, 62, 76, 27, 3, 96, 33, 78, 30]\n",
            "10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8cf040765ab74dbaadad053a354daf90",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=70.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "accuracy on test set: 83.92857142857143\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AhzQjA4HT2wH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce4dc78a-8bec-405d-8423-ffb95693b9e7"
      },
      "source": [
        "!git clone https://github.com/huyvnphan/PyTorch_CIFAR10.git\n",
        "\n",
        "! cp -r /content/PyTorch_CIFAR10/cifar10_models/resnet.py /content\n",
        "\n",
        "!pip install pytorch_lightning\n",
        "!python /content/PyTorch_CIFAR10/train.py --download_weights 1\n",
        "\n",
        "! cp -r /content/cifar10_models/state_dicts /content"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'PyTorch_CIFAR10'...\n",
            "remote: Enumerating objects: 85, done.\u001b[K\n",
            "remote: Counting objects: 100% (85/85), done.\u001b[K\n",
            "remote: Compressing objects: 100% (63/63), done.\u001b[K\n",
            "remote: Total 637 (delta 39), reused 50 (delta 22), pack-reused 552\u001b[K\n",
            "Receiving objects: 100% (637/637), 6.59 MiB | 5.19 MiB/s, done.\n",
            "Resolving deltas: 100% (222/222), done.\n",
            "cp: cannot stat '/content/cifar10_models/state_dicts': No such file or directory\n",
            "Collecting pytorch_lightning\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/12/98/86a89dcd54f84582bbf24cb29cd104b966fcf934d92d5dfc626f225015d2/pytorch_lightning-1.1.4-py3-none-any.whl (684kB)\n",
            "\u001b[K     |████████████████████████████████| 686kB 13.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.6/dist-packages (from pytorch_lightning) (4.41.1)\n",
            "Collecting fsspec[http]>=0.8.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ec/80/72ac0982cc833945fada4b76c52f0f65435ba4d53bc9317d1c70b5f7e7d5/fsspec-0.8.5-py3-none-any.whl (98kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 13.9MB/s \n",
            "\u001b[?25hCollecting PyYAML>=5.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/64/c2/b80047c7ac2478f9501676c988a5411ed5572f35d1beff9cae07d321512c/PyYAML-5.3.1.tar.gz (269kB)\n",
            "\u001b[K     |████████████████████████████████| 276kB 27.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.6/dist-packages (from pytorch_lightning) (2.4.0)\n",
            "Collecting future>=0.17.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/45/0b/38b06fd9b92dc2b68d58b75f900e97884c45bedd2ff83203d933cf5851c9/future-0.18.2.tar.gz (829kB)\n",
            "\u001b[K     |████████████████████████████████| 829kB 40.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.6/dist-packages (from pytorch_lightning) (1.19.5)\n",
            "Requirement already satisfied: torch>=1.3 in /usr/local/lib/python3.6/dist-packages (from pytorch_lightning) (1.7.0+cu101)\n",
            "Collecting aiohttp; extra == \"http\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ad/e6/d4b6235d776c9b33f853e603efede5aac5a34f71ca9d3877adb30492eb4e/aiohttp-3.7.3-cp36-cp36m-manylinux2014_x86_64.whl (1.3MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3MB 50.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests; extra == \"http\" in /usr/local/lib/python3.6/dist-packages (from fsspec[http]>=0.8.1->pytorch_lightning) (2.23.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (0.4.2)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.32.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.0.1)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (0.36.2)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.17.2)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.15.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (51.1.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (3.3.3)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (3.12.4)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.7.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (0.10.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch>=1.3->pytorch_lightning) (3.7.4.3)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch>=1.3->pytorch_lightning) (0.8)\n",
            "Collecting async-timeout<4.0,>=3.0\n",
            "  Downloading https://files.pythonhosted.org/packages/e1/1e/5a4441be21b0726c4464f3f23c8b19628372f606755a9d2e46c187e65ec4/async_timeout-3.0.1-py3-none-any.whl\n",
            "Collecting yarl<2.0,>=1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/da/08/52b26b44bce7b818b410aee37c5e424c9ea420c557bca97dc2adac29b151/yarl-1.6.3-cp36-cp36m-manylinux2014_x86_64.whl (293kB)\n",
            "\u001b[K     |████████████████████████████████| 296kB 60.2MB/s \n",
            "\u001b[?25hCollecting multidict<7.0,>=4.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a1/35/b22524d6b9cacfb4c5eff413a069bbc17c6ea628e54da5c6c989998ced5f/multidict-5.1.0-cp36-cp36m-manylinux2014_x86_64.whl (141kB)\n",
            "\u001b[K     |████████████████████████████████| 143kB 57.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.6/dist-packages (from aiohttp; extra == \"http\"->fsspec[http]>=0.8.1->pytorch_lightning) (20.3.0)\n",
            "Requirement already satisfied: chardet<4.0,>=2.0 in /usr/local/lib/python3.6/dist-packages (from aiohttp; extra == \"http\"->fsspec[http]>=0.8.1->pytorch_lightning) (3.0.4)\n",
            "Collecting idna-ssl>=1.0; python_version < \"3.7\"\n",
            "  Downloading https://files.pythonhosted.org/packages/46/03/07c4894aae38b0de52b52586b24bf189bb83e4ddabfe2e2c8f2419eec6f4/idna-ssl-1.1.0.tar.gz\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests; extra == \"http\"->fsspec[http]>=0.8.1->pytorch_lightning) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests; extra == \"http\"->fsspec[http]>=0.8.1->pytorch_lightning) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests; extra == \"http\"->fsspec[http]>=0.8.1->pytorch_lightning) (2020.12.5)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch_lightning) (1.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (4.2.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (4.6)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard>=2.2.0->pytorch_lightning) (3.3.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch_lightning) (3.1.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (0.4.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard>=2.2.0->pytorch_lightning) (3.4.0)\n",
            "Building wheels for collected packages: PyYAML, future, idna-ssl\n",
            "  Building wheel for PyYAML (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for PyYAML: filename=PyYAML-5.3.1-cp36-cp36m-linux_x86_64.whl size=44621 sha256=70e515543280d8f1e10ca722fbfe26dafd7a521d825656ade642a06dfc3afed6\n",
            "  Stored in directory: /root/.cache/pip/wheels/a7/c1/ea/cf5bd31012e735dc1dfea3131a2d5eae7978b251083d6247bd\n",
            "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for future: filename=future-0.18.2-cp36-none-any.whl size=491057 sha256=ec6a14cf948f98beae44453fdf25abf989de209c5e2d49757847b33b68bb4426\n",
            "  Stored in directory: /root/.cache/pip/wheels/8b/99/a0/81daf51dcd359a9377b110a8a886b3895921802d2fc1b2397e\n",
            "  Building wheel for idna-ssl (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for idna-ssl: filename=idna_ssl-1.1.0-cp36-none-any.whl size=3163 sha256=1b4755e5ecd6016b1ebaf6253c3d9a69787cdd976049eaec99b1ed7868cb436e\n",
            "  Stored in directory: /root/.cache/pip/wheels/d3/00/b3/32d613e19e08a739751dd6bf998cfed277728f8b2127ad4eb7\n",
            "Successfully built PyYAML future idna-ssl\n",
            "Installing collected packages: async-timeout, multidict, yarl, idna-ssl, aiohttp, fsspec, PyYAML, future, pytorch-lightning\n",
            "  Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Found existing installation: future 0.16.0\n",
            "    Uninstalling future-0.16.0:\n",
            "      Successfully uninstalled future-0.16.0\n",
            "Successfully installed PyYAML-5.3.1 aiohttp-3.7.3 async-timeout-3.0.1 fsspec-0.8.5 future-0.18.2 idna-ssl-1.1.0 multidict-5.1.0 pytorch-lightning-1.1.4 yarl-1.6.3\n",
            "100% 979M/979M [01:29<00:00, 10.9MMiB/s]\n",
            "Download successful. Unzipping file...\n",
            "Unzip file successful!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UTxouRaITy8I"
      },
      "source": [
        "#DOPO AVER IMPORTATO IL CONTENUTO DEL TIPO, SCEGLOERE LA RETE CHE SI VUOLE\n",
        "from resnet import resnet50, resnet18, resnet34\n",
        "\n",
        "trials = resnet34(pretrained = True).to('cuda')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4PNt5gwYHm7Q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 708,
          "referenced_widgets": [
            "34fc9558c5c844f8bbbd057d938a13de",
            "6b2dd5c6fe0949b8825b7df113dee6e1",
            "40a58b073e924c459db2c10cced3a3e2",
            "7469612806524cc285c1be1e11da6b83",
            "ad4ffa5060b041649e20b07c54196ecf",
            "a0205e1a3dcb4bee8f8807601a19dfdb",
            "ea81207411964d6f97ad1c3e65d3494f",
            "950639fdad4649d9ab674fb968c180ed"
          ]
        },
        "outputId": "357a65fa-fd1f-454c-a305-47ed32d29d41"
      },
      "source": [
        "#QUI FACCIO FINETUINING SULLA NUOVA RETE, CON LE MIE CLASSI\n",
        "# ORA STO USANDO LE IMMAGINI PRESE DALL'EXEMPLARS SET PER VEDERE SE HO MIGLIORAMENTI\n",
        "train_dataset = Subset(ilCIFAR100(10, 203, train = 'exemplars'), ilCIFAR100(10, 203, train = 'train').get_batch_indexes()[0])\n",
        "test_dataset = Subset(ilCIFAR100(10, 203, train = 'test'), ilCIFAR100(10, 203, train = 'test').get_batch_indexes()[0])\n",
        "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=4, drop_last=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=True, num_workers=4, drop_last=True)\n",
        "diz = ilCIFAR100(10, 203, train = 'train').get_dict()\n",
        "\n",
        "\n",
        "# Prepare Training\n",
        "optimizer = optim.SGD(trials.parameters(), lr=0.008, momentum=0.9, weight_decay= 1e-5)\n",
        "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[14,24], gamma=0.2)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "trials.eval()\n",
        "total = 0.0\n",
        "correct = 0.0\n",
        "for  _, images, labels in test_loader:\n",
        "  labels = torch.tensor([torch.tensor(diz[c.item()]) for c in labels])\n",
        "  labels = labels.to('cuda')\n",
        "  images = images.to('cuda')\n",
        "  outputs = trials(images)\n",
        "  _, preds = torch.max(outputs, dim=1)\n",
        "  total += len(labels)\n",
        "  correct += torch.sum(preds == labels).item()\n",
        "\n",
        "acc = correct / total\n",
        "print('test accuracy before finetuning', acc)\n",
        "\n",
        "\n",
        "trials.train()\n",
        "for epoch in tqdm(range(30)):\n",
        "  tot_loss = 0.0\n",
        "  for _, inputs, labels in train_loader:\n",
        "    inputs = inputs.float().to('cuda')\n",
        "    labels = torch.tensor([diz[c.item()] for c in labels])\n",
        "\n",
        "    labels=labels.to('cuda')\n",
        "    optimizer.zero_grad()\n",
        "    outputs=trials(inputs)\n",
        "\n",
        "    labels_encoded = F.one_hot(labels,10).float().to('cuda') #CAMBIARE ONE_HOT\n",
        "    loss = criterion(outputs, labels_encoded) \n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    tot_loss+=loss.item()\n",
        "  \n",
        "  scheduler.step()\n",
        "  print(f'loss at epoch{epoch}', tot_loss)\n",
        "\n",
        "\n",
        "trials.eval()\n",
        "total = 0.0\n",
        "correct = 0.0\n",
        "for  _, images, labels in test_loader:\n",
        "  labels = torch.tensor([torch.tensor(diz[c.item()]) for c in labels])\n",
        "  labels = labels.to('cuda')\n",
        "  images = images.to('cuda')\n",
        "  outputs = trials(images)\n",
        "  _, preds = torch.max(outputs, dim=1)\n",
        "  total += len(labels)\n",
        "  correct += torch.sum(preds == labels).item()\n",
        "\n",
        "acc = correct / total\n",
        "print('test accuracy after finetuning', acc)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "test accuracy before finetuning 0.14174107142857142\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "34fc9558c5c844f8bbbd057d938a13de",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=30.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "loss at epoch0 21.199261128902435\n",
            "loss at epoch1 10.543528825044632\n",
            "loss at epoch2 6.134027734398842\n",
            "loss at epoch3 3.932923801243305\n",
            "loss at epoch4 2.899127058684826\n",
            "loss at epoch5 2.1529407761991024\n",
            "loss at epoch6 1.9162600487470627\n",
            "loss at epoch7 1.6985274329781532\n",
            "loss at epoch8 1.3835681024938822\n",
            "loss at epoch9 1.2618831489235163\n",
            "loss at epoch10 1.101167593151331\n",
            "loss at epoch11 0.9173290506005287\n",
            "loss at epoch12 0.86254090256989\n",
            "loss at epoch13 1.0169573174789548\n",
            "loss at epoch14 0.7516529085114598\n",
            "loss at epoch15 0.548499371856451\n",
            "loss at epoch16 0.531963087618351\n",
            "loss at epoch17 0.5109023815020919\n",
            "loss at epoch18 0.49481094535440207\n",
            "loss at epoch19 0.48337959591299295\n",
            "loss at epoch20 0.48277781903743744\n",
            "loss at epoch21 0.48180488031357527\n",
            "loss at epoch22 0.4690206050872803\n",
            "loss at epoch23 0.4594260845333338\n",
            "loss at epoch24 0.451158806681633\n",
            "loss at epoch25 0.4557098615914583\n",
            "loss at epoch26 0.4528517108410597\n",
            "loss at epoch27 0.44149372447282076\n",
            "loss at epoch28 0.43964744359254837\n",
            "loss at epoch29 0.4507970530539751\n",
            "\n",
            "test accuracy after finetuning 0.8816964285714286\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zJ4w-tWhY6XC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 824,
          "referenced_widgets": [
            "8ab9014f6da940dcbca287ae87403955",
            "9e1312a916f548cc82647afb4bc1f93e",
            "7f041afe43e14af79fd2f6597cc43b75",
            "80948880c9864340a539797dcfb95562",
            "79cdaacab4f2456e9d70b6eebf42e594",
            "8679353713f544a38be7e308738302c8",
            "154763e3f13d4164acf06ac0f3a8a3d3",
            "971fba7bbe9642a4b7ceeb872429bcec"
          ]
        },
        "outputId": "e47b63f0-8949-4737-84fa-2d56eddc0983"
      },
      "source": [
        "# CODICE PER CREARE LE IMMAGINI SINTETICHE\n",
        "\n",
        "labels_of_modified = torch.zeros(0, dtype = int).to('cuda')\n",
        "for label in batch:\n",
        "  labels = torch.LongTensor([diz[label]]*20).to('cuda')\n",
        "  labels_of_modified = torch.cat((labels_of_modified, labels), dim=0)\n",
        "print('len to be created', len(labels_of_modified))\n",
        "number_of_images_created = 200\n",
        "\n",
        "#SE VOGLIO USARE COME TEACHER LA NOSTRA RETE USARE IL CODICE QUI\n",
        "\n",
        "#teacher = copy.deepcopy(fake_model)\n",
        "#net_teacher = resnet32(num_classes=10).to('cuda')\n",
        "#net_teacher.load_state_dict(teacher.state_dict())\n",
        "#net_teacher.eval()\n",
        "\n",
        "#net_student = resnet32(num_classes=10).to('cuda')\n",
        "net_student = resnet18().to('cuda')\n",
        "\n",
        "trials.eval()\n",
        "\n",
        "inputs = torch.randn((number_of_images_created, 3, 32, 32), requires_grad=True, device='cuda', dtype=torch.float)\n",
        "train_writer = None  # tensorboard writter\n",
        "global_iteration = 0\n",
        "di_lr = 0.05\n",
        "optimizer_di = optim.Adam([inputs], lr=di_lr)\n",
        "plt.imshow(tensor2im(inputs[0]))\n",
        "plt.show()\n",
        "print(\"Starting model inversion\")\n",
        "batch_idx = 0\n",
        "inputs = get_images(net=trials, bs=len(labels_of_modified), epochs=1500, idx=batch_idx, \n",
        "                  net_student=net_student, competitive_scale=10.0, l2_coeff = 3e-8, bn_reg_scale = 5.0,\n",
        "                  train_writer=train_writer, use_amp=False,\n",
        "                  optimizer=optimizer_di, inputs=inputs, \n",
        "                  var_scale=0.001, labels=labels_of_modified) #2.5e-5\n",
        "trials.eval()\n",
        "plt.imshow(tensor2im(inputs[0]))\n",
        "plt.show()\n",
        "print('deepinversion finshed')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "len to be created 200\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Starting model inversion\n",
            "Teacher correct out of 200: 18, loss at 5.609484672546387\n",
            "Student correct out of 200: 20, loss at 2.774320602416992\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8ab9014f6da940dcbca287ae87403955",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=1500.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "It 0\t Losses: total: 41.473,\ttarget: 5.546 \tR_feature_loss unscaled:\t 4.576\n",
            "It 200\t Losses: total: 10.594,\ttarget: 0.021 \tR_feature_loss unscaled:\t 0.363\n",
            "It 400\t Losses: total: 9.516,\ttarget: 0.001 \tR_feature_loss unscaled:\t 0.241\n",
            "It 600\t Losses: total: 9.100,\ttarget: 0.001 \tR_feature_loss unscaled:\t 0.184\n",
            "It 800\t Losses: total: 9.551,\ttarget: 0.001 \tR_feature_loss unscaled:\t 0.227\n",
            "It 1000\t Losses: total: 8.989,\ttarget: 0.001 \tR_feature_loss unscaled:\t 0.142\n",
            "It 1200\t Losses: total: 8.837,\ttarget: 0.001 \tR_feature_loss unscaled:\t 0.150\n",
            "It 1400\t Losses: total: 8.723,\ttarget: 0.001 \tR_feature_loss unscaled:\t 0.130\n",
            "\n",
            "Teacher correct out of 200: 200, loss at 0.0007011687848716974\n",
            "Student correct out of 200: 4, loss at 2.871809959411621\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "deepinversion finshed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "id": "sokaKsjfV9_n",
        "outputId": "c62bec69-ff6a-48bf-907f-d16bde0d83cb"
      },
      "source": [
        "plt.imshow(tensor2im(inputs_data[0]))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g00ncUTSWhBA",
        "outputId": "bfb3707a-a515-47fe-d23a-534341666603"
      },
      "source": [
        "len(inputs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "128"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 153
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0fG-9jw4UAgZ"
      },
      "source": [
        "#IN MODO DA AVERE UN DIZIONARIO UNICO PER IL FINETUNING SINTETICI + IMMAGINI\n",
        "fake_diz = {0:11, 1:5, 2:62, 3:76, 4:27, 5:3, 6:96, 7:33, 8:78, 9:30}\n",
        "labels_of_modified = torch.tensor([fake_diz[c.item()] for c in labels_of_modified]).to('cuda')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3g__5sQg-uzp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3bc5be53-01f5-4605-dda4-85f10778cb83"
      },
      "source": [
        "# CREO UN DATALOADER UNICO CON DATI+IMMAGINI SINTETICHE\n",
        "inputs_data = torch.tensor(inputs, requires_grad=False).cpu()\n",
        "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=4, drop_last=True)\n",
        "\n",
        "porcoddio = torch.utils.data.TensorDataset(torch.randn(len(inputs.data), requires_grad=False).cpu(), inputs_data, labels_of_modified.cpu())\n",
        "porcoddiol = DataLoader(porcoddio, batch_size = 128, shuffle=False, drop_last=False)#, num_workers=4, drop_last=False) #A SECONDA DEL BATCH\n",
        "\n",
        "tt = DataLoader(torch.utils.data.ConcatDataset((train_dataset, porcoddio)), batch_size=128, shuffle=False)#, num_workers=4, pin_memory=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=4, drop_last=False)\n",
        "\n",
        "for param in model.parameters():\n",
        "  param.requires_grad = False\n",
        "\n",
        "model.train('False')\n",
        "correct = 0.0\n",
        "total = 0.0\n",
        "for _, image, label in tt:\n",
        "  labels = torch.tensor(torch.tensor([diz[c.item()] for c in label]))\n",
        "  labels = labels.to('cuda')\n",
        "  image = image.to('cuda')\n",
        "  outputs = model(image)\n",
        "  _, preds = torch.max(outputs, dim=1)\n",
        "  total += len(labels)\n",
        "  correct += torch.sum(preds == labels).item()\n",
        "\n",
        "acc = correct / total\n",
        "model.train('False')\n",
        "print('test accuracy data + syntetic exemplars', acc, total)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "test accuracy data + syntetic exemplars 0.9467307692307693 5200.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NalwamQY5jQ8",
        "outputId": "500a8141-6010-493e-edd5-5930ef1c5aad"
      },
      "source": [
        "#PER VERIFICARE COME LA NOSTRA RETE CLASSIFICA LE IMMAGINI SINTETICHE\n",
        "trials.eval()\n",
        "\n",
        "total = 200.0\n",
        "correct = 0.0\n",
        "\n",
        "label = torch.tensor([torch.tensor(diz[c.item()]) for c in labels_of_modified]).to('cuda')\n",
        "outputs = model(inputs.data)\n",
        "_, preds = torch.max(outputs, dim=1)\n",
        "correct += torch.sum(preds == label).item()\n",
        "#total += len(label)\n",
        "acc = correct / total\n",
        "print('test accuracy only syntetic exemplars', acc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test accuracy with syntetic exemplars 0.845\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FKLFf0sLiKFe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "2a31ea6f07bf47d3b943744ebb4dc2da",
            "979432db69974b7f983e64b9556ae519",
            "e0c734b418f24d02af2c89cb85818e61",
            "1b873a72e9eb4f50a22f743a0c683077",
            "a53c583c6ca440948a71a37875694ccb",
            "b6ddd90b25544dc1a9478d0546c9d537",
            "40e4b81f48f94a379a06f520765652f9",
            "296fe8ad9d23414ab1cd80d06b00960c"
          ]
        },
        "outputId": "fb7d4344-956a-4b9e-cb94-810596d7d4a5"
      },
      "source": [
        "#ALLENO UN MODELLO DA ZERO CON DATI+EXEMPLAR SINTETICI\n",
        "#tt = DataLoader(torch.utils.data.ConcatDataset((train_dataset, porcoddio)), batch_size=128, shuffle=True)#, num_workers=4, pin_memory=True)\n",
        "train_dataset = Subset(ilCIFAR100(10, 203, train = 'train'), ilCIFAR100(10, 203, train = 'train').get_batch_indexes()[0])\n",
        "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=4, drop_last=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=4, drop_last=True)\n",
        "fake_model = resnet32(num_classes=100).to('cuda')\n",
        "fake_model.train()\n",
        "\n",
        "optimizer = optim.SGD(fake_model.parameters(), lr=2, momentum=0.9, weight_decay= 1e-5)\n",
        "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[49,63], gamma=0.2)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "for epoch in tqdm(range(70)):\n",
        "  tot_loss = 0.0 \n",
        "  for _, inputs, labels in tt:\n",
        "    inputs = inputs.float().to('cuda')\n",
        "    labels = torch.tensor([diz[c.item()] for c in labels])\n",
        "\n",
        "    labels=labels.to('cuda')\n",
        "    optimizer.zero_grad()\n",
        "    outputs=fake_model(inputs)\n",
        "\n",
        "    labels_encoded = F.one_hot(labels, 100).float().to('cuda') #CAMBIARE ONE_HOT\n",
        "    loss = criterion(outputs, labels_encoded) \n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    tot_loss+=loss.item()\n",
        "  scheduler.step()\n",
        "  print(f'loss at epoch {epoch}', tot_loss)\n",
        "fake_model.eval()\n",
        "\n",
        "total = 0.0\n",
        "correct = 0.0\n",
        "for  _, images, labels in test_loader:\n",
        "  labels = torch.tensor([torch.tensor(diz[c.item()]) for c in labels])\n",
        "  labels = labels.to('cuda')\n",
        "  images = images.to('cuda')\n",
        "  outputs = fake_model(images)\n",
        "  _, preds = torch.max(outputs, dim=1)\n",
        "  total += len(labels)\n",
        "  correct += torch.sum(preds == labels).item()\n",
        "\n",
        "acc = correct / total\n",
        "print('test accuracy with syntetic exemplars', acc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2a31ea6f07bf47d3b943744ebb4dc2da",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=70.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "loss at epoch 0 2.4540810100734234\n",
            "loss at epoch 1 0.9700034447014332\n",
            "loss at epoch 2 0.8656038641929626\n",
            "loss at epoch 3 0.785343898460269\n",
            "loss at epoch 4 0.7194453729316592\n",
            "loss at epoch 5 0.6722258748486638\n",
            "loss at epoch 6 0.6303174179047346\n",
            "loss at epoch 7 0.5516120810061693\n",
            "loss at epoch 8 0.4870914202183485\n",
            "loss at epoch 9 0.43468355666846037\n",
            "loss at epoch 10 0.42519158590584993\n",
            "loss at epoch 11 0.4045130517333746\n",
            "loss at epoch 12 0.39306805515661836\n",
            "loss at epoch 13 0.3057033601216972\n",
            "loss at epoch 14 0.24645023792982101\n",
            "loss at epoch 15 0.25532248336821795\n",
            "loss at epoch 16 0.2748253480531275\n",
            "loss at epoch 17 0.2523432490415871\n",
            "loss at epoch 18 0.19916161010041833\n",
            "loss at epoch 19 0.17305954755283892\n",
            "loss at epoch 20 0.1533484470564872\n",
            "loss at epoch 21 0.13120939279906452\n",
            "loss at epoch 22 0.10854474175721407\n",
            "loss at epoch 23 0.07234471826814115\n",
            "loss at epoch 24 0.053358428238425404\n",
            "loss at epoch 25 0.03999096667394042\n",
            "loss at epoch 26 0.029273440944962204\n",
            "loss at epoch 27 0.021654711657902226\n",
            "loss at epoch 28 0.026881582962232642\n",
            "loss at epoch 29 0.01737167085229885\n",
            "loss at epoch 30 0.017715373272949364\n",
            "loss at epoch 31 0.009480462504143361\n",
            "loss at epoch 32 0.005044861238275189\n",
            "loss at epoch 33 0.0026598133372317534\n",
            "loss at epoch 34 0.0021308240538928658\n",
            "loss at epoch 35 0.002044381129962858\n",
            "loss at epoch 36 0.0020118399697821587\n",
            "loss at epoch 37 0.0020053692715009674\n",
            "loss at epoch 38 0.0020139500848017633\n",
            "loss at epoch 39 0.002032604257692583\n",
            "loss at epoch 40 0.002058189296803903\n",
            "loss at epoch 41 0.0020886187885480467\n",
            "loss at epoch 42 0.0021223844669293612\n",
            "loss at epoch 43 0.002158326740755001\n",
            "loss at epoch 44 0.002196126995841041\n",
            "loss at epoch 45 0.0022349313912854996\n",
            "loss at epoch 46 0.0022744570633221883\n",
            "loss at epoch 47 0.0023141863239288796\n",
            "loss at epoch 48 0.0023538180757896043\n",
            "loss at epoch 49 0.002360003021749435\n",
            "loss at epoch 50 0.0023678755169385113\n",
            "loss at epoch 51 0.002375762433075579\n",
            "loss at epoch 52 0.0023836740765545983\n",
            "loss at epoch 53 0.0023916044483485166\n",
            "loss at epoch 54 0.0023995298870431725\n",
            "loss at epoch 55 0.0024074393368209712\n",
            "loss at epoch 56 0.0024153122431016527\n",
            "loss at epoch 57 0.0024231407878687605\n",
            "loss at epoch 58 0.002430960928904824\n",
            "loss at epoch 59 0.0024387382727582008\n",
            "loss at epoch 60 0.0024464762755087577\n",
            "loss at epoch 61 0.002454187473631464\n",
            "loss at epoch 62 0.0024618320203444455\n",
            "loss at epoch 63 0.002463058703142451\n",
            "loss at epoch 64 0.0024645832199894357\n",
            "loss at epoch 65 0.0024661042443767656\n",
            "loss at epoch 66 0.0024676241300767288\n",
            "loss at epoch 67 0.002469141541951103\n",
            "loss at epoch 68 0.0024706604053790215\n",
            "loss at epoch 69 0.002472177060553804\n",
            "\n",
            "test accuracy with syntetic exemplars 0.7299107142857143\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RSoKa7qDacFG",
        "outputId": "5fdaa768-7bb4-4fd1-91c5-15c64d0b8d8b"
      },
      "source": [
        "np.concatenate((ilCIFAR100(10, 203, train = 'train').get_batch_indexes()[0], ilCIFAR100(10, 203, train = 'train').get_batch_indexes()[1]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([    3,    11,    37, ..., 49981, 49988, 49991])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 193
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 534,
          "referenced_widgets": [
            "bf39eaf1eea44f0c8d762ab8df73c5ff",
            "5b83f1a6811942b59e33e81f32675894",
            "7bb470edfcca4f4689ab9d218954c24f",
            "897f829eb1fd4021a8b3bf00e4b8acff",
            "654ea4fa57a3410598bbefcdb6982985",
            "91405581e3964a22ab923b4cf26c0836",
            "799ea6b942454dd08f0d86d34a9a9788",
            "065731971c364ca29be56a98229abbda"
          ]
        },
        "id": "hDD941kelDM6",
        "outputId": "1523bd8f-7117-4e39-a6a6-eb5591d5ccfc"
      },
      "source": [
        "#ALLENO UN MODELLO DA ZERO CON SOLO I DATI E CONFRONTO I RISULTATI CON QUELLI PRECEDENTI (PORCODIO)\n",
        "train_dataset1 = Subset(ilCIFAR100(10, 203, train = 'train'), ilCIFAR100(10, 203, train = 'train').get_batch_indexes()[0])\n",
        "train_dataset2 = Subset(ilCIFAR100(10, 203, train = 'train'), ilCIFAR100(10, 203, train = 'train').get_batch_indexes()[0])\n",
        "train_dataset3 = Subset(ilCIFAR100(10, 203, train = 'train'), ilCIFAR100(10, 203, train = 'train').get_batch_indexes()[0])\n",
        "train_dataset4 = Subset(ilCIFAR100(10, 203, train = 'train'), ilCIFAR100(10, 203, train = 'train').get_batch_indexes()[0])\n",
        "train_dataset5 = Subset(ilCIFAR100(10, 203, train = 'train'), ilCIFAR100(10, 203, train = 'train').get_batch_indexes()[0])\n",
        "train_dataset6 = Subset(ilCIFAR100(10, 203, train = 'train'), ilCIFAR100(10, 203, train = 'train').get_batch_indexes()[0])\n",
        "train_dataset7 = Subset(ilCIFAR100(10, 203, train = 'train'), ilCIFAR100(10, 203, train = 'train').get_batch_indexes()[0])\n",
        "train_dataset8 = Subset(ilCIFAR100(10, 203, train = 'train'), ilCIFAR100(10, 203, train = 'train').get_batch_indexes()[0])\n",
        "train_dataset9 = Subset(ilCIFAR100(10, 203, train = 'train'), ilCIFAR100(10, 203, train = 'train').get_batch_indexes()[0])\n",
        "train_dataset10 = Subset(ilCIFAR100(10, 203, train = 'train'), ilCIFAR100(10, 203, train = 'train').get_batch_indexes()[0])\n",
        "tt = DataLoader(torch.utils.data.ConcatDataset((train_dataset1, train_dataset2, train_dataset3, train_dataset4, train_dataset5, train_dataset6, train_dataset7, train_dataset8, train_dataset9, train_dataset10)), batch_size=128, shuffle=True, num_workers=4, pin_memory=True)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=4, drop_last=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=4, drop_last=True)\n",
        "fake_model = resnet32(num_classes=10).to('cuda')\n",
        "\n",
        "#fake_model = resnet34(pretrained = False).to('cuda')\n",
        "fake_model.train()\n",
        "\n",
        "optimizer = optim.SGD(fake_model.parameters(), lr=2, momentum=0.9, weight_decay= 1e-5)\n",
        "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[49,63], gamma=0.2)\n",
        "#USO PARAMETRI DELLO STRONZO https://github.com/chenyaofo/CIFAR-pretrained-models\n",
        "#optimizer = torch.optim.SGD(fake_model.parameters(), lr=0.1, momentum=0.9, dampening=0, weight_decay=1e-4, nesterov=True)\n",
        "#scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200,eta_min=0.001)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "\n",
        "for epoch in tqdm(range(70)):\n",
        "  tot_loss = 0.0 \n",
        "  for _, inputs, labels in tt:\n",
        "    inputs = inputs.float().to('cuda')\n",
        "    labels = torch.tensor([diz[c.item()] for c in labels])\n",
        "\n",
        "    labels=labels.to('cuda')\n",
        "    optimizer.zero_grad()\n",
        "    outputs=fake_model(inputs)\n",
        "\n",
        "    labels_encoded = F.one_hot(labels, 10).float().to('cuda') #CAMBIARE ONE_HOT\n",
        "    loss = criterion(outputs, labels_encoded) \n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    tot_loss+=loss.item()\n",
        "  scheduler.step()\n",
        "  print(f'loss at epoch {epoch}', tot_loss)\n",
        "\n",
        "fake_model.eval()\n",
        "total = 0.0\n",
        "correct = 0.0\n",
        "for  _, images, labels in test_loader:\n",
        "  labels = torch.tensor([torch.tensor(diz[c.item()]) for c in labels])\n",
        "  labels = labels.to('cuda')\n",
        "  images = images.to('cuda')\n",
        "  outputs = fake_model(images)\n",
        "  _, preds = torch.max(outputs, dim=1)\n",
        "  total += len(labels)\n",
        "  correct += torch.sum(preds == labels).item()\n",
        "\n",
        "acc = correct / total\n",
        "print('test accuracy without syntetic exemplars', acc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bf39eaf1eea44f0c8d762ab8df73c5ff",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=70.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "loss at epoch 0 124.5062709748745\n",
            "loss at epoch 1 93.65488928556442\n",
            "loss at epoch 2 80.7460409104824\n",
            "loss at epoch 3 69.81480944156647\n",
            "loss at epoch 4 60.191081427037716\n",
            "loss at epoch 5 52.89395593851805\n",
            "loss at epoch 6 46.07993668317795\n",
            "loss at epoch 7 40.925493493676186\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 772,
          "referenced_widgets": [
            "c1fc62df0e444a798ab7588d23503d91",
            "fd2c504656334a0b923944a1ec25c41b",
            "794d19adb4154452b5054d4242ddba45",
            "c966a8cd8b9545a7a6689e4cb0357e9e",
            "e680fd4baa2d4bf9be5fa51f07028b89",
            "3b8bbc46531843e4ba199661b6aa6963",
            "fb7755e922e848f2a5970a0eda5b8d48",
            "b08e3481631d4bc18fa297155be7f744"
          ]
        },
        "id": "me5FSy9svmgr",
        "outputId": "2fbf1189-6315-4ddf-eb5a-d1fa9aa0716b"
      },
      "source": [
        "# CODICE PER CREARE LE IMMAGINI SINTETICHE\n",
        "\n",
        "labels_of_modified = torch.zeros(0, dtype = int).to('cuda')\n",
        "for label in batch:\n",
        "  labels = torch.LongTensor([diz[label]]*20).to('cuda')\n",
        "  labels_of_modified = torch.cat((labels_of_modified, labels), dim=0)\n",
        "print('len to be created', len(labels_of_modified))\n",
        "number_of_images_created = 200\n",
        "\n",
        "#SE VOGLIO USARE COME TEACHER LA NOSTRA RETE USARE IL CODICE QUI\n",
        "\n",
        "teacher = copy.deepcopy(fake_model)\n",
        "net_teacher = resnet32(num_classes=10).to('cuda')\n",
        "net_teacher.load_state_dict(teacher.state_dict())\n",
        "net_teacher.eval()\n",
        "\n",
        "#net_student = resnet32(num_classes=10).to('cuda')\n",
        "net_student = resnet18().to('cuda')\n",
        "\n",
        "trials.eval()\n",
        "\n",
        "inputs = torch.randn((number_of_images_created, 3, 32, 32), requires_grad=True, device='cuda', dtype=torch.float)\n",
        "train_writer = None  # tensorboard writter\n",
        "global_iteration = 0\n",
        "di_lr = 0.05\n",
        "optimizer_di = optim.Adam([inputs], lr=di_lr)\n",
        "plt.imshow(tensor2im(inputs[0]))\n",
        "plt.show()\n",
        "print(\"Starting model inversion\")\n",
        "batch_idx = 0\n",
        "inputs = get_images(net=net_teacher, bs=len(labels_of_modified), epochs=1000, idx=batch_idx, \n",
        "                  net_student=net_student, competitive_scale=10.0, l2_coeff = 3e-8, bn_reg_scale = 5.0,\n",
        "                  train_writer=train_writer, use_amp=False,\n",
        "                  optimizer=optimizer_di, inputs=inputs, \n",
        "                  var_scale=0.001, labels=labels_of_modified) #2.5e-5\n",
        "trials.eval()\n",
        "plt.imshow(tensor2im(inputs[0]))\n",
        "plt.show()\n",
        "print('deepinversion finshed')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "len to be created 200\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Starting model inversion\n",
            "Teacher correct out of 200: 20, loss at 35.510929107666016\n",
            "Student correct out of 200: 19, loss at 2.457376480102539\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c1fc62df0e444a798ab7588d23503d91",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=1000.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "It 0\t Losses: total: 15274.774,\ttarget: 35.281 \tR_feature_loss unscaled:\t 3046.014\n",
            "It 200\t Losses: total: 981.694,\ttarget: 12.616 \tR_feature_loss unscaled:\t 191.729\n",
            "It 400\t Losses: total: 780.665,\ttarget: 11.310 \tR_feature_loss unscaled:\t 151.813\n",
            "It 600\t Losses: total: 569.170,\ttarget: 10.284 \tR_feature_loss unscaled:\t 109.734\n",
            "It 800\t Losses: total: 790.407,\ttarget: 9.514 \tR_feature_loss unscaled:\t 154.156\n",
            "\n",
            "Teacher correct out of 200: 83, loss at 8.92462158203125\n",
            "Student correct out of 200: 16, loss at 2.4559409618377686\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "deepinversion finshed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zMVWq87mcCqu",
        "outputId": "d927d9d9-2c1d-4a33-c51f-9ee56b46af2f"
      },
      "source": [
        "fake_model.eval()\n",
        "total = 0.0\n",
        "correct = 0.0\n",
        "for  _, images, labels in test_loader:\n",
        "  labels = torch.tensor([torch.tensor(diz[c.item()]) for c in labels])\n",
        "  labels = labels.to('cuda')\n",
        "  images = images.to('cuda')\n",
        "  outputs = fake_model(images)\n",
        "  _, preds = torch.max(outputs, dim=1)\n",
        "  total += len(labels)\n",
        "  correct += torch.sum(preds == labels).item()\n",
        "\n",
        "acc = correct / total\n",
        "print('test accuracy with syntetic exemplars', acc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test accuracy with syntetic exemplars 0.8392857142857143\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "umrCzlLBgXYq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}